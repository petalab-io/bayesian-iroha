[
  {
    "objectID": "notebooks/01_web_performance/roi_decision.html",
    "href": "notebooks/01_web_performance/roi_decision.html",
    "title": "エグゼクティブサマリー",
    "section": "",
    "text": "【分析の背景】 サイト全体の平均LCP改善という表面的な指標に隠れた、決済ページ等の重要ページにおけるパフォーマンス改悪がビジネスに与える損失リスクを評価しました。本分析では、PyMC v5を用いた階層ベイズモデルに加え、ビジネス前提（売上感度）の不確実性を加味した「感度分析」を実行し、意思決定の妥当性を検証しています。\n【主要な発見】 * 期待純利益: Top/Detailページの改善効果により、全体では約 8,210万円 の大幅な増収が期待値として算出されました。 * 収益化確率（勝率）: しかし、全体の収益がプラスになる確率は 65.1% に留まり、約3回に1回は赤字になる という不安定な状態です。 * 潜在的リスクの特定: 最大の懸念点は「決済ページ（Checkout）」です。このページ単体では 77.4% の確率で1万円以上の損失が発生 しており、Topページの稼いだ利益を食いつぶす明確な「出血点」となっています。\n【最終アクション】 「条件付きリリース」 を推奨します。全体を一括リリースすることは、Checkoutページのリスク（勝率65%）が高すぎるため却下とします。 リスクの低い Top/Detail ページの改善のみを部分的にリリース し、Checkout ページに関しては実装をロールバックまたは修正した後、再検証を行うべきです。\n\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport japanize_matplotlib\n\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/arviz/__init__.py:39: FutureWarning: \nArviZ is undergoing a major refactor to improve flexibility and extensibility while maintaining a user-friendly interface.\nSome upcoming changes may be backward incompatible.\nFor details and migration guidance, visit: https://python.arviz.org/en/latest/user_guide/migration_guide.html\n  warn(\n\n\n\n# カラー定義\nCOLOR_PURPLE = \"#9B5DE5\"  # 事後分布・HDI\nCOLOR_YELLOW = \"#F9C74F\"  # ROPE領域\nCOLOR_GREEN  = \"#06D6A0\"  # 改善判定\nCOLOR_RED    = \"#EF476F\"  # 悪化判定\nCOLOR_GRAY   = \"#8D99AE\"  # 等価判定\n\n# 1. Matplotlibのデフォルトカラーサイクルを変更\nfrom cycler import cycler\nplt.rcParams['axes.prop_cycle'] = cycler(color=[COLOR_PURPLE, COLOR_YELLOW, COLOR_GREEN, COLOR_RED, COLOR_GRAY])\n\n# 2. Seabornのスタイル設定\nsns.set_style(\"whitegrid\")\nsns.set_palette([COLOR_PURPLE, COLOR_YELLOW, COLOR_GREEN, COLOR_RED, COLOR_GRAY])\n\n\nprint(f\"PyMC version: {pm.__version__}\")\n\n# 再現性の確保\nRANDOM_SEED = 42\nrng = np.random.default_rng(RANDOM_SEED)\n\nPyMC version: 5.26.1\n\n\n\n# --- 1. Data の生成 (N=20 小規模Data) ---\ndef generate_weighted_scenario_data(n_per_page=20):\n    \"\"\"\n    Top Page 改善、Checkout page 悪化の Trap-data 生成\n    \"\"\"\n    pages = [\"Top\", \"Detail\", \"Contract\", \"Checkout\"]\n\n    scenario = {\n        'Top': (3000, 2500),  # 改善（良）\n        'Detail': (2800, 2600),  # （微良）\n        'Contract': (3500, 3550),  # 変化なし(微増)\n        'Checkout': (3000, 4000)  #  悪化　★ここが罠\n    }\n\n    data = []\n    for page in pages:\n        mu_pre, mu_post = scenario[page]\n\n        # Pre (対数正規分布)\n        lcp_pre = rng.lognormal(mean=np.log(mu_pre), sigma=0.4, size=n_per_page)\n        data.extend([{\"page\": page, \"group\": \"pre\", \"lcp\": val} for val in lcp_pre])\n\n        # Post (対数正規分布)\n        lcp_post = rng.lognormal(mean=np.log(mu_post), sigma=0.4, size=n_per_page)\n        data.extend([{\"page\": page, \"group\": \"post\", \"lcp\": val} for val in lcp_post])\n\n    return pd.DataFrame(data)\n\n\ndf = generate_weighted_scenario_data(n_per_page=20)\n\n\n# 基本統計量の確認\nprint(\"--- 基本統計量（Group × Page） ---\")\n\ndf.groupby(\"group\").agg({\"lcp\": \"mean\"})\n\n--- 基本統計量（Group × Page） ---\n\n\n\n\n\n\n\n\n\nlcp\n\n\ngroup\n\n\n\n\n\npost\n3196.734803\n\n\npre\n3225.695790\n\n\n\n\n\n\n\n単純に df.groupby(\"group\").agg({\"lcp\": \"mean\"}) を見ると、Sample数が均等なため - Pre: 3225ms -&gt; Post: 3196ms\nと、全体として大きな変化はありません。\n\ndf.groupby([\"page\", \"group\"]).agg({\"lcp\": \"mean\"}).unstack()  # 確認\n\n# unstack() は、MultiIndex（重層的なインデックス）を持つ Series や DataFrame のインデックスの最下層を、カラム（列）へと展開（ピボット）する効果があります。\n# このコードの場合、groupby([\"page\", \"group\"]) によってインデックスが page と group の二段構えになりますが、\n# .unstack() を付けることで、group（pre/Post）が横に並び、ページごとの比較がしやすくなります。\n\n\n\n\n\n\n\n\nlcp\n\n\ngroup\npost\npre\n\n\npage\n\n\n\n\n\n\nCheckout\n3840.892359\n3570.660555\n\n\nContract\n3587.029423\n3158.051664\n\n\nDetail\n2607.372864\n3050.224780\n\n\nTop\n2751.644568\n3123.846159\n\n\n\n\n\n\n\nCheckout を見ると明確に悪化しています。\n\n# 分布の可視化\nfig, axes = plt.subplots(2, 2, figsize=(18, 9))\nax_flat = axes.flatten()\n\nfor i, page in enumerate(df[\"page\"].unique()):\n    sns.histplot(data=df[df[\"page\"] == page], x=\"lcp\", hue=\"group\", kde=True, log_scale=True, ax=ax_flat[i])\n    ax_flat[i].set_title(f\"LCP {page}\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nこの可視化において注目すべき点は、「平均値の罠」が視覚的に表現されていることです。\n\nTopページとDetailページ（改善傾向）: 青色（Pre）に比べて、オレンジ色（Post）の分布が左側（LCPが小さい、つまり高速な方向）にシフトしていることが見て取れます。特にTopページでは、ヒストグラムの山が明確に左へ移動しており、パフォーマンス改善の恩恵を最も受けていることがわかります。\nContractページ（変化なし）: PreとPostの分布がほぼ重なっており、施策による影響がほとんど見られません。\nCheckoutページ（改悪傾向：重要）: ここが本分析の「罠」となる部分です。他のページとは逆に、オレンジ色（Post）の分布が右側（LCPが大きい、つまり低速な方向）へシフトし、裾野も広がっています。これは、決済画面において明確なパフォーマンス低下が発生していることを示唆しています。\n\n\n技術的補足と直感的な説明\n\n対数正規分布と対数スケール (Log Scale) 【定義】: LCPのような待ち時間データは、下限が0で右側に長い裾を持つ性質があるため、通常は対数正規分布に従います。このプロットでは log_scale=True を使用することで、歪んだデータを正規分布に近い形に変換し、中心傾向（最頻値や中央値）のズレを視覚的に比較しやすくしています。 【直感】: 待ち時間のデータは、「すごく遅い人」が一部混じるため、単純な平均をとると実態を見誤ります。このグラフでは、データの「偏り」を調整して、PreとPostの「山の位置」がどれくらいズレたかを、人間の目で直感的に捉えやすい形に整えています。\nカーネル密度推定 (KDE: Kernel Density Estimation) 【定義】: ヒストグラムに重なっている曲線はカーネル密度推定です。これは、離散的なデータポイントから連続的な確率密度関数を推定する手法であり、ヒストグラムのビン（棒）の切り方に依存せずに、データの分布形状を滑らかに表現します。 【直感】: 点々としたデータの集まりを、「なだらかな山」として描いたものです。山の形を見ることで、「だいたい何秒くらいのユーザーが一番多いのか」や「分布のばらつき（山の幅）」を一目で比較できるようになります。\n\n\n\n結論としての意思決定支援\nこのプロットから得られるビジネス上の洞察は、「全ページ合計の平均値では改善しているように見えても、最も売上に直結する Checkout ページで致命的な遅延が発生している」というリスクの可視化です。\n\n\n非中心化による階層ベイズモデル\nN=20のような小規模データで階層モデル（特に分散パラメータ \\(\\sigma\\) が小さい場合）を推定すると、MCMCサンプラーが「漏斗（Funnel）のような形状」の確率分布を探索できず、Divergences（発散） というエラーが頻発することがあります。 これはベイズ推論の信頼性を損ってしまう為、回避する為に、変数の依存関係を数式上で切り離す 「非中心化」 テクニックを用います。 - 中心化（Centered）: \\(\\beta \\sim \\text{Normal}(\\mu, \\sigma)\\) - \\(\\beta\\) の値が決まる際、\\(\\mu\\) と \\(\\sigma\\) に直接依存する。 - 非中心化（Non-centered）: \\(z \\sim \\text{Normal}(0, 1)\\), \\(\\beta = \\mu + z \\cdot \\sigma\\) - \\(z\\) は標準正規分布から独立に生成され、あとでスケーリングされる。これによりサンプラーがスムーズに空間を移動できる。\n\nidx_page, pages = pd.factorize(df[\"page\"])\nidx_group, groups = pd.factorize(df[\"group\"])\n\n# PyMC用 Coords (座標) 定義\ncoords = {\n    \"id_obs\": df.index.values,\n    \"page\": pages,\n    \"group\": groups  # [\"Pre\", \"Post\"]\"\n}\n\n複雑な階層モデルの可読性をあげる為、PyMC における - カテゴリ変数のIndex化 - 【定義】: モデル内部での行列演算を効率化するため、文字列（“Top”, “Checkout”等）を整数インデックス（0, 1, …）に変換する処理です。これにより、各観測データがどのグループ（階層）に属するかを、数式内でポインタのように参照できるようになります。 - 【直感】: 「住所録」に名前でアクセスするのではなく、「出席番号」を割り振る作業です。「出席番号1番のLCPはこれ」と番号で管理することで、計算機が迷わず高速にデータを処理できるようになります。 - Coords(座標)の定義 - 【定義】: 確率変数の各次元（Dimension）に対して、人間が理解できるラベルを付与する仕組みです。PyMC（内部的にはxarrayを使用）において、多次元配列の各軸が何を意味しているのか（どのページか、どのグループか）を明示的に宣言します。 - 【直感】: グラフの「軸（ラベル）」を定義することです。ただの「4行2列の数字の塊」に、「縦軸はページ名（Top等）」「横軸は施策の前後（Pre/Post）」という名前を付けることで、後で結果を見たときに「どの数字がCheckoutページのPostの結果か」を一目でわかるようにします。\nというプロセスを実施します。\n\nなぜこれらが必要なのか？（主な3つの目的）\n\n高次元データの整合性確保 (Error Prevention) 階層モデルでは、「ページ(4) × グループ(2)」のようにパラメータが多次元になります。 Coordsを定義しておくと、PyMCが自動的に「4×2の行列」と「観測データ（N=160）」を正しくマッピングしてくれます。これにより、行列のサイズが合わないといった実行時エラーを未然に防ぐことができます。\n意味のある事後分布の解析 (Labeling) MCMCによるサンプリング結果（事後分布）を解析する際、Coordsが定義されていると、ArviZなどのツールを使って以下のように直感的にアクセスできます。 trace.posterior[“mu_cell”].sel(page=“Checkout”, group=“Post”) もしCoordsがないと、trace.posterior[“mu_cell”][:, :, 3, 1] のように「マジックナンバー（3や1が何を指すか不明な状態）」で指定しなければならず、ミスや混乱の元になります。\n非中心化実装の柔軟性 今回のモデルで採用している「非中心化（Non-centered）」モデルでは、標準正規分布からサンプリングした値にズレ（）を掛け合わせる操作を行います。 Coordsがあると、この「ズレ」をどの次元に対して適用するのかを dims=(“page”, “group”) のように名前で指定でき、モデルの構造が非常に読みやすくなります。\n\nこのように、Index化とCoordsの定義は、 「計算機のための効率化」と「人間のための可読性・解析性」 を両立させるための非常に重要なベストプラクティスです。これを行うことで、将来的にページ数が増えたり、新しいセグメント（デバイス別など）を追加したりする場合も、モデルの拡張が容易になります\n\nwith pm.Model(coords=coords) as model:\n    # --- Data Containers ---\n    _idx_page  = pm.Data(\"idx_page\", idx_page, dims=\"id_obs\")\n    _idx_group = pm.Data(\"idx_group\", idx_group, dims=\"id_obs\")\n    _obs_lcp   = pm.Data(\"obs_lcp\", df[\"lcp\"].values, dims=(\"id_obs\"))\n\n    # --- Hierarchical Priors (Non-centered Parameterization)\n\n    # 1. Global Intercept (全体の基準値)\n    mu_global = pm.Normal(\"mu_global\", mu=7.5, sigma=1.0, dims=\"group\")\n\n    # 2. Page-level Deviations (Page ごとのクせ)\n    # sigma_page: ページ間のばらつきの大きさ\n    sigma_page = pm.HalfNormal(\"sigma_page\", sigma=0.5)\n\n    # offset_page_z: 標準正規分布に従う「ズレの素」（非中心化の肝）\n    # shape=(4page, 2group) -&gt; 各ページ・各グループごとに固有の偏差を持つ\n    offset_page_z = pm.Normal(\"offset_page_z\", mu=0.0, sigma=1.0, dims=(\"page\", \"group\"))\n\n    # Deterministic で実際に偏差に変換: beta = mu + z * sigma\n    mu_page = pm.Deterministic(\n        \"mu_page\",\n        mu_global + offset_page_z * sigma_page,  # broadcasting\n        dims=(\"page\", \"group\")\n    )\n\n    # --- Likelihood ---\n    sigma_obs = pm.HalfNormal(\"sigma_obs\", sigma=0.5)\n\n    # 予測値の構築\n    # mu_page は (page, group) の２次元配列なので、Index で参照\n    mu_LCP = mu_page[_idx_page, _idx_group]\n\n    lcp = pm.Lognormal(\"lcp\", mu=mu_LCP, sigma=sigma_obs, observed=_obs_lcp, dims=(\"id_obs\"))\n\n\npm.model_to_graphviz(model)\n\n\n\n\n\n\n\n\n\n\n\n非中心化による階層ベイズモデル\nこの構造により、モデルは「情報の借用（Shrinkage）」を適切に行いつつ、小規模データでもサンプリングが収束しやすい（発散しにくい）堅牢なものになっています。 - 【直感】: 個性の強さ」と「個々のズレ」を分ける 「ページごとの個性がどれくらい強いか（sigma_page）」というボリュームのつまみと、 「各ページが標準からどの方向にズレているか（offset_page_z）」という方向のつまみを分けたイメージです。 これにより、データが少ないCheckoutページなどの推定値が、全体の平均（mu_global）に向かって適切に引き寄せられ、極端な外れ値に振り回されるのを防ぎます。\n\n# --- 推論 ---\nwith model:\n    trace = pm.sample(draws=2000, tune=1000, target_accept=0.95, random_seed=RANDOM_SEED)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu_global, sigma_page, offset_page_z, sigma_obs]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 15 seconds.\n\n\n\n# 推論結果\naz.plot_trace(trace, compact=False, var_names=[\"mu_global\", \"sigma_page\", \"offset_page_z\"])\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n1. トレースプロットの形状（Fuzzy Caterpillar）\n見た目の特徴: 各チェーン（異なる色の線）が互いにしっかりと混ざり合い、特定の傾向（右肩上がりや下がり）を持たず、一定の範囲を細かく上下に振動しています。\n解釈: これがいわゆる「毛虫（Fuzzy Caterpillar）」のような状態であり、サンプラーが事後分布の全体を効率よく探索できていることを示します。 特定の場所で停滞したり、チェーンごとに大きく値が離れたりしていないため、良好な収束のサインです。\n\naz.summary(trace, var_names=[\"mu_global\", \"sigma_page\", \"offset_page_z\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nmu_global[pre]\n8.010\n0.082\n7.854\n8.166\n0.001\n0.002\n3190.0\n3060.0\n1.0\n\n\nmu_global[post]\n8.000\n0.082\n7.841\n8.157\n0.001\n0.002\n3341.0\n3321.0\n1.0\n\n\nsigma_page\n0.130\n0.070\n0.007\n0.251\n0.002\n0.002\n1738.0\n2415.0\n1.0\n\n\noffset_page_z[Top, pre]\n-0.082\n0.709\n-1.399\n1.297\n0.010\n0.008\n5245.0\n5857.0\n1.0\n\n\noffset_page_z[Top, post]\n-0.701\n0.740\n-2.079\n0.713\n0.010\n0.009\n4986.0\n5565.0\n1.0\n\n\noffset_page_z[Detail, pre]\n-0.130\n0.724\n-1.475\n1.238\n0.010\n0.009\n4953.0\n5164.0\n1.0\n\n\noffset_page_z[Detail, post]\n-0.922\n0.743\n-2.343\n0.441\n0.011\n0.008\n4651.0\n5738.0\n1.0\n\n\noffset_page_z[Contract, pre]\n0.038\n0.710\n-1.280\n1.388\n0.010\n0.008\n5021.0\n5358.0\n1.0\n\n\noffset_page_z[Contract, post]\n0.689\n0.728\n-0.703\n2.014\n0.011\n0.009\n4769.0\n5210.0\n1.0\n\n\noffset_page_z[Checkout, pre]\n0.289\n0.716\n-1.009\n1.689\n0.010\n0.009\n4986.0\n5239.0\n1.0\n\n\noffset_page_z[Checkout, post]\n0.981\n0.754\n-0.393\n2.427\n0.011\n0.010\n4460.0\n4933.0\n1.0\n\n\n\n\n\n\n\n\n\n2. \\(\\hat{R}\\) (R-hat) 指標\n数値 : すべてのパラメータにおいて、値が 1.00 になっています。\n解釈 : \\(\\hat{R}\\) は「チェーン間のばらつき」と「チェーン内のばらつき」を比較する指標です。 一般に 1.1 未満（厳密には 1.05 未満）であれば収束したとみなされます。 今回の結果は、全て 1.00 に極めて近いため、複数の独立した試行（チェーン）がすべて同じ統計的結論に達していることを意味します。\n\n\n3. 効サンプルサイズ (ESS: Effective Sample Size)\ness_bulk および ess_tail カラムを確認してください。\n数値 : 今回の設定（draws=2000, chains=4 の場合、計 8000 サンプル）に対して、十分な大きさ（数百〜数千以上）が確保されています。\n解釈 : MCMC のサンプルは前後の値に相関があるため、実際のサンプル数よりも「有効な（独立した）情報量」は少なくなります。 ESS が大きいことは、自己相関が低く、事後分布の平均や裾（テイル）の推定が安定していることを示します。\n1, 2, 3, より MCMC は、非常に良好に収束しており、推論結果は十分に信頼できる状態と判断。\n\n# --- Shrinkage (情報の借用)の可視化 ---\n\n# 1. Source-data の平均（MEL: 最尤推定に相当）\nmeans_raw_log = {}\nfor page in pages:\n    for group in groups:\n        mask = (df[\"page\"] == page) & (df[\"group\"] == group)\n        means_raw_log[(page, group)] = np.log(df.loc[mask, \"lcp\"]).mean()\n\n# 2. 事後分布の平均（Bayes 推定値）\nmu_posterior_page = trace.posterior[\"mu_page\"].mean(dim=[\"chain\", \"draw\"])\n\n# 3. 全体の平均 (Grand Mean)\nmu_posterior_global = trace.posterior[\"mu_global\"].mean(dim=\n                                                        [\"chain\", \"draw\"])\n\n# --- 可視化 ---\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\ncolors = plt.cm.tab10.colors  # Color-Palet\n\nfor i, group in enumerate(groups):\n    ax = axes[i]\n\n    # 各ページのポイント\n    for j, page in enumerate(pages):\n        # Source-data の平均（x軸） - Dict から取得\n        mean_raw = means_raw_log[(page, group)]\n\n        # Bayes推定の事後平均（y軸）\n        mean_bayes = float(mu_posterior_page.sel(page=page, group=group).values)\n\n        # Plot\n        ax.scatter(mean_raw, mean_bayes, s=150, color=colors[j], zorder=3)\n        ax.annotate(page, (mean_raw, mean_bayes), textcoords=\"offset points\", xytext=(8, 5), fontsize=11)\n\n    # Grand Mean (全体平均)の水平線\n    mean_grand = float(mu_posterior_global.sel(group=group).values)\n    ax.axhline(mean_grand, color=\"red\", linestyle=\"--\", alpha=0.7, label=f\"Grand Mean ({np.exp(mean_grand):.0f}ms\")\n\n    # 45度線（Shrinkage なしの場合）\n    x_all = [means_raw_log[(p, group)] for p in pages]\n    y_all = [float(mu_posterior_page.sel(page=p, group=group).values) for p in pages]\n    val_min = min(x_all + y_all) - 0.1\n    val_max = max(x_all + y_all) + 0.1\n\n    ax.plot([val_min, val_max], [val_min, val_max], color=\"gray\", linestyle=\":\", alpha=0.5, label=\"No Shrinkage (45°)\")\n    ax.set_xlim(val_min, val_max)\n    ax.set_ylim(val_min, val_max)\n\n    ax.set_xlabel(\"Raw Data Mean (log scale)\", fontsize=12)\n    ax.set_ylabel(\"Bayesian Posterior Mean (log scale)\", fontsize=12)\n    ax.set_title(f\"Shrinkage Plot: {group}\", fontsize=14)\n    ax.legend(loc=\"lower right\")\n    ax.grid(True, alpha=0.3)\n    ax.set_aspect(\"equal\", adjustable=\"box\")\n\nplt.suptitle(\"Visualization of Shrinkage\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPlot の構成要素\n\nX軸 (Raw Data Mean) : 生データから計算した各ページの平均 LCP（対数スケール）。最尤推定（MLE）に相当します。\nY軸 (Bayesian Posterior Mean) : 階層ベイズモデルによる推定値（事後平均）\n各点（Top, Detail, Contract, Checkout） : 各ページの推定値を表すプロット\n赤い破線（Grand Mean） : 全ページの全体平均（階層モデルの「親」の推定値）\n灰色の点線（45度線） : Shrinkage がない場合の基準線（生データ = ベイズ推定値）\n\n\n\n解釈\n今回の N=20 という小規模データでは： すべてのページで若干の Shrinkage が発生しているはずです（点が45度線から少し離れている）。 - Grand Mean に近いページ（例: Contract）は、あまり Shrinkage の影響を受けません（もともと全体平均に近いため）。 - Grand Mean から離れたページ（例: Top や Checkout）は、より強く全体平均に引き寄せられます。\nCheckout ページのデータは N=20 しかないが、階層モデルが他のページの傾向を借用して推定値を安定させている様子です。 その結果、生データよりも信頼できる推定値が得られていると判断できます。\n\n\n解釈のポイント\n\n\n45度線との位置関係\n\n\n点が45度線上にある場合: Shrinkage なし。生データの平均がそのまま推定値として採用されています。\n点が45度線から離れて赤線（Grand Mean）に近づいている場合: Shrinkage が働いています。モデルが「このページのデータだけでは不確実なので、他のページの情報を借りて調整しよう」と判断しています。\n\n\n\n\nShrinkage が起こる理由 階層ベイズモデルでは、データが少ない or ばらつきが大きいグループほど、全体平均に向かって「引き寄せられる」傾向があります。\n\n\n\n\n以下のような効果をもたらします：\n\n\nデータが少ない : 外れ値に振り回されるのを防ぐ\nばらつきが大きい : 過度に極端な推定を避ける\nデータが十分にある : 生データをほぼそのまま信頼\n\n\n\n# Shrinkage の強さを数値化\nprint(\"--- Shrinkage Analysis ---\")\nprint(f\"{'Page':&lt;12} {'Group':&lt;6} {'Raw (ms)':&lt;12} {'Bayes (ms)':&lt;12} {'Shrinkage %':&lt;12}\")\nprint(\"-\" * 60)\n\nfor page in pages:\n    for group in groups:\n        raw = means_raw_log[(page, group)]\n        bayes = float(mu_posterior_page.sel(page=page, group=group).values)\n        grand = float(mu_posterior_global.sel(group=group).values)\n\n        # Shrinkage率: Source-data から全体平均にどれだけ引き寄せられたか\n        if abs(raw - grand) &gt; 0.001:\n            pct_shrinkage = (raw - bayes) / (raw - grand) * 100\n        else:\n            pct_shrinkage = 0.0\n\n        # 実空間(ms)に変換して表示\n        print(f\"{page:&lt;12} {group:&lt;6} {np.exp(raw):&lt;12.0f} {np.exp(bayes):&lt;12.0f} {pct_shrinkage:&lt;12.1f}%\")\n\n--- Shrinkage Analysis ---\nPage         Group  Raw (ms)     Bayes (ms)   Shrinkage % \n------------------------------------------------------------\nTop          pre    2961         2982         41.9        %\nTop          post   2612         2736         35.1        %\nDetail       pre    2937         2966         39.5        %\nDetail       post   2505         2666         35.8        %\nContract     pre    3038         3029         32.5        %\nContract     post   3396         3246         34.6        %\nCheckout     pre    3176         3121         32.9        %\nCheckout     post   3590         3362         35.2        %\n\n\n\n\n\nROPE 判定（実質的等価姓の確認）\nROPE定義 : 「変化が ±50ms 以内であれば、User は気づかない」と定義する。\n\n# 事後分布の抽出と Data 整形\nposterior = trace.posterior\n\n# mu_page: (chain, draw, page, group) -&gt; (samples, page, group)\n# stack chain and draw\nmu_samples = posterior[\"mu_page\"].stack(sample=(\"chain\", \"draw\")).values.transpose(2, 0, 1)\n\n# 対数空間から実空間(ms)へ\nsamples_lcp_ms = np.exp(mu_samples)\n# shape: (samples, 4_pages, 2_groups) -&gt; 0:Pre, 1:Post\n\n# 改善量 (Pre - Post)\nms_diff = samples_lcp_ms[:, :, 0] - samples_lcp_ms[:, :, 1]\n# shape: (samples, 4_pages)\n\n\n# --- ROPE Analysis ---\nROPE_RANGE = [-100, 100]  # ±100ms は誤差とみなす\n\nprint(\"--- ROPE Analysis (Probability of Practical Effect)---\")\nfor i, page in enumerate(pages):\n    # ROPE内に入っている確立\n    prob_in_rope = np.mean((ms_diff[:, i] &gt; ROPE_RANGE[0]) & (ms_diff[:, i] &lt; ROPE_RANGE[1]))\n\n    # ROPE より改善している (&gt;500ms) 確率\n    prob_better = np.mean(ms_diff[:, i] &gt;= ROPE_RANGE[1])\n\n    # ROPE より悪化している (&lt;-500ms) 確率\n    prob_worse = np.mean(ms_diff[:, i] &lt;= ROPE_RANGE[0])\n\n    print(f\"Page: {page}\")\n    print(f\"    Mean Diff: {ms_diff[:, i].mean():.1f} ms\")\n    print(f\"    Prob Better (&gt;{ROPE_RANGE[1]}ms):  {prob_better * 100:.1f}%\")\n    print(f\"    Prob Worse: (&lt;{ROPE_RANGE[0]}ms): {prob_worse * 100:.1f}%\")\n    print(f\"    Prob Equiv  (In ROPE): {prob_in_rope * 100:.1f}%\")\n    print(\"-\" * 30)\n\n--- ROPE Analysis (Probability of Practical Effect)---\nPage: Top\n    Mean Diff: 245.6 ms\n    Prob Better (&gt;100ms):  68.9%\n    Prob Worse: (&lt;-100ms): 10.5%\n    Prob Equiv  (In ROPE): 20.5%\n------------------------------\nPage: Detail\n    Mean Diff: 299.9 ms\n    Prob Better (&gt;100ms):  75.8%\n    Prob Worse: (&lt;-100ms): 7.0%\n    Prob Equiv  (In ROPE): 17.2%\n------------------------------\nPage: Contract\n    Mean Diff: -218.6 ms\n    Prob Better (&gt;100ms):  14.9%\n    Prob Worse: (&lt;-100ms): 64.3%\n    Prob Equiv  (In ROPE): 20.8%\n------------------------------\nPage: Checkout\n    Mean Diff: -243.8 ms\n    Prob Better (&gt;100ms):  14.2%\n    Prob Worse: (&lt;-100ms): 65.8%\n    Prob Equiv  (In ROPE): 20.0%\n------------------------------\n\n\n\n# --- 可視化: 事後分布の HDI と ROPE の重なり具合 ---\nHDI_PROB = 0.94  # 94% HDI\n\n# --- 可視化 ---\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\naxes_flat = axes.flatten()\n\nfor i, page in enumerate(pages):\n    ax = axes_flat[i]\n    data_page = ms_diff[:, i]  # 各ページの改善量サンプル (shape: 8000,)\n\n    # 1. 事後分布の Histogram / KDE\n    az.plot_kde(data_page, ax=ax, plot_kwargs={\"color\": \"mediumpurple\", \"linewidth\": 2}, fill_kwargs={\"alpha\": 0.4})\n\n    # 2. HDI (94%) を計算してプロット\n    hdi = az.hdi(data_page, hdi_prob=HDI_PROB)\n    ax.axvline(hdi[0], color=\"mediumpurple\", linestyle=\"--\", linewidth=1.5, label=f\"{HDI_PROB * 100:.0f}% HDI\")\n    ax.axvline(hdi[1], color=\"mediumpurple\", linestyle=\"--\", linewidth=1.5)\n    # HDI範囲の塗りつぶし（Option）\n    ax.axvspan(hdi[0], hdi[1], alpha=0.1, color=\"mediumpurple\")\n\n    # 3. ROPE を塗りつぶしてプロット\n    ax.axvspan(ROPE_RANGE[0], ROPE_RANGE[1], alpha=0.2, color=\"orange\",\n               label=f\"ROPE ({ROPE_RANGE[0]} to {ROPE_RANGE[1]} ms)\")\n\n    # 4. 参照戦（ゼロ）\n    ax.axvline(0, color=\"black\", linestyle=\":\", linewidth=1, label=\"No Effect (0)\")\n\n    # 5. タイトルと判定ラベル\n    mean_diff = np.mean(data_page)\n\n    # 判定ロジック\n    if hdi[0] &gt; ROPE_RANGE[1]:\n        verdict = \"明確な改善\"\n        verdict_color = \"seagreen\"\n    elif hdi[1] &lt; ROPE_RANGE[0]:\n        verdict = \"明確な悪化\"\n        verdict_color = \"indianred\"\n    elif hdi[0] &gt; ROPE_RANGE[0] and hdi[1] &lt; ROPE_RANGE[1]:\n        verdict = \"実質的に等価\"\n        verdict_color = \"slategray\"\n    else:\n        verdict = \"判断保留\"\n        verdict_color = \"goldenrod\"\n\n    ax.set_title(f\"{page} Page\\nMean: {mean_diff:.0f} ms | {verdict}\", fontsize=12, color=verdict_color,\n                 fontweight=\"bold\")\n    ax.set_xlabel(\"LCP Change (Pre - Post) [ms]\")\n    ax.set_ylabel(\"Density\")\n    ax.legend(loc=\"upper right\", fontsize=8)\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle(\"ROPE Analysis: HDI vs. ROPE Overlap\", fontsize=16, fontweight=\"bold\")\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_427225/221660050.py:54: UserWarning: Glyph 21028 (\\N{CJK UNIFIED IDEOGRAPH-5224}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/tmp/ipykernel_427225/221660050.py:54: UserWarning: Glyph 26029 (\\N{CJK UNIFIED IDEOGRAPH-65AD}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/tmp/ipykernel_427225/221660050.py:54: UserWarning: Glyph 20445 (\\N{CJK UNIFIED IDEOGRAPH-4FDD}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/tmp/ipykernel_427225/221660050.py:54: UserWarning: Glyph 30041 (\\N{CJK UNIFIED IDEOGRAPH-7559}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 21028 (\\N{CJK UNIFIED IDEOGRAPH-5224}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 26029 (\\N{CJK UNIFIED IDEOGRAPH-65AD}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 20445 (\\N{CJK UNIFIED IDEOGRAPH-4FDD}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 30041 (\\N{CJK UNIFIED IDEOGRAPH-7559}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\n\nROPE 分析結果の解釈\n\nROPE の意味 ROPE（Region of Practical Equivalence）は、実際の業務において差異が顕著に現れる範囲を指します。 この範囲内で変化が認められるかどうかは、ビジネスのコンテキストや User の体験に大きく左右されます。\n\n\n分析結果の解釈\n今回の分析では、「±100ms以内の変化は、ユーザーにとって体感できない（実質的に同等）」という閾値を設定しました。 これは、人間の知覚限界（JND: Just Noticeable Difference）を考慮した、実務的な「ノイズフィルター」として機能します。 Googleの「RAILモデル」においても、100ms以内の応答はユーザーに「即座」に感じさせると定義されています。 そのため、100ms以下の変化は「実質的に同じ（Practical Equivalence）」とみなす考え方が合理的です。\n\n\n\n事後分布の HDI が ROPE とどの程度重なっているか可視化\n\n\n3つの判定結果が一目でわかる HDIとROPEの重なり方によって、以下の3つの結論を視覚的に即座に判断できます。\n\n\n\n\n\n\n\n\n\n\nHDIとROPEの関係\n判定\n図のイメージ\n\n\n\n\nHDI全体がROPEの外側（右）\n✅ 明確な改善\n[—ROPE—]…..[===HDI===]\n\n\nHDI全体がROPEの外側（左）\n❌ 明確な悪化\n[===HDI===]…..[—ROPE—]\n\n\nHDI全体がROPEの内側\n➖ 実質的に等価（変化なし）\n[–[=HDI=]–] (ROPE内にHDI)\n\n\nHDIがROPEと部分的に重なる\n⚠️ 判断保留（データ不足）\n[—RO[==HDI==]PE—]\n\n\n\n\n\n不確実性の大きさが伝わる\n\n\nHDIが狭い: 推定に自信がある。\nHDIが広い: データが少なく、推定がまだ曖昧。\n\nHDIの「幅」は、推定の不確実性（データの少なさ、ばらつき）を反映します。 これにより、「改善しているが、まだ確証はない」といったニュアンスを経営層に伝えることができます。\n\n\n\nROPE分析結果の解釈\n可視化の読み方（凡例）\n\n\n\n\n\n\n\n要素\n意味\n\n\n\n\n紫の曲線・網掛け\n改善量（Pre - Post）の事後分布と94% HDI\n\n\n黄色の網掛け\nROPE（±100ms）：この範囲内の変化は「体感できない」とみなす\n\n\n灰色の点線\nゼロライン（変化なし）\n\n\n\n\n各ページの判定結果\n\nTop ページ：✅ 明確な改善 観察結果:\n\n事後分布（紫）が、ROPE（黄色）の右側に完全に位置している\n94% HDI の下限が ROPE の上限（+100ms）を超えている\n平均改善量は約 +500ms 前後\n\n解釈: TopページのLCPは、施策後にユーザーが確実に体感できるレベルで高速化しました。 94%の確信度で「100ms以上の改善がある」と言えます。\n【直感】: 「速くなったかも？」ではなく、「間違いなく速くなった」と自信を持って言える状態です。\nDetail ページ：✅ 明確な改善（または改善傾向） 観察結果:\n\n事後分布（紫）の大部分が ROPE の右側に位置\n94% HDI がほぼ ROPE 外にある（わずかに重なる可能性あり）\n平均改善量は約 +200ms 前後\n\n解釈: Detailページも体感可能な改善が認められます。 Topほどの確信度ではないものの、80〜95%の確率で体感できる改善があります。\n【直感】: 「おそらく速くなった」と言える状態。追加データがあれば確信が強まります。\nContract ページ：➖ 実質的に等価（変化なし） 観察結果:\n\n事後分布（紫）が ROPE（黄色）の内部にほぼ収まっている\n94% HDI の両端が ROPE 内に含まれている\n平均改善量は約 ±0〜50ms 程度\n\n解釈: Contractページは、統計的にわずかな差があったとしても、ユーザーにとっては「変わっていない」と判断できます。 この施策による影響は実質ゼロです。\n【直感】: 「良くも悪くもなっていない」中立の状態。このページへの追加対応は不要です。\nCheckout ページ：❌ 明確な悪化（Blocker） 観察結果:\n\n事後分布（紫）が、ROPE（黄色）の左側に完全に位置している\n94% HDI の上限が ROPE の下限（-100ms）を下回っている\n平均悪化量は約 -1000ms（約1秒） 前後\n\n解釈: Checkoutページは、施策後にユーザーが確実に体感できるレベルで遅延しています。 94%の確信度で「100ms以上の悪化がある」と言え、実際には約1秒もの遅延が発生しています。\n【直感】: 「遅くなったかも？」ではなく、「間違いなく遅くなった」という、リリースをブロックすべき明確なシグナルです。\n\n\n\n結果サマリー\n\n\n\nページ\nHDIとROPEの関係\n判定\nアクション\n\n\n\n\nTop\nHDI全体がROPE右側\n✅ 明確な改善\nリリース推奨\n\n\nDetail\nHDIの大部分がROPE右側\n✅ 改善傾向\nリリース推奨\n\n\nContract\nHDI全体がROPE内\n➖ 等価（変化なし）\n対応不要\n\n\nCheckout\nHDI全体がROPE左側\n❌ 明確な悪化\nブロッカー\n\n\n\n\n\n意思決定への示唆\nこの可視化から導かれる結論は以下の通りです。 ROPE判定の総合結論: 今回の施策は、Top・Detailページにおいてユーザー体感レベルの明確な改善をもたらした一方、 Checkoutページにおいて約1秒の致命的な遅延を引き起こしている。Checkoutページの悪化は、94% HDI が ROPE を完全に下回っており、 統計的にも実務的にも「許容不可」 と判断される。\n\n\n\n\nROI-シミュレーション (経済的判断)\n\n# --- Business Parameters ---\nn_samples = ms_diff.shape[0]  # サンプル数を取得\n\n# 基本パラメータ\nPV_MONTHLY = np.array([1_000_000, 100_000, 10_000, 1_000])\nAOV = 5_000\nCOST_IMPLEMENTATION = 100_000\n\n# 感度(Sensitivity) を (N_sample, 4) の行列として生成\nmatrix_sensitivity = np.tile([0.0001, 0.0005, 0.0050, 0.0000], (n_samples, 1))  # 固定値で初期化\n\n# 【重要】Checkout (Index 3) の感度を「幅のある分布」として生成\n# \"0.04〜0.06の間でばらつく\" -&gt; 一様分布 (Uniform Distribution) を採用\n# ※ より確信がある場合は正規分布 rng.normal(loc=0.05, scale=0.005, size=n_samples) なども可\nsensitivity_checkout_dist = rng.uniform(low=0.04, high=0.06, size=n_samples)\nmatrix_sensitivity[:, 3] = sensitivity_checkout_dist\n\n# 1ms あたりの価値も分布になる（N_samples, 4）\nvalue_per_ms_dist = matrix_sensitivity * PV_MONTHLY * AOV\n\n\nビジネスパラメータの定義：固定値と不確実性の統合\nROIシミュレーションを行うために、ビジネス上の「固定パラメータ」と、リスクを考慮するための「確率的パラメータ（不確実性）」をそれぞれ定義します。\n\n1. 固定ビジネスパラメータ（Business Constants）\nこれらは、アクセス解析や財務データから明確に値が決まっているパラメータです。\n\nPV_MONTHLY（月間ページビュー）\n\n【定義】: 各ページタイプの1ヶ月あたりのアクセス数です。\n\nTop (1,000,000): 圧倒的にアクセスが多い場所です。\nCheckout (1,000): 全体の0.1%しか到達しない、貴重な購入直前のステップです。\n\n【直感】: 「影響が及ぶ人数」です。Topページは改善の幅が小さくても、人数が多いため塵も積もれば山となります。逆にCheckoutページは人数が少ないため、ここ単体の平均値だけを見ていても全体の数字は動きにくいという性質があります。\n\nAOV（平均客単価）\n\n【定義】: Average Order Valueの略で、1回の購入あたりの平均売上金額（ここでは5,000円）です。\n【直感】: 「1回の成功の重み」です。CVRが上がった結果、1件の成約が増えるごとにいくら売上が増えるのかを計算するために使用します。\n\nCOST_IMPLEMENTATION（実装コスト）\n\n【定義】: パフォーマンス改善施策（エンジニアの工数やツール費用など）にかかった一時的な費用です（ここでは10万円）。\n【直感】: 「この施策にかかった投資額」です。施策による売上増がこのコストを上回らなければ、ビジネスとしては「赤字」と判断されます。\n\n\n\n\n\n2. 確率的パラメータ（Probabilistic Parameters for Sensitivity Analysis）\nここでは「売上感度」を固定値ではなく確率分布として扱うことで、前提条件がズレていた場合のリスクを織り込みます。\n\nsensitivity_checkout_dist（Checkoutページの感度分布）\n\n【定義】: Checkoutページの売上感度を、固定値ではなく 一様分布 Uniform(0.04, 0.06) に従う確率変数として定義します。\n【直感】: 「感度はだいたい0.05%くらいだが、最悪0.06%（敏感）、良くても0.04%（鈍感）の範囲のどこかにある」という、分析者の “迷い” や “想定の幅” をそのままモデルに反映させています。\n\nmatrix_sensitivity（感度行列）\n\n【定義】: MCMCのサンプル数に対応した (N_samples, 4) の行列です。Topページなどは固定値ですが、Checkout列には上記の確率分布（ばらつき）が格納されています。\n【直感】: 「もしもの世界」のカタログです。「ある世界では客が怒りやすく（感度高）、別の世界では寛容（感度低）」といった何千通りものパラレルワールドを表現しています。\n\nvalue_per_ms_dist（確率的な1msの価値）\n\n【定義】: 感度の不確実性を反映した、1msあたりの金銭的価値の分布です。 \\[Value\\_per\\_ms \\sim PV \\times Sensitivity(\\text{分布}) \\times AOV\\]\n【直感】: これまでは「1ms遅れると250円の損失」と決め打ちしていましたが、ここでは「200円〜300円の間のどこか」というように、損失単価自体が揺れ動く幅を持っています。これにより、「感度が高く、かつ速度も遅くなった」という最悪のシナリオ（ワーストケース）を含めた評価が可能になります。\n\n\n\n# --- ROI Calculation with Sensitivity Analysis ---\n\n# ページごとの損益インパクト (円)\n# ms_diff (速度の不確実性) × value_per_ms_dist (ビジネス感度の不確実性)\nimpact_per_page_prob = ms_diff * value_per_ms_dist\n\n# トータルの純利益分布\ntotal_revenue_uplift = np.sum(impact_per_page_prob, axis=1)\nprofit_net_prob = total_revenue_uplift - COST_IMPLEMENTATION\n\n\n# --- Visualization ---\nplt.figure(figsize=(12, 6))\n\n# Plot 01: Total Net Profit (Sensitivity Included)\nplt.subplot(1, 2, 1)\nplt.hist(profit_net_prob, bins=50, alpha=0.7, density=True, label=\"Net Profit Dist\")\nplt.axvline(0, linestyle=\"--\", linewidth=2, color=COLOR_YELLOW, label=\"Break-even\")\nplt.title(\"Total ROI Distribution\\n(w/ Sensitivity Uncertainty)\")\nplt.xlabel(\"Net Profit (JPY)\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nROI分布の可視化：感度の不確実性を含めた純利益シミュレーション\nこのプロットは、今回の施策がもたらす純利益（Net Profit）の確率分布を示しています。\n\nプロットの構成要素\n\n\n\n\n\n\n\n要素\n意味\n\n\n\n\nヒストグラム（青/紫）\nMCMCサンプルから計算された純利益の分布。横軸が利益額（円）、縦軸が確率密度。\n\n\n黄色の破線（Break-even）\n損益分岐点（0円）。この線より右が黒字、左が赤字を意味する。\n\n\n\n\n\n読み取り方\n\n分布の中心（山の位置）: 期待される純利益の「最も起こりやすい値」を示します。\n分布の幅（山の広がり）: 利益予測の不確実性を表します。幅が広いほど「振れ幅が大きい」ことを意味します。\n0円ラインとの重なり: 分布のどの程度が0円より左（赤字領域）にあるかで、赤字リスクを視覚的に把握できます。\n\n\n\n解釈のポイント\n\n分布全体が0より右にある場合: 高い確率で黒字。施策は経済的に成功と判断できます。\n分布が0を跨いでいる場合: 黒字になる可能性も赤字になる可能性もある状態。リスクの定量化が必要です。\n分布全体が0より左にある場合: 高い確率で赤字。施策の見直しが必要です。\n\n\n\n技術的補足\n\n【定義】: 感度の不確実性 (Sensitivity Uncertainty)\n「1msの改善がCVRを何%向上させるか」という売上感度（Sensitivity）自体にも不確実性があります。このシミュレーションでは、感度パラメータにも確率分布を仮定し、LCP改善量の不確実性と同時に考慮しています。これにより、単一の点推定ではなく、より現実的なリスク評価が可能になります。\n\n\n【直感】\n「100ms速くなったら売上が1%上がる」と決め打ちするのではなく、「0.5%〜1.5%くらいの幅で上がるかもしれない」という幅を持たせて計算しています。これにより、「最悪のケースでも黒字か？」「最良のケースでどこまで伸びるか？」といった問いに答えられます。\n\n\n# Plot 2: Checkout Page Impact Only\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 2)\nidx_checkout = list(pages).index(\"Checkout\")\nplt.hist(impact_per_page_prob[:, idx_checkout], bins=50, color=COLOR_RED, alpha=0.7, density=True)\nplt.axvline(0, color=\"black\", linestyle=\"--\", linewidth=2)\nplt.title(\"Checkout Page Imapct Risk\\n(Sensitivity: 0.04~0.06)\")\nplt.xlabel(\"Revenue Change (JPY)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCheckout ページ売上インパクトリスク分布の可視化\nこのプロットは、Checkout ページ単体が施策によってどれだけの売上影響を受けるかを確率分布として可視化したものです。\n\nプロットの構成要素\n\n\n\n\n\n\n\n要素\n意味\n\n\n\n\nヒストグラム（赤色）\nMCMCサンプルから計算されたCheckoutページの売上変動分布。横軸が収益変動額（円）、縦軸が確率密度。\n\n\n黒い破線（0円ライン）\n損益分岐点。この線より右が収益増、左が収益減（損失）を意味する。\n\n\nタイトルの「Sensitivity: 0.04~0.06」\nCheckoutページの売上感度を固定値ではなく、0.04〜0.06の一様分布として不確実性を織り込んでいることを示す。\n\n\n\n\n\n読み取り方\n\n分布の中心（山の位置）: Checkoutページにおける期待される売上変動額を示します。\n分布の幅（山の広がり）: LCP変動の不確実性と売上感度の不確実性を同時に反映しており、予測の振れ幅を表します。\n0円ラインとの位置関係: 分布全体が0より左（マイナス領域）にあるほど、このページが収益に対してリスク要因であることを示します。\n\n\n\n解釈のポイント\n\n分布が0より完全に左にある場合: Checkoutページは明確な損失源であり、施策全体の黒字を帳消しにする可能性があります。\n分布が0を跨いでいる場合: 損失になる確率と利益になる確率が混在しており、追加検証が必要です。\nタイトルに「Imapct Risk」とある理由: 売上直結のCheckoutページでの遅延は、PVあたりの損失単価が極めて高いため、全体のROIを左右するリスクファクターとして特別に可視化しています。\n\n\n\n技術的補足\n\n【定義】: 感度の確率的モデリング\n通常のROI計算では「1msあたりの売上影響 = 固定値」として計算しますが、このシミュレーションでは Checkout ページの感度を Uniform(0.04, 0.06) の一様分布として定義しています。これにより、「感度が予想より高かった場合」と「低かった場合」の両方のシナリオを同時に評価しています。\n\n\n【直感】\n「1秒遅れると5%離脱」という前提が、実は「4%〜6%の幅のどこかにある」という不確かさを認め、最悪ケースでどれくらいの損失が出るかを可視化したものです。これにより、単なる期待値ではなくリスクの下限（ワーストケース）を意思決定に組み込むことができます。\n\n\n\nビジネス上の示唆\nこのプロットから得られる重要な洞察は、「全体のROIがプラスでも、Checkoutページ単体で見ると大きな損失が発生している可能性がある」という点です。特にECサイトにおいて、決済直前のパフォーマンス劣化は「カゴ落ち」を招き、LTV（顧客生涯価値）の低下やブランド毀損につながる取り返しのつかない損失となるリスクがあります。\n\n# --- 4. 意思決定指標 (Decision Metric) ---\nmean_profit = np.mean(profit_net_prob)\nrate_win = np.mean(profit_net_prob &gt; 0)\n\n# 感度が下振れ(0.04)した場合も含めた、Checkout ページの損失リスク\nrisk_checkout_loss = np.mean(impact_per_page_prob[:, idx_checkout] &lt; -10_000)\n\nprint(\"=== Sensitivity Analysis Result ===\")\nprint(\"Checkout Sensitivity: Uniform(0.04, 0.06)\")\nprint(f\"Expected Net Profit: {mean_profit:,.0f} JPY\")\nprint(f\"Win Rate: (Profit &gt; 0): {rate_win * 100:.1f}%\")\nprint(f\"Checkout Risk (Loss &gt; 10k): {risk_checkout_loss * 100:.1f}%\")\n\n=== Sensitivity Analysis Result ===\nCheckout Sensitivity: Uniform(0.04, 0.06)\nExpected Net Profit: 82,099,757 JPY\nWin Rate: (Profit &gt; 0): 65.1%\nCheckout Risk (Loss &gt; 10k): 77.4%\n\n\n\n\n\n結論と推奨アクション\n\nROI シミュレーション結果サマリー\n\n\n\n\n\n\n\n\n指標\n値\n解釈\n\n\n\n\n期待純利益 (Expected Net Profit)\n約 8,210 万円\n平均的には大幅な黒字が見込める\n\n\n勝率 (Win Rate: Profit &gt; 0)\n65.1%\n約3回に1回は赤字になるリスクあり\n\n\nCheckout 損失リスク (Loss &gt; 1万円)\n77.4%\n⚠️ 極めて高いリスク\n\n\n\n\n\n\n\n分析結果の解釈\n今回の感度分析（Sensitivity: 0.04〜0.06）を含めた ROI シミュレーションにより、以下の重要な事実が判明しました。\n\n✅ 良い点：全体 ROI の期待値はプラス\n\n期待純利益は約 8,200 万円 と、実装コスト（10万円）を大幅に上回る。\nTop ページと Detail ページの改善効果が、金額ベースで大きく貢献している。\n\n\n\n❌ 問題点：Checkout ページが致命的なリスク要因\n\n勝率 65.1% は、一見すると「勝ち越し」に見えるが、約35%の確率で赤字になる。\nCheckout ページ単体で 1万円以上の損失が発生する確率が 77.4% と極めて高い。\nこれは、Checkout ページの約1秒の遅延が、高感度（4〜6%/100ms）と掛け合わさることで、他ページの改善効果を打ち消しうることを意味する。\n\n\n\n\n\n意思決定のロジック\n\n「全体で黒字なら良い」という判断は危険である。\n\nCheckout ページの体験悪化は、今回のモデルに含まれていない以下の 長期的・間接的な損失 を招く可能性がある：\n\nカゴ落ち率の上昇: 決済直前の離脱は、最も機会損失が大きい。\nブランド毀損: 「このサイトは決済が遅い」という口コミ・記憶が残る。\nLTV（顧客生涯価値）の低下: 一度悪い体験をしたユーザーは戻ってこない。\n\n\n\n\n推奨アクション\n\n判定: 条件付きリリース承認（Blocker あり）\n「全体の ROI 期待値は約 8,200 万円のプラスですが、Checkout ページのパフォーマンス劣化が 許容不可（Blocker） です。\n承認条件: 1. Top ページ・Detail ページの改善コードのみをマージ 2. Checkout ページに影響する変更はロールバック（または修正） 3. リリース後、Checkout ページの LCP を 1週間モニタリング し、悪化が見られた場合は即時ロールバック\n部分リリースにより、改善効果の大部分を享受しつつ、致命的なリスクを回避します。」\n\n\n\n\n技術的補足\n\n【定義】: 条件付きリリース (Conditional Release)\n施策の一部のみを本番環境に適用し、リスクの高い部分は除外または修正後に再評価する手法。A/B テストと組み合わせることで、さらにリスクを低減できる。\n\n\n【直感】\n「料理で例えると、全体としては美味しいコース料理でも、1品だけ食中毒のリスクがある食材が入っていたら、その1品だけ外してお客様に出す」という判断です。\n\n\n\n\n次のステップ (Next Step)\n\n\n\n優先度\nアクション\n担当\n\n\n\n\n🔴 高\nCheckout ページの遅延原因を特定・修正\nエンジニア\n\n\n🟡 中\n修正後、N=20 以上のデータで再分析\nデータサイエンティスト\n\n\n🟢 低\n売上感度パラメータの精緻化（実データ検証）\nマーケティング",
    "crumbs": [
      "Web Performance ROI Analysis",
      "エグゼクティブサマリー"
    ]
  },
  {
    "objectID": "notebooks/03_Sales_forecast_by_market_location/hierarchical_baysian_linear_regression.html",
    "href": "notebooks/03_Sales_forecast_by_market_location/hierarchical_baysian_linear_regression.html",
    "title": "階層ベイズ線形回帰によるマルシェ出店地点別の売上予測",
    "section": "",
    "text": "過去12件の出店データに基づき、階層ベイズモデルを用いて地点ごとの - 売上ポテンシャル - 天候リスク\nを分離・評価しました。単純な平均値では判断できない「不確実性（リスク）」を可視化 し、ROI（投資対効果）に基づいた戦略的な判断材料を提供します。"
  },
  {
    "objectID": "notebooks/03_Sales_forecast_by_market_location/hierarchical_baysian_linear_regression.html#エグゼクティブサマリー",
    "href": "notebooks/03_Sales_forecast_by_market_location/hierarchical_baysian_linear_regression.html#エグゼクティブサマリー",
    "title": "階層ベイズ線形回帰によるマルシェ出店地点別の売上予測",
    "section": "",
    "text": "過去12件の出店データに基づき、階層ベイズモデルを用いて地点ごとの - 売上ポテンシャル - 天候リスク\nを分離・評価しました。単純な平均値では判断できない「不確実性（リスク）」を可視化 し、ROI（投資対効果）に基づいた戦略的な判断材料を提供します。"
  },
  {
    "objectID": "notebooks/03_Sales_forecast_by_market_location/hierarchical_baysian_linear_regression.html#事前準備",
    "href": "notebooks/03_Sales_forecast_by_market_location/hierarchical_baysian_linear_regression.html#事前準備",
    "title": "階層ベイズ線形回帰によるマルシェ出店地点別の売上予測",
    "section": "事前準備",
    "text": "事前準備\n確率モデルの構築やデータ分析に必要なライブラリのインポートや独自カラー定義を行います。\n\n# 必要なライブラリの読み込み\nimport os\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\nimport arviz as az\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 不要な出力をしないように制御\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\n# プロジェクト共通のカラー定義\nCOLOR_PURPLE = \"#985DE5\"  # 事後分布・HDI\nCOLOR_YELLOW = \"#F9C74F\"  # ROPE 領域\nCOLOR_GREEN  = \"#06D6A0\"  # 改善判定\nCOLOR_RED    = \"#EF476F\"  # 悪化判定\nCOLOR_GRAY   = \"#8D99AE\"  # 等価判定・参照線\n\nplt.rcdefaults()  # plt の現在のカラー定義をリセット\npalette_brand = [COLOR_PURPLE, COLOR_YELLOW, COLOR_GREEN, COLOR_RED, COLOR_GRAY]\n\nsns.set_theme(style=\"whitegrid\", palette=palette_brand)\nplt.rcParams[\"axes.prop_cycle\"] = plt.cycler(color=palette_brand)\n\nprint(f\"Brand Style Applied: The visual identity was applied.\")\n\nBrand Style Applied: The visual identity was applied.\n\n\n\nTips:\n分析結果のビジュアルを統一し、どのグラフを見ても「紫は実力、緑は成功」と直感的に理解できるようにしています。 Color Cycle グラフを描画する際、色が指定されていない場合に自動的に適用される色の順番です。 最初は紫、次を黄色…と決めておくことで、毎回指定しなくても色が揃います。\nビジネスレポートにおいて「色の意味」を固定することは、意思決定のスピードを高めます。 複数の Notebook を管理する実務現場では、これを共通モジュール化し、メンテナンス姓を高めることを推奨します。\n１番大事なことですが、自分のお気に入りの配色にするとバイブスが上がります。"
  },
  {
    "objectID": "notebooks/03_Sales_forecast_by_market_location/hierarchical_baysian_linear_regression.html#データの準備とホワイトボックス化",
    "href": "notebooks/03_Sales_forecast_by_market_location/hierarchical_baysian_linear_regression.html#データの準備とホワイトボックス化",
    "title": "階層ベイズ線形回帰によるマルシェ出店地点別の売上予測",
    "section": "データの準備とホワイトボックス化",
    "text": "データの準備とホワイトボックス化\n分析の原材料となるデータの読み込み、集計可能な数値型へ変換します。 今回は、１年分の簡単なデータが CSV で記録してあった、とします。\n\n# データの読み込み（外部データより読み込み）\nPATH_ROW_DATA = \"sales_performance.csv\"\n\ndf_raw = pd.read_csv(PATH_ROW_DATA)\ndf_raw\n\n\n\n\n\n\n\n\nOpening_DateTime\nLocation\nWeather\nSales(sales_obs)\nFixed_Costs\nCommission_Rate\n\n\n\n\n0\n2025-04-12 08:00-17:00\nA: StationSquare\nSunny\n73,900\n45,000\n0.10\n\n\n1\n2025-06-20 08:00-17:00\nA: StationSquare\nrain\n17,100\n45,000\n0.10\n\n\n2\n2025-12-15 08:00-17:00\nA: StationSquare\nCloudy\n38,500\n45,000\n0.10\n\n\n3\n2025-05-10 08:00-17:00\nB: ParkStreet\nrain\n17,300\n30,000\n0.05\n\n\n4\n2025-08-05 08:00-17:00\nB: ParkStreet\nSunny\n84,000\n30,000\n0.05\n\n\n5\n2025-10-25 08:00-17:00\nB: ParkStreet\nCloudy\n40,500\n30,000\n0.05\n\n\n6\n2025-07-15 08:00-17:00\nC: Shopping mall\nSunny\n90,500\n55,000\n0.15\n\n\n7\n2025-11-02 08:00-17:00\nC: Shopping mall\nrain\n14,400\n55,000\n0.15\n\n\n8\n2025-01-20 08:00-17:00\nC: Shopping mall\nCloudy\n39,100\n55,000\n0.15\n\n\n9\n2025-03-25 08:00-17:00\nD: TempleGrounds\nCloudy\n51,700\n35,000\n0.05\n\n\n10\n2025-09-12 08:00-17:00\nD: TempleGrounds\nSunny\n72,300\n35,000\n0.05\n\n\n11\n2025-02-10 08:00-17:00\nD: TempleGrounds\nrain\n13,600\n35,000\n0.05\n\n\n\n\n\n\n\n\n# 数値変換のクレンジング\ndf = pd.DataFrame()\ndf[\"sales_obs\"] = df_raw[\"Sales(sales_obs)\"].str.replace(\",\", \"\").astype(float)\ndf[\"cost_fixed\"] = df_raw[\"Fixed_Costs\"].str.replace(\",\", \"\").astype(float)\ndf[\"rate_commission\"] = df_raw[\"Commission_Rate\"]\n\n# 読み込み直後のデータ確認\ndf\n\n\n\n\n\n\n\n\nsales_obs\ncost_fixed\nrate_commission\n\n\n\n\n0\n73900.0\n45000.0\n0.10\n\n\n1\n17100.0\n45000.0\n0.10\n\n\n2\n38500.0\n45000.0\n0.10\n\n\n3\n17300.0\n30000.0\n0.05\n\n\n4\n84000.0\n30000.0\n0.05\n\n\n5\n40500.0\n30000.0\n0.05\n\n\n6\n90500.0\n55000.0\n0.15\n\n\n7\n14400.0\n55000.0\n0.15\n\n\n8\n39100.0\n55000.0\n0.15\n\n\n9\n51700.0\n35000.0\n0.05\n\n\n10\n72300.0\n35000.0\n0.05\n\n\n11\n13600.0\n35000.0\n0.05\n\n\n\n\n\n\n\nデータクレンジングとは、カンマや型不整合などのノイズを取り除き、統計モデルが計算できる形に 整える工程です。\n\ntips:\n生データのことを Raw Data と言います。よって df_raw という変数名にしています。\nデータを raw のまま保持することには以下のメリットがあります。 1. 再現性の確保 - 前処理のコード（洗浄工程）に間違いが見つかった際、いつでも初期状態の df_raw からやり直すことができます。 2. ベイズ推論への橋渡し - PyMC5 での実装では、文字列（場所など）を数値インデックス（0, 1, 2, …）に変換する工程が必須です。 df_raw を残しておくことで、「インデックス０番はどの場所だったか？」という対応関係（Coords / Dims）をいつでも参照できます。\n\n\ndf_raw.describe()\n\n\n\n\n\n\n\n\nCommission_Rate\n\n\n\n\ncount\n12.000000\n\n\nmean\n0.087500\n\n\nstd\n0.043301\n\n\nmin\n0.050000\n\n\n25%\n0.050000\n\n\n50%\n0.075000\n\n\n75%\n0.112500\n\n\nmax\n0.150000"
  },
  {
    "objectID": "notebooks/03_Sales_forecast_by_market_location/hierarchical_baysian_linear_regression.html#特徴量生成とインデックス化",
    "href": "notebooks/03_Sales_forecast_by_market_location/hierarchical_baysian_linear_regression.html#特徴量生成とインデックス化",
    "title": "階層ベイズ線形回帰によるマルシェ出店地点別の売上予測",
    "section": "特徴量生成とインデックス化",
    "text": "特徴量生成とインデックス化\n「場所」や「天気」といった文字情報を、数式で扱える「背番号（インデックス）」に変換します。 また、日付から「季節」という売上に影響する重要な文脈を抽出します。\n\n# 季節情報の抽出\ndf[\"opening_datetime\"] = pd.to_datetime(df_raw[\"Opening_DateTime\"].str.split(\" \").str[0])\ndf[\"idx_month\"] = df[\"opening_datetime\"].dt.month\n\n\ndef define_season(month: int) -&gt; str:\n    \"\"\"\n    与えられた月に基づいて季節を決定します。この関数は入力された月を評価し、春、夏、秋、冬の4つの季節のいずれかに分類します。\n\n    :param month: 整数で表された月 (1 月は 1、2 月は 2、...、12 月は 12)。\n    :type month: int\n    :return: 指定された月に対応する季節の名前。\n    :rtype: str\n    \"\"\"\n    if month in [3, 4, 5]: return \"Spring\"\n    if month in [6, 7, 8]: return \"Summer\"\n    if month in [9, 10, 11]: return \"Autumn\"\n    return \"Winter\"\n\n\ndf[\"season_label\"] = df[\"idx_month\"].apply(define_season)\ndf\n\n\n\n\n\n\n\n\nsales_obs\ncost_fixed\nrate_commission\nopening_datetime\nidx_month\nseason_label\n\n\n\n\n0\n73900.0\n45000.0\n0.10\n2025-04-12\n4\nSpring\n\n\n1\n17100.0\n45000.0\n0.10\n2025-06-20\n6\nSummer\n\n\n2\n38500.0\n45000.0\n0.10\n2025-12-15\n12\nWinter\n\n\n3\n17300.0\n30000.0\n0.05\n2025-05-10\n5\nSpring\n\n\n4\n84000.0\n30000.0\n0.05\n2025-08-05\n8\nSummer\n\n\n5\n40500.0\n30000.0\n0.05\n2025-10-25\n10\nAutumn\n\n\n6\n90500.0\n55000.0\n0.15\n2025-07-15\n7\nSummer\n\n\n7\n14400.0\n55000.0\n0.15\n2025-11-02\n11\nAutumn\n\n\n8\n39100.0\n55000.0\n0.15\n2025-01-20\n1\nWinter\n\n\n9\n51700.0\n35000.0\n0.05\n2025-03-25\n3\nSpring\n\n\n10\n72300.0\n35000.0\n0.05\n2025-09-12\n9\nAutumn\n\n\n11\n13600.0\n35000.0\n0.05\n2025-02-10\n2\nWinter\n\n\n\n\n\n\n\n\n# カテゴリのインデックス化\ndf[\"idx_location\"], locations = pd.factorize(df_raw[\"Location\"])\ndf[\"idx_weather\"], weathers = pd.factorize(df_raw[\"Weather\"])\ndf[\"idx_season\"], seasons = pd.factorize(df[\"season_label\"])\n\npd.factorize() は、ユニークな文字列に対して一意の整数を割り当てるラベルエンコーディング手法です。\n\n# PyMC用座標定義\ncoords = {\n    \"location\": locations,\n    \"weather\": weathers,\n    \"season\": seasons,\n    \"id_obs\": np.arange(len(df))\n}\n\nprint(f\"Location Map: {list(enumerate(locations))}\")\nprint(f\"Weather Map: {list(enumerate(weathers))}\")\nprint(f\"Season Map: {list(enumerate(seasons))}\")\n\nLocation Map: [(0, 'A: StationSquare'), (1, 'B: ParkStreet'), (2, 'C: Shopping mall'), (3, 'D: TempleGrounds')]\nWeather Map: [(0, 'Sunny'), (1, 'rain'), (2, 'Cloudy')]\nSeason Map: [(0, 'Spring'), (1, 'Summer'), (2, 'Winter'), (3, 'Autumn')]\n\n\n「名前」ではなく「ID」で呼ぶようにします。あとで名簿（locations等）と突き合わせれば、誰が誰だか分かります。"
  },
  {
    "objectID": "notebooks/03_Sales_forecast_by_market_location/hierarchical_baysian_linear_regression.html#確率モデルの構築",
    "href": "notebooks/03_Sales_forecast_by_market_location/hierarchical_baysian_linear_regression.html#確率モデルの構築",
    "title": "階層ベイズ線形回帰によるマルシェ出店地点別の売上予測",
    "section": "確率モデルの構築",
    "text": "確率モデルの構築\n売上に影響を与える「立地」「天気」「季節」を独立した変数として扱い、それぞれの影響度をベイズ線形回帰の枠組みで推定します。\n\nwith pm.Model(coords=coords) as hierarchical_bayesian_linear_regression_model:\n    # 入力データ\n    idx_location = pm.Data(\"idx_location\", df[\"idx_location\"], dims=\"id_obs\")  # 各観測データに対応する地点ID\n    idx_weather = pm.Data(\"idx_weather\", df[\"idx_weather\"], dims=\"id_obs\")  # 各観測データに対応する天候ID\n    idx_season = pm.Data(\"idx_season\", df[\"idx_season\"], dims=\"id_obs\")  # 各観測データに対応する季節ID\n    y_obs = pm.Data(\"y_obs\", df[\"sales_obs\"], dims=\"id_obs\")  # モデルが学習する売上の実測値\n\n    # --- Priors（事前分布A）---\n    # 全体のベースライン（log scale）: e^11 ≒ 6万円\n    mu_global = pm.Normal(\"mu_global\", mu=11, sigma=2)  # 全地点を通じた平均売上のベースライン（Log単位）\n    sigma_global = pm.HalfNormal(\"sigma_global\", sigma=1)  # 地点間で「実力」がどれくらいバラつくかの標準偏差\n\n    # --- Hierarchical Structure (Non-centered) ---\n    # 地点ごとの偏差。各地点の力を共通分布からのズレとして定義\n    offset_location = pm.Normal(\"offset_location\", mu=0, sigma=1, dims=\"location\")  # 標準正規分布からの地点ごとのズレ（非中心化用）\n    alpha = pm.Deterministic(\"alpha\", mu_global + offset_location * sigma_global, dims=\"location\")  # 地点ごとの固有の売上実力\n\n    # --- Fixed Effects (回帰係数)---\n    beta_weather = pm.Normal(\"beta_weather\", mu=0, sigma=0.5, dims=\"weather\")  # 天候が売上を何%増減させるかの影響度（傾き）\n    beta_season = pm.Normal(\"beta_season\", mu=0, sigma=0.5, dims=\"season\")  # 季節が売上を何%増減させるかの影響度（傾き）\n\n    # --- Likelihood (尤度関数)---\n    # 対数スケールで線形予測子\n    mu = alpha[idx_location] + beta_weather[idx_weather] + beta_season[idx_season]  # 各要素を足し合わせた予測売上の中央値（log単位）\n    sigma = pm.HalfNormal(\"sigma\", sigma=1)  # モデルでは説明しきれない観測誤差 (ノイズ)\n\n    pm.LogNormal(\"sales\", mu=mu, sigma=sigma, observed=y_obs, dims=\"id_obs\")  # 売上の正値性と裾の長さを考慮した最終的な確率分布\n\n\n# モデルの構造の可視化\npm.model_to_graphviz(hierarchical_bayesian_linear_regression_model)\n\n\n\n\n\n\n\n\n\n結果の解釈\n\nビジネス上の意味\nこのモデルによって、 - 雨の日でも売上が落ちにくい地点 - 特定の季節に強い地点\nをデータが少ない中でもあぶりだそうとしています。季節ごとの人員配置や、天候による在庫リスク管理の精度を直接的に向上させ、無駄なコストを削減（ROIの向上）することに期待しています。\n\nTips\n非中心化パラメータ化 (Non-centered parameterization)\n階層モデルにおいて offset を介して間接的に個別のパラメータ (alpha) を定義しています。 立地の売上をそのまま予測するのではなく、「全体の売上平均」と「そこからの立地の偏差値（ズレ）」に分けて計算するようなのです。 データが少ない地点でも、平均からのズレとして測ることで、予測が極端に外れるのを防げます。\n\n\n\n統計的根拠と分布の選択\n\n対数正規分布 (LogNormal)\n売上データは「100万円の次は 120万円」といった「割合」で動くことが多く、またマイナスの売上は存在しません。そのため、対数をとると正規分布になる対数正規分布を選択しました。 - \\(\\mu \\sim Normal(11, 2^2)\\) &gt; 事前分布を最初から対数スケールの数値(11付近)で定義しておくことで、モデル全体が正しく「対数スケールの線形回帰」として機能します。 &gt; &gt; 売上実績データの sales_obs は、おおよそ 40,000円～100,000円の範囲にあります。これを自然対数 (ln) に変換すると以下のようになります。 &gt; - \\(\\ln(40,000) \\approx 10.6\\) &gt; - \\(\\ln(100,000) \\approx 11.5\\) &gt; &gt; コード内で mu_global の平均 (mu) を 11 に設定しているのは、「売上の中心が \\(e^{11} \\approx 60,000\\) 円くらいである」という事前知識を対数スケールで表現していることになります。 &gt; もし、ここを mu=60_000 としてしまうと、後の LogNormal 尤度 (Likelihood) と整合が取れず、計算が破綻します。\n\n\n事前分布の選択\nbeta_weather や beta_season に　Normal(0, 0.5) を指定しています。\n\n\\(\\beta_{\\text{weather}} \\sim Normal(0, 0.5^2)\\)\n\\(\\beta_{\\text{season}} \\sim Normal(0, 0.5^2)\\)\n\nこれは、対数スケールで ±0.5 (約0.6倍～1.6倍) 程度の変動が「もっともらしい」という、ビジネス感覚に近い弱情報事前分布 (Weakly Informative Prior) です。\n\n対数正規分布\n変数 \\(x\\) の対数 \\(ln(x)\\) が正規分布に従う確率分布。正の値のみを取り、右側に長い裾（ロングテール）を持つのが特徴です。 「掛け算で決まる世界」の分布、というイメージで今回のように「客数 × 客単価」のように要素が掛け合わさって決まるため、平均的な日よりも「たまに大爆発する日」が出やすくなります。 正規分布（足し算の世界）ではこの「大爆発」を予測できません。\n\n\n弱情報事前分布 (Weakly Informative Prior)\nベイズ統計において - 実務的な妥当性 - データの尊重\nを両立させるための非常に賢いテクニックです。\nパラメータが取り得る「物理的・実務的に妥当な範囲」を緩やかに指定する分布です。 モデルが計算不能な極端な値（異常な高値や負の値A）に迷い込むのを防ぎつつ、十分なデータがあればそのデータの結果を優先するように設計されます。\n「経験上、売上はこのくらいになるはずだ（常識）」という意見は持っていますが、新しいデータが来れば柔軟に意見を変えることができ、ちょうど良いバランスの姿勢を持っているイメージです。\n分析者が「データを分析する前」に持っている知識の強さを、３つのパターンで可視化してみます。今回の売上予測モデルで \\(Normal(11, 2^2)\\) を選んだ意図を明確にします。\n\n\n\n事前分布の比較シミュレーション\n\n\n今回のモデルで弱情報事前分布 を採用した理由は３つあります。 1. 正規化 (過学習の抑制): データが極端に少ない地点 (12件中の数件)でも、推定値が「無限大」や「ゼロ」に飛んでしまうのを防ぎます。 2. 計算の安定化: 完全にフラットな「無情報分布」を使うと、MCMCサンプリングが広大な範囲を探索しすぎて計算が終わらなくなることがありますが、弱情報事前分布は探索範囲を「現実的な場所」に絞り込みます。 3. 対数スケールへの適合: 売上を対数 (log) で扱う場合、平均 11　に対して標準偏差 2 という設定は、元データのスケールで「数千円～数千万円」という広い範囲をカバーします。これは「常識外れな予測はしないが、可能性は広く残す」という理想的な設定です。\nSMB市場のような「スモールデータ」の環境では、無情報分布 (何も知らない) はむしろ危険になります。データが少ないと、たまたま起きたノイズを「真実」だと誤認してしまいます。\n弱情報事前分布を組み込むことで、「過去の業界常識 (事前知識)」と「目の前の実績 (データ)」を数学的に正しく合算 (ベイズ更新) でき、少ないデータからでもリスクの低い投資判断を下すことが可能になります。\n\n\nTips\nもし「前回のキャンペーンでは売上が1.5倍になった」という確固たる過去の証拠がある場合は、弱情報から少し「強情報（SDを小さくする）」に寄せることで、より精度の高い予測が可能になります"
  },
  {
    "objectID": "notebooks/03_Sales_forecast_by_market_location/hierarchical_baysian_linear_regression.html#推論-mcmc-と診断",
    "href": "notebooks/03_Sales_forecast_by_market_location/hierarchical_baysian_linear_regression.html#推論-mcmc-と診断",
    "title": "階層ベイズ線形回帰によるマルシェ出店地点別の売上予測",
    "section": "推論 (MCMC) と診断",
    "text": "推論 (MCMC) と診断\n\nサンプル数が少ないこと\n構築したモデルが複雑であること\n\nを考えると一度の推論で発散せずにモデルが安定しない確度が高いです。 MCMCサンプリングは計算負荷が高いため 「成功した（発散がない）結果だけをキャッシュ（保存）し、次回はそれを再利用する」 という関数を組みます。\n\ndef run_inference_with_cache(path, model, draw, tune, chains, target_accept, random_seed):\n    if os.path.exists(path):\n        # 保存された .nc ファイルがある場合はロード\n        print(f\"Inference: Checked for existing inference results, Loading from {path} ...\")\n        inference = az.from_netcdf(path)\n        return inference\n    else:\n        # .nc ファイルがない場合のみ推論を実行\n        print(\"Inference: No inference results found, so we will start sampling. This may take a few minutes.\")\n        with model:\n            inference = pm.sample(draw=draw,  # 事後分布から取得・記録する有効なサンプル数\n                                  tune=tune,  # 調整用（バーンイン）として捨てる初期ステップ数\n                                  chains=chains,  # 独立した計算経路（鎖）を並行して回す数\n                                  target_accept=target_accept,  # アルゴリズムの慎重さ（高めるほど計算エラーを防ぐ）\n                                  random_seed=random_seed)  # 計算結果の再現性を確保するための乱数固定\n            # --- 発散 (Divergences) のチェック ---\n            # total_divergences = inference.sample_stats.divergiing の合計が 0 ならば「成功」とみなす\n            total_divergences = inference.sample_stats.diverging.sum().item()\n\n            if total_divergences == 0:\n                # 推論が成功 (発散がゼロ)した場合のみ保存\n                inference.to_netcdf(path)\n                print(f\"Inference: Sampling successful with zero divergences. saved to {path}\")\n                return inference\n            else:\n                # 発散が発生した場合は保存せずに警告を出す\n                print(f\"Warning: {total_divergences} divergences occurred during sampling.\")\n\n構築したモデルと関数で推論を行います。 まず、 - draw=2000 - tune=1000 - chains=4 - target_accept=0.95\nに設定して推論を行います。\n\n引数の設定値の根拠\nなぜ draws=2000 なのか？\nベイズ推定では、期待値だけでなく「裾野（リスク）」を評価することが重要です。 特に今回のような ROI（投資対効果） を算出する場合、2000サンプル程度確保することで、94% HDI（確信区間）の端の方まで安定して推定できるようになります。\nなぜ target_accept=0.95 （高め）なのか？\n今回のモデルは 「階層ベイズ（Hierarchical Model）」 です。 階層モデルは、地点ごとの個性が強い場合に「じょうご（Funnel）」のような非常に鋭い谷間を持つ地形になりやすく、標準設定（0.8）ではサンプリングが「発散（Divergence）」しやすくなります。 「慎重さ」を 0.95 まで高めることで、この鋭い谷間も取りこぼさずに探索させ、モデル診断におけるエラーを未然に防いでいます。\n\n\nPATH_INFERENCE = \"inference_hierarchical_bayesian_linear_regression_failed.nc\"  # 推論結果を保存するファイル名\n\n\ninference = run_inference_with_cache(PATH_INFERENCE,\n                                     hierarchical_bayesian_linear_regression_model,  # 構築したモデル\n                                     draw=2000,  # 事後分布から取得・記録する有効なサンプル数\n                                     tune=1000,  # 調整用（バーンイン）として捨てる初期ステップ数\n                                     chains=4,  # 独立した計算経路（鎖）を並行して回す数\n                                     target_accept=0.95,  # アルゴリズムの慎重さ（高めるほど計算エラーを防ぐ）\n                                     random_seed=42)  # 計算結果の再現性を確保するための乱数固定\n\nInference: No inference results found, so we will start sampling. This may take a few minutes.\n\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu_global, sigma_global, offset_location, beta_weather, beta_season, sigma]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 53 seconds.\nThere were 3 divergences after tuning. Increase `target_accept` or reparameterize.\n\n\nWarning: 3 divergences occurred during sampling.\n\n\n3件の発散 (Divergences) が発生しました。原因として - データが少な過ぎる - 坂道 (勾配) が急すぎてサンプラーが滑落している\nと推測されます。\n\\(N=12\\) という極小データで「地点・天候・季節」のすべてを推定しようとすると、事後分布の計上が非常に鋭い「じょうご型（Funnel）」になり、発散が置きやすくなります。 モデルの記述をより「堅牢」にしチューニングしてみます。\n\nDivergence (発散)\nハミルトニアンモンテカルロ法において、計算上のエネルギー保存則が破綻し、シミュレーションが無限遠へ飛んでしまう現象。 「計算上の足踏み外し」というイメージです。急な坂道（事後分布の鋭い谷）を大股で歩こうとして、バランスを崩して滑落してしまった状態を指します。\n\n\nESS (有効サンプルサイズ)が小さい\n自己相関を考慮した際、実質的に独立していると見なせるサンプル数です。 「意見の多様性」 のようなイメージで、仮に 2000回サンプリングしても、ESS が低いというとは「同じような似た意見ばかりが連続している」状態で、分布の全容 (真実) を捉え切れていないことを意味します。\n\n\n# モデルのチューニング\nwith pm.Model(coords=coords) as hierarchical_bayesian_linear_regression_model_refined:\n    # --- Data Contains ---\n    idx_location = pm.Data(\"idx_location\", df[\"idx_location\"], dims=\"id_obs\")\n    idx_weather = pm.Data(\"idx_weather\", df[\"idx_weather\"], dims=\"id_obs\")\n    idx_season = pm.Data(\"idx_season\", df[\"idx_season\"], dims=\"id_obs\")\n    y_obs = pm.Data(\"y_obs\", df[\"sales_obs\"], dims=\"id_obs\")\n\n    # --- Priors: 弱情報事前分布をさらに「タイト」にする ---\n    # sigma (標準偏差) が広すぎると、データ不足の時に発散の原因になります\n    mu_global = pm.Normal(\"mu_global\", mu=10.8, sigma=1)  # sigma を 2 から 1 へ (探索範囲を現実的に絞る)\n    sigma_global = pm.HalfNormal(\"sigma_global\", sigma=0.5)  # 地点間の差を「あり得ないほど大きく」しない\n\n    # --- Hierarchical (Non-centered) ---\n    offset_location = pm.Normal(\"offset_location\", mu=0, sigma=1, dims=\"location\")\n    alpha = pm.Deterministic(\"alpha\", mu_global + offset_location * sigma_global, dims=\"location\")\n\n    # --- Fixed Effects: 正規化を強める ---\n    # 天候や季節が「売上を３倍にする」といった極端な予測を抑制します\n    beta_weather = pm.Normal(\"beta_weather\", mu=0, sigma=1.0, dims=\"weather\")  # 天候の影響 (雨 = -1.2) を許容できるよう、sigma を 0.5 から 1.0 へ拡大\n    beta_season = pm.Normal(\"beta_season\", mu=0, sigma=0.5, dims=\"season\")\n\n    # --- Likelihood ---\n    mu = alpha[idx_location] + beta_weather[idx_weather] + beta_season[idx_season]  # 観測ノイズの許容範囲を絞る\n    sigma = pm.HalfNormal(\"sigma\", sigma=0.5)\n\n    pm.LogNormal(\"sales\", mu=mu, sigma=sigma, observed=y_obs, dims=\"id_obs\")\n\n今回の \\(N=12\\) という「極小データ」に対して、パラメータ数が多い（地点・天候・季節など）モデルは、統計的には 「オーバーフィッティング（過学習）」の一歩手前 にあります。\nサンプラーは、「どのパラメータが原因でこの売上になったのか？」という特定に迷い、事後分布の中に「非常に細くて深い谷（じょうご）」を作ってしまいます。 &gt;sigma の引き締め: 「地点差」や「天候効果」が現実離れした大きさにならないよう制限をかけることで、谷の深さを和らげます。\n####　発散をゼロに抑えることと、戦略的意思決定への関係 1. HDI (統計的実証): 発散がある状態での HDI は、端が切り捨てられた「不正確な予報」です。発散を消すことで、初めて正確な 94% HDI が算出できます。 2. ROPE (実質的等価性): サンプラーの足腰が安定して初めて、微細な誤差と実務的なインパクト（ROPE）を正しく切り分けられます。 3. ROI (経済的評価): ESSが低いと「稀に起こる大赤字」を見逃すリスクがあります。十分なサンプリングにより、「最悪のシナリオ」を含めた純利益分布を描き出すことができます。\n\n# 再度、推論\ninference = run_inference_with_cache(PATH_INFERENCE,\n                                     hierarchical_bayesian_linear_regression_model_refined,\n                                     draw=2000,\n                                     tune=2000,  # チューニング期間を倍増 (地形をじっくり学習させる)\n                                     chains=4,\n                                     target_accept=0.99,  # 0.95 から 0.99 へ (一歩を極限まで小さく、慎重にする)\n                                     random_seed=42\n                                     )\n\nInference: No inference results found, so we will start sampling. This may take a few minutes.\n\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu_global, sigma_global, offset_location, beta_weather, beta_season, sigma]\n\n\n\n\n\n\n\n\nSampling 4 chains for 2_000 tune and 1_000 draw iterations (8_000 + 4_000 draws total) took 185 seconds.\n\n\nInference: Sampling successful with zero divergences. saved to inference_hierarchical_bayesian_linear_regression_failed.nc\n\n\ntarget_accept=0.99: 慎重さを極限まで高めることで、細い谷をさらに慎重に歩くようにしました。 チューニングの結果、今回は無事に発散せずに推論できた様子です。推論データを可視化して診断しています。\n\ninference = az.from_netcdf(PATH_INFERENCE)\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 inference = az.from_netcdf(PATH_INFERENCE)\n\nFile ~/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/arviz/data/io_netcdf.py:36, in from_netcdf(filename, engine, group_kwargs, regex)\n     34 if group_kwargs is None:\n     35     group_kwargs = {}\n---&gt; 36 return InferenceData.from_netcdf(\n     37     filename, engine=engine, group_kwargs=group_kwargs, regex=regex\n     38 )\n\nFile ~/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/arviz/data/inference_data.py:456, in InferenceData.from_netcdf(filename, engine, group_kwargs, regex, base_group)\n    445 if err.errno == -101:\n    446     raise type(err)(\n    447         str(err)\n    448         + (\n   (...)    454         )\n    455     ) from err\n--&gt; 456 raise err\n\nFile ~/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/arviz/data/inference_data.py:411, in InferenceData.from_netcdf(filename, engine, group_kwargs, regex, base_group)\n    405     raise ValueError(\n    406         f\"Invalid value for engine: {engine}. Valid options are: h5netcdf or netcdf4\"\n    407     )\n    409 try:\n    410     with (\n--&gt; 411         h5netcdf.File(filename, mode=\"r\")\n    412         if engine == \"h5netcdf\"\n    413         else nc.Dataset(filename, mode=\"r\")\n    414     ) as file_handle:\n    415         if base_group == \"/\":\n    416             data = file_handle\n\nFile ~/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/h5netcdf/core.py:1660, in File.__init__(self, path, mode, format, invalid_netcdf, phony_dims, **kwargs)\n   1658         self._preexisting_file = os.path.exists(path) and mode != \"w\"\n   1659         self._h5py = h5py\n-&gt; 1660         self.__h5file = self._h5py.File(\n   1661             path, mode, track_order=track_order, **kwargs\n   1662         )\n   1663 elif isinstance(path, h5py.File):\n   1664     self._preexisting_file = mode in {\"r\", \"r+\", \"a\"}\n\nFile ~/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/h5py/_hl/files.py:566, in File.__init__(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, track_times, **kwds)\n    557     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n    558                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n    559                      alignment_threshold=alignment_threshold,\n    560                      alignment_interval=alignment_interval,\n    561                      meta_block_size=meta_block_size,\n    562                      **kwds)\n    563     fcpl = make_fcpl(track_order=track_order, track_times=track_times,\n    564                      fs_strategy=fs_strategy, fs_persist=fs_persist,\n    565                      fs_threshold=fs_threshold, fs_page_size=fs_page_size)\n--&gt; 566     fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n    568 if isinstance(libver, tuple):\n    569     self._libver = libver\n\nFile ~/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/h5py/_hl/files.py:241, in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)\n    239     if swmr and swmr_support:\n    240         flags |= h5f.ACC_SWMR_READ\n--&gt; 241     fid = h5f.open(name, flags, fapl=fapl)\n    242 elif mode == 'r+':\n    243     fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n\nFile h5py/_objects.pyx:54, in h5py._objects.with_phil.wrapper()\n\nFile h5py/_objects.pyx:55, in h5py._objects.with_phil.wrapper()\n\nFile h5py/h5f.pyx:104, in h5py.h5f.open()\n\nFileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'inference_hierarchical_bayesian_linear_regression_failed.nc', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n\n\n\n\n# 収束結果の可視化 (意思決定)\naz.plot_trace(inference, var_names=[\"alpha\", \"beta_weather\", \"beta_season\"], compact=True)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n主要な説明変数の収束を「チェーン間の合意（Consensus）」という視点で確認します。\n\ncompact=True 全てのチェーンを同じグラフ上に重ね合わせて表示するように設定しています。\n左側の分布図で異なる色の線（各チェーン）が、ピタリと重なって「1つの山」を作っているかを確認します。 &gt;もし、線が分離している場合、チェーンごとに推論結果が異なっていることを意味し、収束失敗とみなしますが上手く収束していそうです。\n右側のトレース図で、全てのチェーンが同じ帯（範囲）の中に収まり、色が混ざり合って「太い1本の毛虫」に見えるかを確認します。 &gt;他のチェーンから大きく外れて動いている色が1つでもあれば、そのサンプリング結果は採用できませんが綺麗な毛虫状になっていそうです。\n\n\n# 収束結果の可視化 (健康診断)\naz.plot_trace(inference, var_names=[\"mu_global\", \"sigma_global\", \"sigma\"], compact=False)\nplt.tight_layout()\n\n\n\n\n\n\n\n\nモデルの構造を支配する変数が、どのチェーン（試行）においても等しく「定常状態」に達しているかを厳密にチェックします。\n\nこちらは、compact=False を設定して、各チェーンを縦に並べて表示し、個別の挙動を可視化しています。\n右側のトレース図を確認して、 各チェーンが特定の傾向（トレンド）を持たず、一定の範囲を細かく振動していれば、計算は安定していることになるので今回は大丈夫そうです。\n\n\nここで収束が確認できない場合、そのモデルから得られる利益予測は「計算ミス」と同義であり、投資判断の材料にはなり得ないので注意が必要です。\n\nプロットによる視覚的確認を、数学的な指標で補強します。\n\n# 収束指標（健康診断）の数値化\naz.summary(inference,\n           var_names=[\n               \"alpha\", \"beta_weather\", \"beta_season\",  # 意思決定に関わる変数\n               \"mu_global\", \"sigma_global\", \"sigma\",  # 推論の健康診断に関わる変数\n           ],\n           kind=\"diagnostics\"  # 収束診断に特化した指標を抽出\n           )\n\n\n\n\n\n\n\n\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nalpha[A: StationSquare]\n0.015\n0.009\n1350.0\n1872.0\n1.0\n\n\nalpha[B: ParkStreet]\n0.015\n0.009\n1344.0\n1866.0\n1.0\n\n\nalpha[C: Shopping mall]\n0.015\n0.009\n1343.0\n1743.0\n1.0\n\n\nalpha[D: TempleGrounds]\n0.015\n0.009\n1329.0\n1646.0\n1.0\n\n\nbeta_weather[Sunny]\n0.013\n0.010\n1523.0\n1644.0\n1.0\n\n\nbeta_weather[rain]\n0.013\n0.010\n1506.0\n1661.0\n1.0\n\n\nbeta_weather[Cloudy]\n0.013\n0.010\n1541.0\n1707.0\n1.0\n\n\nbeta_season[Spring]\n0.007\n0.005\n1405.0\n1651.0\n1.0\n\n\nbeta_season[Summer]\n0.007\n0.005\n1376.0\n1721.0\n1.0\n\n\nbeta_season[Winter]\n0.007\n0.005\n1422.0\n1605.0\n1.0\n\n\nbeta_season[Autumn]\n0.007\n0.005\n1405.0\n1659.0\n1.0\n\n\nmu_global\n0.015\n0.009\n1324.0\n1726.0\n1.0\n\n\nsigma_global\n0.002\n0.004\n1289.0\n1717.0\n1.0\n\n\nsigma\n0.001\n0.003\n654.0\n802.0\n1.0\n\n\n\n\n\n\n\n\n\\(\\hat{R}\\) (R-hat): 全変数で 1.00 となっており、サンプリングが完全に定常状態に達していることを示しています。\nESS (Effective Sample Size) - Bulk / Tail: 通常 400 以上あれば合格ですが、1500前後の有効サンプルが確保されており、不確実性の見積もりが非常に正確です\nMCSE (Monte Carlo Standard Error) - Mean / SD: 画像では数値が非常に小さく（0.0…単位）、ビジネス上の判断（数万〜数億円単位）において完全に無視できる誤差範囲に収まっています。\n\n上記より、数学的な指標でも問題なく推論できていることが確認できそうです。\n\n各指標の意味\n1. MCSE (Monte Carlo Standard Error) - Mean\n\n【定義】: サンプリングの回数が有限であることによって生じる、平均値の推定誤差。\n【解釈】: 「もう一度同じ計算をした時の、結果のブレ幅」 です。この値がビジネスで扱う金額（例：1万円単位）に対して十分に小さければ、「計算回数が足りないせいで判断を誤る」リスクがないと言えます。 ##### 2. MCSE (Monte Carlo Standard Error) - SD\n【定義】: 標準偏差（不確実性の幅）そのものに含まれる推定誤差。\n【解釈】: 「リスクの見積もり自体の不確かさ」 です。ビジネスの「勝率」を計算する際、この値が大きいと「勝率70%と言ったが、実は計算誤差で±10%ズレるかも」という話になります。 ##### ESS (Effective Sample Size) - Bulk\n【定義】: 自己相関（サンプル同士の似通い）を考慮した、実質的な独立サンプル数。分布の「中心付近」の精度を表す。\n【解釈】: 「連写した写真の中の、ブレていない有効枚数」 です。4,000回サンプリングしても、似たような値ばかりだと情報は増えません。この数値が 400（1チェーンあたり100相当）以上あれば、平均値の推定は十分に安定しています。 ##### 3. ESS (Effective Sample Size) - Tail\n【定義】: 分布の端（裾）の部分における有効サンプル数。\n【解釈】: 「レアケースに対する目撃証言の数」 です。ビジネスのリスク（最悪のケース）を見積もるには、平均値だけでなく「端っこ」の分布が安定している必要があり、意思決定において非常に重要です。 ##### \\(\\hat{R}\\) (R-hat)\n【定義】: 各チェーンの「内側の分散」と「チェーン間の分散」を比較した指標。1.00に近いほど良く、1.01未満が合格ライン。\n【解釈】: 「4人の調査員の合致度」 です。別々に調査した4人が全く同じ結論（分布）に辿り着いたかを確認します。1.01を超えると「誰か一人が違う意見（計算ミス）」を持っている状態です。"
  },
  {
    "objectID": "notebooks/03_Sales_forecast_by_market_location/hierarchical_baysian_linear_regression.html#層フィルタリングhdi-rope-roi分析",
    "href": "notebooks/03_Sales_forecast_by_market_location/hierarchical_baysian_linear_regression.html#層フィルタリングhdi-rope-roi分析",
    "title": "階層ベイズ線形回帰によるマルシェ出店地点別の売上予測",
    "section": "３層フィルタリング(HDI, ROPE, ROI)分析",
    "text": "３層フィルタリング(HDI, ROPE, ROI)分析\n\n推論結果とコスト構造の動的ロード・シミュレーション\n\ndf_summary = az.summary(inference) よりパラメータ\ndf_raw からコスト構造\n\nを動的に取得し、天候不明のリスクと日々のノイズ（\\(\\sigma\\)）をすべてのシミュレーショに反映します。\n\n# --- 推論結果の読み込み ---\ndf_summary = az.summary(inference)\n\n# ロケーションごとのコスト構造を抽出\nfor col in df_raw.columns[3:]:\n    if df_raw[col].dtype == \"object\":\n        df_raw[col] = df_raw[col].str.replace(\",\", \"\").astype(float)\n\n# ロケーションごとのコスト構造を抽出\ninfo_cost = df_raw.groupby(\"Location\").agg({\"Fixed_Costs\": \"mean\", \"Commission_Rate\": \"mean\"}).to_dict(\"index\")\n\n# --- 2026年02月の全組み合わせシナリオ生成 ---\nresults_scenario = []\nn_samples = inference.posterior.dims[\"draw\"]\nrng = np.random.default_rng(42)\ntarget_season = df_summary.index[9]  # beta_season[Winter]\nval_sigma = df_summary.loc[\"sigma\", \"mean\"]\n\nfor location in locations:\n    for weather in weathers:\n        # パラメータ抽出\n        alpha_mean, alpha_sd = df_summary.loc[f\"alpha[{location}]\", \"mean\"], df_summary.loc[f\"alpha[{location}]\", \"sd\"]\n        weather_mean, weather_sd = df_summary.loc[f\"beta_weather[{weather}]\", \"mean\"], df_summary.loc[\n            f\"beta_weather[{weather}]\", \"sd\"]\n        season_mean, season_sd = df_summary.loc[target_season, \"mean\"], df_summary.loc[target_season, \"sd\"]\n\n        # 推論された sigma の分布から値を引く\n        sigma_mean, sigma_sd = df_summary.loc[\"sigma\", \"mean\"], df_summary.loc[\"sigma\", \"sd\"]\n        sigma_samples = rng.normal(sigma_mean, sigma_sd, n_samples)\n        # sigma は正の値である必要があるため、年の為クリップ\n        sigma_samples = np.maximum(sigma_samples, 1e-6)\n\n        # シミュレーション (mu の生成: 構造的な実力)\n        mu_samples = (rng.normal(alpha_mean, alpha_sd, n_samples) +\n                      rng.normal(weather_mean, weather_sd, n_samples) +\n                      rng.normal(season_mean, season_sd, n_samples))\n\n        # 個別予測 (mu と sigma の両方の不確実性を統合)\n        sales_samples = np.exp(rng.normal(mu_samples, sigma_samples))\n\n        # 利益の算出 (地点別コストを動的に取得)\n        cost_fixed = info_cost[location][\"Fixed_Costs\"]\n        rate_commission = info_cost[location][\"Commission_Rate\"]\n        profits = sales_samples * (1 - rate_commission) - cost_fixed\n\n        for profit in profits:\n            results_scenario.append({\"Location\": location, \"Weather\": weather, \"Sales_Pred\": profit})\n\ndf_scenarios = pd.DataFrame(results_scenario)\n\n\nsigma の役割:\n対数正規分布における「形状パラメータ（Shape parameter）」。対数スケールでの標準偏差を表します。 この値が大きいほど、地点や天気が同じでも、日々の売上が激しく上下します。\n\nTips\nなぜ sigma_val (固定値) ではなくサンプリングするのか:\n推論された sigma にも標準偏差（不確実性）が存在するため「ノイズの音量はおよそ10（mean）」と分かっていても、実は「9〜11（sd）」の範囲で確信が持てないかもしれません。 この「確信のなさ」を無視すると、最悪のケース（大赤字のリスク）を過小評価してしまう可能性があります。\n\n\n\n【統計的根拠と分布の選択】\n対数正規分布の個別予測: np.exp(rng.normal(mu, sigma)) は、対数正規分布 \\(LogNormal(\\mu, \\sigma)\\) から直接サンプリングしていることと同義です。 exp(rng.normal(mu, sigma)) は「その条件で発生し得るある1日の売上」を算出する式になります。\n\n# --- プロットの作成 (boxplot) ---\nimport japanize_matplotlib\n\nplt.figure(figsize=(14, 7))\nsns.boxplot(data=df_scenarios, x=\"Location\", y=\"Sales_Pred\", hue=\"Weather\", fliersize=0, width=0.7)\nplt.axhline(0, color=COLOR_RED, linestyle=\"--\", alpha=0.6, label=\"Break-even point\")\nplt.ylim(top=300000)\nplt.title(\"February 2026: Profit simulation by location and weather (all-round risk visualization)\", fontsize=14)\nplt.xlabel(\"Store location\")\nplt.ylabel(\"Estimated Net Profit (yen)\")\nplt.grid(axis=\"y\", alpha=0.3)\nplt.legend(title=\"Forecast Weather\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n【結果の解釈とビジネス上の意味】\n\n意思決定の信頼性: sigma の不確実性を入れた上でも各ボックスが 0 を超えていれば、それは「どれだけ運が悪く、どれだけ予測がブレても、ほぼ確実に利益が出る」という最強の証明になります。\n勝率（Win Rate）の精度: 利益が 0 を下回る確率を計算する際、sigma の不確実性を入れることで、より保守的（安全側）なリスク見積もりが可能になり、中小規模ビジネスにおける「不意の赤字による資金ショート」を防ぐ手助けとなります。\n\n\n\n\n2026年02月 出店売上・コスト予測表 (ROPE判定付)\n推論結果（summary_inference.csv）とコスト構造（sales_performance.csv）から動的に値を抽出し、全ロケーションの損益予測を算出します。\n経営層が投資判断を下すために必要な「期待値」と「リスクの振れ幅（上下限）」を、金額ベースで一覧化します。\n\n# --- シミュレーション結果を集計し、意思決定テーブルを作成 ---\n\n# 利益の期待値と不可実性の範囲 (HDI に相当するパーセンタイル) を算出\ndf_scenarios.columns = [\"出店場所\", \"天気\", \"予想利益\"]\ndf_summary_table = df_scenarios.groupby(\"出店場所\")[\"予想利益\"].agg([\n    (\"期待利益平均\", \"mean\"),\n    (\"利益下限(3%)\", lambda x: np.percentile(x, 3)),\n    (\"利益上限(97%)\", lambda x: np.percentile(x, 97)),\n    (\"黒字確率\", lambda x: (x &gt; 0).mean())\n]).copy()\n\n# ROPE判定: 実務上の損益分岐点を 20,000円 と定義\nrope_threshold = 20_000\ndf_summary_table[\"ROPE判定(利益確保)\"] = df_summary_table[\"期待利益平均\"] &gt; rope_threshold\n\n# 結果の表示\ndf_summary_table\n\n\n\n\n\n\n\n\n期待利益平均\n利益下限(3%)\n利益上限(97%)\n黒字確率\nROPE判定(利益確保)\n\n\n出店場所\n\n\n\n\n\n\n\n\n\nA: StationSquare\n16394.361692\n-39532.307369\n199965.364421\n0.424667\nFalse\n\n\nB: ParkStreet\n41752.023798\n-24262.116184\n269624.005475\n0.624667\nTrue\n\n\nC: Shopping mall\n4837.206013\n-50018.424593\n173663.379247\n0.348667\nFalse\n\n\nD: TempleGrounds\n33043.642141\n-28961.800335\n221855.668784\n0.565000\nTrue\n\n\n\n\n\n\n\n\nROPE (実質的等価性)\n統計的な有意差ではなく、実務上のインパクト（今回は2万円の利益）があるかどうかの境界線になります。 「1円でも黒字ならOK」ではなく、「準備の手間を考えて、最低これくらいは残ってほしい」という合格ラインです。\n\n\nマージナライズ (平均化)\n特定の変数（天気）のすべての可能性を考慮して、全体としての期待値を算出することです。 「晴れの点数」と「雨の点数」を混ぜこぜにして、「結局、平均して何点取れそうか」 を計算します。\n\nこのテーブルにより、どのロケーションが「天候リスクに対して頑健（タフ）か」が明らかになります。期待利益が高く、かつ「利益下限(3%)」が 0 を大きく上回っている地点は、ビジネス上のリスクが極めて低い「優良案件」と判断できます。\n\n\n2026年02月 最適出店ロケーションの戦略提案\n最終的な ROI(投資対効果）とダウンサイドリスク (赤字の危険性)を評価し、具体的なアクションを提案します。\n単なる「平均」ではなく、「勝つときはいくら勝ち、負けるときはいくら負けるか」というリスクの非対称性を可視化し、意思決定の判断を助けれるようにしようと思います。\n\n# ROI (経済的評価)に基づいた最終推奨地点の特定 ---\n\n# ROPE をクリアし、かつ期待利益が最大のロケーションを抽出\nlocation_best = df_summary_table[df_summary_table[\"ROPE判定(利益確保)\"] == True][\"期待利益平均\"].idxmax()\n\n# 推奨地点の利益分布を抽出してリスク構造を分解\ndist_best_profit = df_scenarios[df_scenarios[\"出店場所\"] == location_best][\"予想利益\"]\nprofit_plus = dist_best_profit[dist_best_profit &gt; 0]\nprofit_minus = dist_best_profit[dist_best_profit &lt;= 0]\n\nprint(f\"【最終戦略提案】2026年02月は『{location_best}』への出店を強く推奨します。\")\nprint(\"=\" * 60)\nprint(f\"■ 黒字時の期待利益: +{profit_plus.mean():,.0f} 円\")\nprint(f\"  - 発生確率(勝率): {len(profit_plus) / len(dist_best_profit):,.1%}\")\nprint(f\"■ 赤字時の平均損失: {profit_minus.mean() if len(profit_minus) &gt; 0 else 0:,.0f} 円\")\nprint(f\"  - 発生確率: {len(profit_minus) / len(dist_best_profit):,.1%}\")\nprint(\"-\" * 60)\nprint(f\"※ この予測は、天候不詳リスクおよび日々の売上変動 (sigma) をすべて考慮した結果です。\")\n\n【最終戦略提案】2026年02月は『B: ParkStreet』への出店を強く推奨します。\n============================================================\n■ 黒字時の期待利益: +75,009 円\n  - 発生確率(勝率): 62.5%\n■ 赤字時の平均損失: -13,597 円\n  - 発生確率: 37.5%\n------------------------------------------------------------\n※ この予測は、天候不詳リスクおよび日々の売上変動 (sigma) をすべて考慮した結果です。"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ベイジアンいろは",
    "section": "",
    "text": "小規模データ×階層ベイズでビジネスの勝率を導く「意思決定のいろは」。PyMC v5を用い、単なる有意差検定を超えたROPE/ROI分析によるリスク可視化フレームワークを実装・蓄積するナレッジベース。\n\n\nここには、本プロジェクトで実施した分析レポートを掲載しています。\n\n\n\nプロジェクト概要（README）\n分析実証ログ（Notebook）\n\n\n\n\n\nプロジェクト概要（README）\n分析実証ログ（Notebook）\nヘルパーメソッド定義（Python）\n\n\n\n\n-プロジェクト概要 - 分析実証ログ"
  },
  {
    "objectID": "index.html#概要",
    "href": "index.html#概要",
    "title": "ベイジアンいろは",
    "section": "",
    "text": "小規模データ×階層ベイズでビジネスの勝率を導く「意思決定のいろは」。PyMC v5を用い、単なる有意差検定を超えたROPE/ROI分析によるリスク可視化フレームワークを実装・蓄積するナレッジベース。\n\n\nここには、本プロジェクトで実施した分析レポートを掲載しています。\n\n\n\nプロジェクト概要（README）\n分析実証ログ（Notebook）\n\n\n\n\n\nプロジェクト概要（README）\n分析実証ログ（Notebook）\nヘルパーメソッド定義（Python）\n\n\n\n\n-プロジェクト概要 - 分析実証ログ"
  },
  {
    "objectID": "notebooks/03_Sales_forecast_by_market_location/weakly_information_prior.html",
    "href": "notebooks/03_Sales_forecast_by_market_location/weakly_information_prior.html",
    "title": "事前分布の比較シミュレーション",
    "section": "",
    "text": "分析者が「データを分析する前」に持っている知識の強さを、3つのパターンで可視化しています。今回の売上予測モデルで Normal(11, 2) を選んだ意図を明確にします。\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport japanize_matplotlib\n\n# パラメータ設定\nx = np.linspace(-5, 15, 1000)\n\n# 1. 無情報事前分布 (Flat/Uninformative): 何も知らない\nprior_uninformative = stats.norm.pdf(x, 0, 100)\n\n# 2. 弱情報事前分布 (Weakly Informative): 「この範囲ならあり得る」という常識\nprior_weakly = stats.norm.pdf(x, 11, 2)\n\n# 3. 強情報事前分布 (Informative): 「絶対にこうだ」という強い信念\nprior_informative = stats.norm.pdf(x, 11, 0.2)\n\n# 可視化\nplt.figure(figsize=(12, 6))\nplt.plot(x, prior_uninformative, label=\"無情報 (Flat): 予測がつかない\", linestyle=\"--\", color=\"gray\")\nplt.plot(x, prior_weakly, label=\"弱情報 (Weakly): 妥当な範囲を示す\", linewidth=3, color=\"C0\")\nplt.plot(x, prior_informative, label=\"強情報 (Informative): 頑固な確信\", linewidth=2, color=\"C1\")\n\nplt.title(\"事前分布のタイプ別比較 (売上の対数スケール想定)\", fontsize=14)\nplt.xlabel(\"パラメータの値 (例: 推定される売上の対数)\", fontsize=12)\nplt.ylabel(\"確率密度\", fontsize=12)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
    "objectID": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html",
    "href": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html",
    "title": "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト",
    "section": "",
    "text": "本ノートブックでは、MoneyPuck.com から取得した 2024 年シーズンの実ショットデータを用い、シュートの「場所の難易度」を考慮した上でゴテンダーの真の実力（GSAx）を推定します。 特に、データが限られた「小規模データ（500件）」環境において、ベイズ階層モデルがいかにして安定した意思決定を支援するかを、3層の戦略的フィルタリングを通じて実証します。\n# 必要なライブラリ\nimport os\nimport requests\nimport zipfile\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom cycler import cycler\nfrom sklearn.preprocessing import SplineTransformer\n\n# 不要な出力の制御\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n# プロジェクト共通のカラー定義（実際の運用では /src にモジュール化を推奨）\nCOLOR_PURPLE = \"#9B5DE5\"  # 事後分布・HDI\nCOLOR_YELLOW = \"#F9C74F\"  # ROPE領域\nCOLOR_GREEN = \"#06D6A0\"  # 改善判定\nCOLOR_RED = \"#EF476F\"  # 悪化判定\nCOLOR_GRAY = \"#8D99AE\"  # 等価判定・参照線\n\nplt.rcdefaults()\npalette_brand = [COLOR_PURPLE, COLOR_YELLOW, COLOR_GREEN, COLOR_RED, COLOR_GRAY]\nsns.set_theme(style=\"whitegrid\", palette=palette_brand)\nplt.rcParams[\"axes.prop_cycle\"] = cycler(color=palette_brand)\n\nprint(\"Brand Style Applied: The visual identity was applied.\")\n\nBrand Style Applied: The visual identity was applied.\n分析に必要な統計・可視化ライブラリをロードし、レポートの品質を担保するためのデザイン設定（Seaborn / Japanize-Matplotlib）を行います。",
    "crumbs": [
      "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト"
    ]
  },
  {
    "objectID": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#データ準備preparation",
    "href": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#データ準備preparation",
    "title": "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト",
    "section": "1. データ準備（Preparation）",
    "text": "1. データ準備（Preparation）\n外部ソース（Moneypuck.com）から ZIP形式の巨大なデータセットを自動取得し、解析可能な状態に展開します。\n\n# データのダウンロード（MoneyPuck 2024 shots Data）\nURL_DATA = \"https://peter-tanner.com/moneypuck/downloads/shots_2024.zip\"\nDIR_EXTRACT = \"data_raw\"\nNAME_ZIP = \"shots_2024.zip\"\nNAME_CSV = \"shots_2024.csv\"\nPATH_CSV = os.path.join(DIR_EXTRACT, NAME_CSV)\n\nif os.path.exists(PATH_CSV):\n    # すでに CSV が存在する場合はスキップ\n    print(f\"Data Preparation: Local CSV found at {PATH_CSV}. Skipping download.\")\n\nelse:\n    # ファイルがない場合: ダウンロードと展開を実行する\n    print(\"Data Preparation: CSV not found. Starting automated setup...\")\n\n    # 保存先ディレクトリの作成\n    if not os.path.exists(DIR_EXTRACT):\n        os.makedirs(DIR_EXTRACT)\n        print(f\"Data Preparation: Created directory '{DIR_EXTRACT}\")\n\n    # ダウンロード\n    print(f\"Data Preparation: Downloading from {URL_DATA}...\")\n    try:\n        response = requests.get(URL_DATA)\n        with open(NAME_ZIP, \"wb\") as f:\n            f.write(response.content)\n\n        # ZIP の展開\n        print(f\"Data Preparation: Extracting {NAME_ZIP}...\")\n        with zipfile.ZipFile(NAME_ZIP, \"r\") as ref_zip:\n            ref_zip.extractall(DIR_EXTRACT)\n\n        # 元の ZIP ファイルの削除（クリーンアップ）\n        if os.path.exists(NAME_ZIP):\n            os.remove(NAME_ZIP)\n            print(f\"Data Preparation: Cleand up temporary ZIP file '{NAME_ZIP}'\")\n\n        # 展開ファイルの確認\n        files = os.listdir(DIR_EXTRACT)\n        print(f\"Data Preparation: Extracted files: {files}\")\n    except Exception as e:\n        print(f\"Data Preparation Error: {e}\")\n\nData Preparation: CSV not found. Starting automated setup...\nData Preparation: Downloading from https://peter-tanner.com/moneypuck/downloads/shots_2024.zip...\nData Preparation: Extracting shots_2024.zip...\nData Preparation: Cleand up temporary ZIP file 'shots_2024.zip'\nData Preparation: Extracted files: ['shots_2024.csv', 'goalie_final_investment_report.csv', 'data_goalie_rope_analysis.csv', 'final_roi_report_chart.png']\n\n\n\n# データのプレビュー（ホワイトボックス化）\nCSV_TARGET = os.path.join(DIR_EXTRACT, \"shots_2024.csv\")\n\n# 先頭５行を確認\ndf_preview = pd.read_csv(CSV_TARGET, nrows=5)\nprint(\"Data Preparation: Preview of the data:\")\ndf_preview.head()\n\nData Preparation: Preview of the data:\n\n\n\n\n\n\n\n\n\nshotID\narenaAdjustedShotDistance\narenaAdjustedXCord\narenaAdjustedXCordABS\narenaAdjustedYCord\narenaAdjustedYCordAbs\naverageRestDifference\nawayEmptyNet\nawayPenalty1Length\nawayPenalty1TimeLeft\n...\nxCordAdjusted\nxFroze\nxGoal\nxPlayContinuedInZone\nxPlayContinuedOutsideZone\nxPlayStopped\nxRebound\nxShotWasOnGoal\nyCord\nyCordAdjusted\n\n\n\n\n0\n0\n52.0\n57.0\n57.0\n-41.0\n41.0\n0.0\n0\n0\n0\n...\n57\n0.238455\n0.012537\n0.394229\n0.301072\n0.022807\n0.030900\n0.710867\n-40\n-40\n\n\n1\n1\n33.0\n71.0\n71.0\n-28.0\n28.0\n-6.0\n0\n0\n0\n...\n71\n0.198306\n0.021962\n0.404919\n0.313773\n0.023774\n0.037266\n0.759039\n-28\n-28\n\n\n2\n2\n48.0\n48.0\n48.0\n-24.0\n24.0\n-12.6\n0\n0\n0\n...\n48\n0.213829\n0.028057\n0.405311\n0.294682\n0.025849\n0.032272\n0.696901\n-24\n-24\n\n\n3\n3\n58.0\n-40.0\n40.0\n-31.0\n31.0\n0.0\n0\n0\n0\n...\n41\n0.209478\n0.009832\n0.449775\n0.277671\n0.019667\n0.033577\n0.610530\n-31\n31\n\n\n4\n4\n56.0\n-35.0\n35.0\n15.0\n15.0\n0.0\n0\n0\n0\n...\n36\n0.376712\n0.028884\n0.307725\n0.205568\n0.022266\n0.058845\n0.799576\n15\n-15\n\n\n\n\n5 rows × 137 columns\n\n\n\n膨大なカラムを持つデータから、空間的な「期待ゴール率」と「ゴテンダー評価」に直結する最小限の変数のみを抽出してロードします。\n\n# 利用する列を展開してロード（メモリ効率化）\nCOLS_USE = [\n    \"shotID\",\n    \"goal\",  # 1: Goal, 0: No Goal\n    \"xCordAdjusted\",  # 調整済み x 座標\n    \"yCordAdjusted\",  # 調整済み y 座標\n    \"goalieIdForShot\",  # ゴテンダー ID\n    \"goalieNameForShot\"  # 選手名\n]\n\nprint(\"Data Preparation: Loading real dataset from CSV...\")\ndf_raw = pd.read_csv(CSV_TARGET, usecols=COLS_USE)\n\nprint(f\"Data Preparation: Data load finished. {len(df_raw):,} rows\")\ndf_raw.head()\n\nData Preparation: Loading real dataset from CSV...\nData Preparation: Data load finished. 119,870 rows\n\n\n\n\n\n\n\n\n\nshotID\ngoal\ngoalieIdForShot\ngoalieNameForShot\nxCordAdjusted\nyCordAdjusted\n\n\n\n\n0\n0\n0\n8480045\nUkko-Pekka Luukkonen\n57\n-40\n\n\n1\n1\n0\n8480045\nUkko-Pekka Luukkonen\n71\n-28\n\n\n2\n2\n0\n8480045\nUkko-Pekka Luukkonen\n48\n-24\n\n\n3\n3\n0\n8474593\nJacob Markstrom\n41\n31\n\n\n4\n4\n0\n8474593\nJacob Markstrom\n36\n-15\n\n\n\n\n\n\n\n投影（Projection / usecols） - 巨大なデータセットから、特定の属性（列）のみの選択をしてメモリに読み込むようにします。 データ全てを覚えるのではなく、必要なカラムだけを抽出することで作業効率を劇的に上げるテクニックです。",
    "crumbs": [
      "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト"
    ]
  },
  {
    "objectID": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#データ加工processing",
    "href": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#データ加工processing",
    "title": "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト",
    "section": "2. データ加工（Processing）",
    "text": "2. データ加工（Processing）\nキーバーがいない状況でのゴールなど、能力評価においてノイズとなるデータを除外します。\n\n# クレンジング:\ndf_clean = df_raw.dropna(subset=[\"goalieIdForShot\"]).copy()  # ゴテンダー不在（Empty Net）データの厳密な除外\ndf_clean = df_clean[df_clean[\"goalieIdForShot\"] &gt; 0].copy()  # MoneyPuck データでは無人ゴール時に ID が 0 や欠損になるため、これを除外します。\n\nprint(f\"Data Processing: {len(df_clean):,} rows left after cleaning.\")\n\nData Processing: 118,900 rows left after cleaning.\n\n\nデータが少ない環境（新規事業や特定セグメントの分析）を再現し、不可実性下での判断プロセスを構築します。\n\n# 小規模データ化（３桁台のサンプリング）\n# 意思決定の難易度が高い「事例が少ない」状況を再現するため、あえて 500 件に絞ります\nSIZE_SAMPLE = 500\ndf_small = df_clean.sample(n=SIZE_SAMPLE, random_state=42).reset_index(drop=True)\n\ndf_small\n\n\n\n\n\n\n\n\nshotID\ngoal\ngoalieIdForShot\ngoalieNameForShot\nxCordAdjusted\nyCordAdjusted\n\n\n\n\n0\n77213\n0\n8480947\nKevin Lankinen\n57\n-17\n\n\n1\n62564\n1\n8482487\nJakub Dobes\n42\n12\n\n\n2\n50703\n0\n8480382\nAlexandar Georgiev\n76\n10\n\n\n3\n80853\n1\n8479406\nFilip Gustavsson\n72\n14\n\n\n4\n65495\n0\n8481020\nJustus Annunen\n23\n-39\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n43089\n0\n8475683\nSergei Bobrovsky\n56\n-12\n\n\n496\n50168\n0\n8481611\nPyotr Kochetkov\n66\n35\n\n\n497\n15962\n0\n8480280\nJeremy Swayman\n62\n16\n\n\n498\n68658\n0\n8475683\nSergei Bobrovsky\n37\n11\n\n\n499\n41174\n0\n8481692\nDustin Wolf\n37\n-13\n\n\n\n\n500 rows × 6 columns\n\n\n\nカテゴリ変数（ID）を確率モデルが扱える形式に変換し、結果の解釈をしやすくするための紐付けを行います。\n\n# インデックス化\n# 選手ID を 0 からの連番に変換し、ID から名前を引く辞書を作成します\ncodes_goalie, uniques_goalie = pd.factorize(df_small[\"goalieIdForShot\"])\ndf_small[\"idx_goalie\"] = codes_goalie\ndf_small[\"obs_shots\"] = df_small[\"goal\"].astype(int)\n\n# ID から名前を引くためのマップ\nid_to_name = df_small.set_index(\"idx_goalie\")[\"goalieNameForShot\"].to_dict()\n\nprint(f\"Data Processing: unique count of Goalie {len(uniques_goalie):,}\")\ndf_small[[\"goalieIdForShot\", \"idx_goalie\", \"obs_shots\"]]\n\nData Processing: unique count of Goalie 79\n\n\n\n\n\n\n\n\n\ngoalieIdForShot\nidx_goalie\nobs_shots\n\n\n\n\n0\n8480947\n0\n0\n\n\n1\n8482487\n1\n1\n\n\n2\n8480382\n2\n0\n\n\n3\n8479406\n3\n1\n\n\n4\n8481020\n4\n0\n\n\n...\n...\n...\n...\n\n\n495\n8475683\n25\n0\n\n\n496\n8481611\n30\n0\n\n\n497\n8480280\n18\n0\n\n\n498\n8475683\n25\n0\n\n\n499\n8481692\n29\n0\n\n\n\n\n500 rows × 3 columns\n\n\n\n座標（\\(x\\), \\(y\\)）とう数値を、モデルが「どのエリアが危険か」を滑らかに学習できる高度な特徴量に変換します。\n\n# 特徴量生成（空間基底関数の適用）\n# 座標データから滑らかな空間曲面（スプライン基底）を生成します\nspline = SplineTransformer(n_knots=5, degree=3, include_bias=False)\nbasis_spatial = spline.fit_transform(df_small[[\"xCordAdjusted\", \"yCordAdjusted\"]])\n\nprint(f\"Data Processing: Feature matrix calculation {basis_spatial.shape}\")\n\nData Processing: Feature matrix calculation (500, 12)\n\n\nこのセルでは、「シュート位置（x, y）」という単純な 2 つの数値を、この SplineTransformer に通すことで、 「ゴール付近ならこの値が大きくなる」「端の方ならこの値が変化する」といった、 空間的な特徴を持つ複数の変数（多次元の行列） へと変換しています。\nこれにより、モデルは「座標 (x, y)」をただの数字としてではなく、 「リンク上のどのエリアがゴールになりやすいか」という滑らかな非線形の関係として学習できるようになります。\n\nスプライン基底関数\n一言でいうと、「複雑な1つの曲面を、いくつかの『シンプルな山の形（部品）』の組み合わせで表現する仕組み」 のことです。\n\n1. 「基底（きてい）」とは「部品」のこと\n例えば、絵の具で「オレンジ色」を作るとき、「赤」と「黄色」を混ぜます。このとき、赤と黄色が「基底（部品）」です。 データ分析における基底関数も同じです。 「座標 (x, y)」という生データをそのまま使うのではなく、「特定のエリアに反応するいくつかの部品（関数）」に変換します。\n\n\n2. スプライン基底関数の仕組み（イメージ）\nアイスホッケーのリンクを想像してください。 1. エリアの分割: リンクの上に、いくつかの「小さな山（テントのような形）」を等間隔に並べます。 2. 反応する場所: - ゴール正面にある「山A」は、シュートが正面から打たれたときだけ高い値を出します。 - サイドにある「山B」は、サイドから打たれたときだけ反応します。\n合成: これらの「山（部品）」をそれぞれ「どれくらい重視するか（重み）」を掛け合わせて足し算すると、リンク全体の「シュートの危険度マップ」が完成します。 この「一つ一つの山」が「スプライン基底関数」です。\n\n\n3. なぜ「スプライン」なのか？\n単にエリアを四角く区切る（＝モザイク状にする）だけだと、エリアの境界線で値が突然跳ね上がってしまい、不自然です。\n「スプライン」 という手法を使うと、隣り合う「山」同士が滑らかに重なり合うように設計されます。これにより\n\n「ここから急にゴールしやすくなる」という不自然な段差がなくなる。\n「ゴールに近づくにつれて、なだらかに危険度が高まる」 という自然な表現が可能になる。\n\n\n\n4. SplineTransformer がやっていること\nコードにある以下の処理は、まさにこの「部品への変換」を行っています。\n# 座標 (x, y) を、複数の「山の反応度」に変換する\nbasis_spatial = spline.fit_transform(df_small[[\"xCordAdjusted\", \"yCordAdjusted\"]])\n\n入力: [x座標, y座標] （2つの数字）\n出力: [山Aの反応, 山Bの反応, 山Cの反応, …] （多くの数字の列）\n\nの変換のおかげで、あとの統計モデル（PyMC）は「座標から直接計算する」という難しいことをしなくて済み、 「どの山の重みを大きくすれば、実際のゴール率と一致するか？」 を計算するだけで済むようになります。\n\n\nまとめ\nスプライン基底関数とは、 複雑な空間の形を捉えるために、「滑らかに重なり合った、場所ごとの反応パーツ」に分解して表現する手法のことです。\nSplineTransformar(n_knots=5, degree=3, include_bias=False)\nsciket-learn で提供されている、数値データ。今回は、座標から「スプライン基底関数」と呼ばれる特徴量を生成。\nArgs: &gt; n_konots=5 (ノットの数) &gt; ノット（データを分割する「結び目（ノット」））の数。 &gt; スプライン曲線は、データをいくつかの区間に分けて、それぞれの区間で多項式を当てはめます。 &gt; この値を大きくするほど、より細かく複雑な形状（曲面）を表現できるようになりますが、 &gt; 大きくしすぎると過学習（ノイズに反応しすぎること）のリスクが高まります。\n\ndegree=3 (多項式の次数) 各区間で使用する多項式の次数。 - degree=1: 線形（カクカクした折れ線） - degree=2: 2次関数（放物線） - degree=3: 3次関数（立方スプライン）\nこのコードでは: 一般的に最もよく使われる 3（3次スプライン） が指定されています。 これにより、結び目の境界でも変化が非常に滑らか（数学的に2回微分可能）な曲面を作ることができます。\n\n\ninclude_b=False (バイアス項を含めるか) 全ての基底の和が 1 になるような定数項（インターセプト）を特徴量に含めるかどうか。 True にすると、生成される行列に「常に 1 となる列」のような定数成分が含まれます。 このコードでは: False に設定されています。 これは、後続の統計モデル（PyMCなど）側で別途切片（Intercept）を用意する場合や、 冗長な変数を避けるためによく取られる設定です。",
    "crumbs": [
      "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト"
    ]
  },
  {
    "objectID": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#確率モデル構築modeling",
    "href": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#確率モデル構築modeling",
    "title": "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト",
    "section": "3. 確率モデル構築（Modeling）",
    "text": "3. 確率モデル構築（Modeling）\n「場所の難易度」と「キーパーの実力」を統計的に分離して推定します。\n\n# PyMC用の Coords(座標)定義\ncoords = {\n    \"goalie\": [id_to_name[i] for i in range(len(uniques_goalie))],\n    \"shot\": np.arange(len(df_small)),\n    \"basis\": np.arange(basis_spatial.shape[1])\n}\n\n確率変数の各次元に対してラベルを付与します。 後で可視化や結果を確認する際に人間が理解しやすいようにします。\n\nwith pm.Model(coords=coords) as hockey_spatial_model:\n    # --- データ登録(pm.Data: モデル内で参照する固定データ) ---\n    x_basis = pm.Data(\"x_basis\", basis_spatial, dims=(\"shot\", \"basis\"))  # 空間特徴量（スプライン基底行列）\n    idx_goalie = pm.Data(\"idx_goalie\", df_small[\"idx_goalie\"], dims=\"shot\")  # 各シュートの担当GKインデックス\n    obs_shot = pm.Data(\"obs_shots\", df_small[\"obs_shots\"], dims=\"shot\")  # 正解ラベル（ゴールしたか否か）\n\n    # --- 空間効果 (Spatial Component: 場所によるはいりやすさの共通トレンド) ---\n    coeffs_spatial = pm.Normal(\"coeffs_spatial\", mu=0, sigma=1, dims=\"basis\")  # 各場所（基底）に対する重み係数\n    trend_spatial = pm.Deterministic(\"trend_spatial\", pm.math.dot(x_basis, coeffs_spatial),\n                                     dims=\"shot\")  # 場所による「決まりやすさ」のトレンド（線形結合）\n\n    # 階層構造 (Hierarchical Component: 人の実力)\n    mu_league = pm.Normal(\"mu_league\", mu=0, sigma=1.5)  # リーグ全体の平均防御力\n    sigma_goalie = pm.HalfNormal(\"sigma_goalie\", sigma=0.5)  # リーグ全体の実力格差（ばらつき）\n    z_goalie_offset = pm.Normal(\"z_goalie_offset\", mu=0, sigma=1, dims=\"goalie\")  # 各GKの偏差値（標準化された個体差\n\n    # --- ゴテンダー個別の実力値 (Non-centered Parameterization) ---\n    effect_goalie = pm.Deterministic(\"effect_goalie\", mu_league + z_goalie_offset * sigma_goalie, dims=\"goalie\")\n\n    # --- 結合と尤度 (Likelihood: 答え合わせ) ---\n    p_logit = trend_spatial + effect_goalie[idx_goalie]  # 対数オッズ（場所の効果 + 人の効果）\n    xG = pm.Deterministic(\"xG\", pm.math.invlogit(p_logit), dims=\"shot\")  # 確率（0~1）への変換 = 期待ゴール率\n\n    # ベルヌーイ分布で観測データと比較し、パラメータを推論\n    pm.Bernoulli(\"obs\", logit_p=p_logit, observed=obs_shot, dims=\"shot\")\n\n\n# 確率モデル構造の可視化\npm.model_to_graphviz(hockey_spatial_model)\n\n\n\n\n\n\n\n\n\n確率モデル構造の解読（Graphviz の見方）\nモデルは大きく「左・右・下」の3つのブロックで構成されています。\n\n1. 左側：空間効果（Spatial Component）\nシュートが放たれた「場所」の有利・不利を計算するルートです。\n\nx_basis (Data) \\(\\rightarrow\\) trend_spatial (Deterministic)\n\nx_basis（12次元のスプライン基底）に、推定された重み coeffs_spatial を掛け合わせることで、リンク上の各地点の「決まりやすさの基準値」を算出します。\n\n役割:\n\n「ゴール正面は決まりやすく、端は決まりにくい」という物理的なコンテキストを司ります。これにより、キーパーの実力評価から「場所の有利不利」というノイズを排除できます。\n\n\n\n\n2. 右側：ゴテンダーの実力（Goalie Hierarchy）\n各キーパーの阻止能力を推定するルートです。ここには NCP（非中心化パラメータ化） という高度な階層構造が組み込まれています。\n\n最上流 (mu_league, sigma_goalie):\n\nリーグ全体の「平均的な防御力」と「実力差のバラつき」です。\n\n中流 (z_goalie_offset):\n\n各キーパーの「平均からのズレ」を標準化した数値（偏差値のようなもの）です。\n\n下流 (effect_goalie):\n\n上記を統合し、\\(\\mu + z \\cdot \\sigma\\) の計算によって各キーパーの真の実力を復元します。\n\n役割:\n\nデータが少ないキーパーの評価をリーグ平均に引き寄せ（収縮推定）、「たまたま数回止めただけ」の選手を過大評価するリスクを統計的に防ぎます。\n\n\n\n\n③ 下部：観測データとの結合（Likelihood）\n予測と現実を突き合わせ、モデルを学習させる心臓部です。\n\np_logit \\(\\rightarrow\\) xG \\(\\rightarrow\\) obs (Observed)\n\ntrend_spatial（場所の影響）と effect_goalie（人の能力）の和が p_logit（対数オッズ）となり、それを確率に直したものが xG（期待ゴール率）です。\n最後に、実際のゴール成否 obs と比較されます。\n\n役割:\n\n現実のデータ（obs）との誤差が最小になるように、矢印を逆流する形で全てのパラメータ（白ノード）の値が微調整（サンプリング）されます。\n\n\n\nTips: 確率モデル構造図（DAG）の読み解きガイド\nグラフ内の各要素は、統計学的な役割を「色」と「形」で表現しています。\n1. ノード（枠）の「色」が表すもの：データの確定度\n\nグレー（塗りつぶし）: 観測データ / 固定値\n\n外界から与えられた、モデルが変更することのない「事実（pm.Data や observed）」です。すべての推論はこの数値を土台に開始されます。\n\n白（背景なし）: 未知の変数 / 推定対象\n\nモデルがデータから正体を突き止めようとしている数値です。ここが「白」であることは、その値が不確実であり、推論が必要であることを意味します。\n\n\n2. ノード（枠）の「形」が表すもの：変数の性質\n\n楕円（白い丸枠）: 確率変数（Stochastic Variables）\n\n定義: Normal や HalfNormal といった統計分布を持つ変数です。\n役割: サンプリング（MCMC）によって、その値の分布（もっともらしい範囲）が推定される「モデルの核心」となる変数です。\n\n四角形（白い四角枠 / 二重線）: 決定論的変数（Deterministic Variables）\n\n定義: 他のノードからの計算（足し算や行列掛け算など）によって一意に決まる変数です。\n役割: それ自体がランダムに変動するわけではなく、計算結果に「GSAx」や「空間トレンド」といった名前（ラベル）を付けて解釈しやすくするために存在します。\n\n\n3. プレート（外側の大きな四角枠）\n\n意味: 反復処理の範囲（Plate）\n\n枠の右下にある数字は、その中の計算が何回繰り返されるかを示します。\nshot プレート (500): 全500件のシュート1件ずつに対して、個別に期待値を計算している領域。\ngoalie プレート (N): キーパーの人数分だけ、個別の実力パラメータを用意している領域。\n\n\n4. 矢印（有向エッジ）\n\n意味: 依存関係 / 因果の流れ\n\n「A の値が B の計算に使われる」という情報の流れを示します。矢印を遡ることで、ある結果が「場所の影響」なのか「個人の実力」なのか、どの因拠に基づいているかを特定できます。\n\n\n\n\n\n\nこの構造がビジネスにもたらす価値\nこのグラフをステークホルダーに見せることは、単なる技術説明以上の意味を持ちます。\n\n「公平な評価」の可視化:\n\n場所の影響（左）と個人の能力（右）が別々のルートで計算され、最後に合流していることを示せます。これは、「厳しい状況で守っているキーパーを正当に評価している」というロジックの証明です。\n\nホワイトボックス化による信頼:\n\n「なぜその評価になったのか」を、この図に沿って説明できます。「この sigma_goalie という機能がノイズを抑制しているから、新人の評価も安全に行えるのです」といった、専門的な説得が可能になります。\n\n拡張性の提示:\n\n例えば「シュートの強さ」を追加したい場合は、左側の枝に新しいノードを足せばよいことが一目でわかります。モデルの成長ロードマップを共有する際の地図になります。",
    "crumbs": [
      "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト"
    ]
  },
  {
    "objectID": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#推論とモデル診断diagnostics",
    "href": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#推論とモデル診断diagnostics",
    "title": "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト",
    "section": "4. 推論とモデル診断（Diagnostics）",
    "text": "4. 推論とモデル診断（Diagnostics）\n推論を実行し、計算が正しく収束し、モデルが信頼できる状態であることを証明します。\n\nPATH_TRACE = \"inference_hockey_spatial.nc\"  # 推論結果を保存するファイル名\n\nif os.path.exists(PATH_TRACE):\n    # 保存されたファイルがある場合はロード\n    print(f\"Inference: Checked for existing inference results. Loading from {PATH_TRACE}.\")\n    inference = az.from_netcdf(PATH_TRACE)\nelse:\n    # ファイルがない場合のみ推論を実行\n    print(\"Inference: No inference results found, so we will start sampling. This may take a few minutes.\")\n    with hockey_spatial_model:\n        # ターゲット受容率を高めに設定し、安定性を確保\n        inference = pm.sample(2000, tune=1000, target_accept=0.95, random_seed=42)\n\n    # 結果を NetCDF 形式で保存（永続化）\n    inference.to_netcdf(PATH_TRACE)\n    print(f\"Inference: Inference completed and results saved to {PATH_TRACE}.\")\n\nInference: Checked for existing inference results. Loading from inference_hockey_spatial.nc.\n\n\n\n# 収束診断\nsummary = az.summary(inference, var_names=[\"mu_league\", \"sigma_goalie\"])\nr_hat_max = summary[\"r_hat\"].max()\n\ndisplay(summary)\nprint(f\"Diagnostics: Max r_hat = {r_hat_max:.4f}\")\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nmu_league\n-2.829\n0.646\n-4.018\n-1.592\n0.007\n0.007\n7615.0\n6261.0\n1.0\n\n\nsigma_goalie\n0.293\n0.214\n0.000\n0.668\n0.003\n0.002\n4414.0\n3784.0\n1.0\n\n\n\n\n\n\n\nDiagnostics: Max r_hat = 1.0000\n\n\n\n\\(\\hat{R}\\) (R-hat) 指標\n数値 : すべてのパラメータにおいて、値が 1.00 になっています。\n解釈 : \\(\\hat{R}\\) は「チェーン間のばらつき」と「チェーン内のばらつき」を比較する指標です。 一般に 1.1 未満（厳密には 1.05 未満）であれば収束したとみなされます。 今回の結果は、全て 1.00 に極めて近いため、複数の独立した試行（チェーン）がすべて同じ統計的結論に達していることを意味します。\n\n# 収束結果の可視化\naz.plot_trace(inference, var_names=[\"mu_league\", \"sigma_goalie\"], compact=False)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nトレースプロットの形状（Fuzzy Caterpillar）\n各チェーン（異なる色の線）が互いにしっかりと混ざり合い、特定の傾向（右肩上がりや下がり）を持たず、一定の範囲を細かく上下に振動しています。 これがいわゆる「毛虫（Fuzzy Caterpillar）」のような状態であり、サンプラーが事後分布の全体を効率よく探索できていることを示します。 特定の場所で停滞したり、チェーンごとに大きく値が離れたりしていないため、良好な収束のサインになります。\nよって、問題なく推論できていると判断できます。",
    "crumbs": [
      "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト"
    ]
  },
  {
    "objectID": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#戦略的評価３層フィルタリング",
    "href": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#戦略的評価３層フィルタリング",
    "title": "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト",
    "section": "5. 戦略的評価（３層フィルタリング）",
    "text": "5. 戦略的評価（３層フィルタリング）\n\n第１層: HDI（統計的実装）\n実力が運ではなく、統計的に見てリーグ平均を超えているかを検証します。\n\nfrom privacy_utils import obfuscate_name  # 名前を匿名化\n\n# GSAx を算出（１試合 30シュートあたりに正規化）\nsamples_xg = inference.posterior[\"xG\"].values.reshape(-1, len(df_small))\nactual_goals = df_small[\"obs_shots\"].values\n\nlist_goalie_gsax = []\nfor idx_g in range(len(uniques_goalie)):\n    mask = (df_small[\"idx_goalie\"] == idx_g)\n    # GSAx = Σ(xG) - Σ(Actual)\n    dist_gsax = (samples_xg[:, mask].sum(axis=1) - actual_goals[mask].sum()) / (mask.sum() / 30)\n    list_goalie_gsax.append(dist_gsax)\n\n# データの集計とソート準備\n# 各ゴテンダーの「名前」「分布」「平均値」をセットにしてまとめます。\ndata_goalie_ranking = []\nfor idx_g, dist in enumerate(list_goalie_gsax):\n    data_goalie_ranking.append({\n        \"name\": id_to_name[idx_g],\n        \"dist\": dist,\n        \"mean\": dist.mean()\n    })\n\n# 平均値（Mean GSAx）で昇順にソート\n# plot_forest は「下から上」に向かって描画されるため、昇順にソートすることで一番上が「最高成績」になります\ndata_goalie_ranking.sort(key=lambda x: x[\"mean\"], reverse=True)\n\n# ArivZ に渡すための「名前付き辞書」を作成\n# このキー（名前）が縦軸のラベルとして採用されます。\n# obfuscate_name() で選手名を匿名化しています。関数定義については GitHub をご参考下さい。\ndict_plot = {obfuscate_name(item[\"name\"]): item[\"dist\"] for item in data_goalie_ranking}\n\n# HDI の可視化\naz.plot_forest(dict_plot,\n               hdi_prob=0.94,\n               combined=True,\n               figsize=(12, max(8, len(uniques_goalie) * 0.4))\n               )\n\n# --- 背景のゾーンを色分け ---\n# 現在のグラフの表示範囲を取得\nxmin, xmax = plt.xlim()\nlimit = max(abs(xmin), abs(xmax))  # 0 を中心にバランスをとるために設定\nplt.axvspan(-limit, 0, color=COLOR_RED, alpha=0.1, label=\"Underperforming Zone\")  # マイナスエリア: 赤\nplt.axvspan(0, limit, color=COLOR_GREEN, alpha=0.1, label=\"Overperforming Zone\")\nplt.axvline(0, color=COLOR_YELLOW, linestyle=\"-\", linewidth=2.5, label=\"League Average\")  # 境界線\n\nplt.axvspan(-0.05, 0.05, color=COLOR_GRAY, alpha=0.2, label=\"ROPE (Error Margin)\")\n\n# グラフの装飾\nplt.title(\"Goalie Performance Ranking: GSAx per 30 Shots\", fontsize=16, pad=25)\nplt.xlabel(\"Goals Saved Above Expected (GSAx)\")\nplt.grid(axis=\"x\", linestyle=\":\", alpha=0.5)\nplt.legend(loc=\"upper left\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHDI(Highest Density Interval)\n事後分布において、最も確率密度が高い範囲(94%)になります。 紫のバーが各選手の「実力はこの範囲に収まると 94% の自信を持って言える」というエラーバーになります。バー全体が 0 を超えていれば、その選手の成果は高いと言える可能性が高いと解釈できます。\n\nTips\n各選手の HDI の平均値順にソートし、リーグ全体の平均値(黄色の縦線) を境界線としてエリア分けをしています。\nGreen ゾーンに HDI がある選手: チームに貯金を作っている選手。統計的な確信を持って「獲得/維持すべき」という判断基準を与えることができます。\nRed ゾーンに HDI がある選手: 期待以上にゴールを許してしまっている選手。小規模データゆえ「たまたま不調」の可能性もありますが、現状の戦力外リスクとして注視すべきという情報を統計的な観点から与えることができます。\nYellowライン: リーグ平均の壁になります。ここを基準に左右どれだけ離れられるか、が選手の市場価値評価の鍵となります。\n\n\n\n第２層: ROPE（実質的評価）\n統計的な差異が、ビジネス上のインパクトを伴う「意味のある差」であるかを判定します。 各選手の分析結果（平均貢献度、傑出確率、判定ステータス）を構造化された表形式に変換し、外部ファイルとして永続化します。\n\nROPE_THRESHOLD = 0.05  # ±0.05点以内は実務上の誤差とみなす\nresults_rope = []  # 分析結果を格納するリストの準備\n\nfor i, dist in enumerate(list_goalie_gsax):\n    mean_gsax = dist.mean()\n    prob_above_rope = (dist &gt; ROPE_THRESHOLD).mean()\n\n    if mean_gsax &gt; ROPE_THRESHOLD:\n        status = \"Beyond ROPE (Outstanding)\"\n    elif mean_gsax &lt; -ROPE_THRESHOLD:\n        status = \"Below ROPE (Poor)\"\n    else:\n        status = \"Inside ROPE (Equivalent)\"\n\n    # リストに追加\n    results_rope.append({\n        \"Coalie\": obfuscate_name(id_to_name[i]),  # 選手名を匿名化\n        \"Mean_GSAx\": round(mean_gsax, 3),\n        \"Outstanding_Prob\": round(prob_above_rope * 100, 1),\n        \"Status\": status\n    })\n\n# DataFrame の作成とソート（貢献度順）\ndf_rope_analysis = pd.DataFrame(results_rope).sort_values(by=\"Mean_GSAx\", ascending=False)\n\ndf_rope_analysis\n\n\n\n\n\n\n\n\nCoalie\nMean_GSAx\nOutstanding_Prob\nStatus\n\n\n\n\n62\nA. K.\n2.847\n100.0\nBeyond ROPE (Outstanding)\n\n\n64\nV. H.\n2.757\n100.0\nBeyond ROPE (Outstanding)\n\n\n73\nJ. H.\n2.735\n100.0\nBeyond ROPE (Outstanding)\n\n\n69\nJ. S.\n2.706\n100.0\nBeyond ROPE (Outstanding)\n\n\n39\nD. R.\n2.667\n100.0\nBeyond ROPE (Outstanding)\n\n\n...\n...\n...\n...\n...\n\n\n50\nP. M.\n-5.048\n0.5\nBelow ROPE (Poor)\n\n\n36\nS. M.\n-5.944\n0.1\nBelow ROPE (Poor)\n\n\n14\nA. F.\n-6.085\n0.3\nBelow ROPE (Poor)\n\n\n37\nE. M.\n-6.390\n0.3\nBelow ROPE (Poor)\n\n\n1\nJ. D.\n-8.300\n0.0\nBelow ROPE (Poor)\n\n\n\n\n79 rows × 4 columns\n\n\n\n\n戦略的解釈：ROPE分析を通じた意思決定\n実ショットデータ 500 件に基づく、全 79 名のゴテンダーを対象とした ROPE（実質的等価領域）分析の結果を解釈します。この分析では、単なる平均値ではなく、場所の難易度を考慮した上での「真の貢献度」を評価しています。\n\n分析結果のサマリー\n全 79 名の分布は以下の通りです。\n\nBeyond ROPE (Outstanding): 55名\n\n統計的誤差を超えて、明らかにリーグ平均以上の価値を提供している選手。\n\nBelow ROPE (Poor): 23名\n\n統計的誤差を超えて、期待値以上の失点を許している、改善が必要な選手。\n\nInside ROPE (Equivalent): 1名 (F. G. 選手)\n\n数値上はプラスですが、実務上は「リーグ平均と等価」とみなすべき選手。\n\n\n\n\n\n\n注目選手の詳細分析\n\n1. トップパフォーマー：A. K., V. H., J. H. 選手\n\n平均 \\(GSAx\\) (2.7 〜 2.8): 1試合（30シュート想定）あたり、平均的なキーパーよりも 約 2.7 点以上多く防いでいます。\n傑出確率(100.0%): 彼らが「本物」である確率は 100% と推定され、運ではなく実力であることが統計的に断定されています。\nビジネス判断: 市場価値が急騰する前に確保すべき「最優先資産」です。\n\n\n\n2. ボーダーライン：F. G. 選手 (Inside ROPE)\n\n平均 \\(GSAx\\) (0.005): プラスの数値ではありますが、ROPE（\\(\\pm 0.05\\)）の範囲内です。\nビジネス判断: 現時点では「平均的な選手」と能力の差が認められません。高額な契約を提示するにはリスクが伴います。\n\n\n\n3. ボトムパフォーマー：J. D., E. M. 選手\n\n平均 \\(GSAx\\) (-8.3 〜 -6.3): 平均的な選手と比較して、1試合あたり 6〜8 点分多く失点している計算になります。\nビジネス判断: 早急な戦力補強、または守備システム自体の抜本的な見直しが必要です。\n\n\n\n\n\nビジネス・アクションプラン\n\n「隠れた名手」の特定: チームの勝率が低くても、このリストで Beyond ROPE に入っている選手は、「チームの守備力が低いだけで、個人は極めて優秀」な 割安の優良資産 である可能性が高いです。\n投資対効果の最大化: Outstanding Prob が高い選手に予算を集中させ、Inside ROPE の選手についてはコストパフォーマンスを厳格に評価します。\nエビデンスに基づく編成会議: 「なんとなく調子が悪い」といった主観を排除し、この \\(GSAx\\) と \\(ROPE\\) の指標を用いることで、法務・経営陣に対して論理的で透明性の高い編成案を提示できます。\n\n\n# CSVファイルとして保存（ビジネス用補助データ）\nFILE_OUTPUT = os.path.join(DIR_EXTRACT, \"data_goalie_rope_analysis.csv\")\n\ndf_rope_analysis.to_csv(FILE_OUTPUT, index=False)\n\nデータフレーム化（Tabularization）することで、どの選手が「実質的なプラスの差」を生み出しているかが一目で比較可能になります。また、CSVとしてファイル化しておくことで、「分析環境を持っていない経営層や現場スタッフ」が Excel 等でデータを確認できるようになり、組織全体の意思決定のスピードと質を向上させます。\n\n\n\n第３層: ROI (経済的評価)\n統計的評価を「ビジネスの投資判断」へ昇華させます。 GSAx を金額換算し、さらに「収益がプラスになる確率」と「確信の範囲（94% HDI）」を併記することで、リスクとリターンを一目で把握できる最終報告用の資料を生成します。\n\n# 経済価値のパラメータ設定（ビジネスモデルに合わせて調整可能）\nVALUE_GOAL = 1_000_000  # １ゴールを防ぐ価値 = 100万円\nANNUAL_GAMES = 60  # 年間の想定出場試合数\n\nresults_roi = []\n\nprint(\"Layer 3: ROI Assessment calculating...\")\n\n# list_goalie_gsax は各選手の「１試合（30本）あたりの GSAx分布」\nfor i, dist in enumerate(list_goalie_gsax):\n    profit_expected_annual = dist.mean() * ANNUAL_GAMES * VALUE_GOAL  # 年間の期待合計貢献額\n    prob_profitability = (dist &gt; 0).mean()  # 収益化確率（平均以上の貢献をする確率）\n\n    # 94% HDI による収益の確信範囲\n    hdi_lower, hdi_upper = az.hdi(dist, hdi_prob=0.94)\n    profit_hdi_lower = hdi_lower * ANNUAL_GAMES * VALUE_GOAL\n    profit_hdi_upper = hdi_upper * ANNUAL_GAMES * VALUE_GOAL\n\n    results_roi.append({\n        \"Goalie\": obfuscate_name(id_to_name[i]),\n        \"Expected_Annual_Profit\": int(profit_expected_annual),\n        \"Profitability_Prob\": round(prob_profitability * 100, 1),\n        \"Profit_HDI_Lower\": int(profit_hdi_lower),\n        \"Profit_HDI_Upper\": int(profit_hdi_upper)\n    })\n\n# DataFrame化とソート\ndf_roi = pd.DataFrame(results_roi).sort_values(by=\"Expected_Annual_Profit\", ascending=False)\n\ndf_roi\n\nLayer 3: ROI Assessment calculating...\n\n\n\n\n\n\n\n\n\nGoalie\nExpected_Annual_Profit\nProfitability_Prob\nProfit_HDI_Lower\nProfit_HDI_Upper\n\n\n\n\n62\nA. K.\n170790436\n100.0\n51198446\n297205298\n\n\n64\nV. H.\n165417100\n100.0\n53097600\n279289732\n\n\n73\nJ. H.\n164094708\n100.0\n42952293\n286368258\n\n\n69\nJ. S.\n162353045\n100.0\n39940961\n289616400\n\n\n39\nD. R.\n159999358\n100.0\n49822147\n268045832\n\n\n...\n...\n...\n...\n...\n...\n\n\n50\nP. M.\n-302851857\n0.5\n-397484645\n-195002825\n\n\n36\nS. M.\n-356628658\n0.1\n-426010415\n-280890982\n\n\n14\nA. F.\n-365104667\n0.3\n-453358070\n-245912157\n\n\n37\nE. M.\n-383429912\n0.3\n-479982156\n-251180136\n\n\n1\nJ. D.\n-497985072\n0.0\n-572780334\n-404946765\n\n\n\n\n79 rows × 5 columns\n\n\n\n\n# CSV として保存\nFILE_ROI = os.path.join(DIR_EXTRACT, \"goalie_final_investment_report.csv\")\ndf_roi.to_csv(FILE_ROI, index=False)\n\n統計量を「金額」と「リスク幅」に変換し、経営層が直感的に投資判断（契約更新や獲得交渉）を行える形式に可視化します。\n\n# 最終レポートの可視化（上位15名）\ndf_top = df_roi.head(15).copy()\n\n# 通貨単位（万円）に変換して見やすくする\ndf_top[\"Profit_Million\"] = df_top[\"Expected_Annual_Profit\"] / 10_000\ndf_top[\"Lower_Million\"] = df_top[\"Profit_HDI_Lower\"] / 10_000\ndf_top[\"Upper_Million\"] = df_top[\"Profit_HDI_Upper\"] / 10_000\n\n# プロット実行\n# 期待値のバーチャート\nplt.figure(figsize=(9, 6))\nbars = plt.barh(df_top[\"Goalie\"][::-1], df_top[\"Profit_Million\"][::-1],\n                color=COLOR_PURPLE, alpha=0.7, label=\"Expected Annual Contribution Studies\"\n                )\n\n# 不確実性のエラーバー（HDI）\nplt.errorbar(df_top[\"Profit_Million\"][::-1], np.arange(len(df_top)),\n             xerr=[(df_top[\"Profit_Million\"] - df_top[\"Lower_Million\"])[::-1],\n                   (df_top[\"Upper_Million\"] - df_top[\"Profit_Million\"])[::-1]],\n             fmt=\"none\", ecolor=COLOR_GRAY, capsize=5, label=\"94% HDI (Confidence range)\")\n\nplt.axvline(0, color=COLOR_RED, linestyle=\"--\", linewidth=1)\nplt.title(\"Final Investment Value Report: Expected Annual Economic Contribution (TOP 15)\", fontsize=16,\n          fontweight=\"bold\", pad=20)\nplt.xlabel(\"Expected contribution amount (unit: 10,000 yen)\", fontsize=12)\nplt.ylabel(\"Player\", fontsize=12)\nplt.legend(loc=\"lower right\")\n\n# 値のラベル付け\nfor i, v in enumerate(df_top[\"Profit_Million\"][::-1]):\n    plt.text(v + 50, i, f\"{v:,.0f}yen\", va=\"center\", fontweight=\"bold\", color=COLOR_YELLOW)\n\nplt.tight_layout()\nIMG_FINAL_REPORT = os.path.join(DIR_EXTRACT, \"final_roi_report_chart.png\")\nplt.savefig(IMG_FINAL_REPORT, dpi=150)\nplt.show()\n\n\n\n\n\n\n\n\nゴテンダーのパフォーマンスがチームのキャッシュフローに対して億単位のインパクトを与えることを示唆しています。\n\nトップ層（上位 5 名：A. K., V. H. 等）の価値\n\n期待収益: リーグ平均の選手と比較して、年間で 約 1.6 億円 〜 1.7 億円 の損失（失点）を回避する価値があると算出されました。\n解釈: 彼らに対して 1 億円の年俸を支払ったとしても、なおチームに数千万円の「純利益（失点抑制による勝利への貢献）」をもたらす計算になります。\n\n\ndf_top[[\"Goalie\", \"Expected_Annual_Profit\", \"Profitability_Prob\", \"Profit_HDI_Lower\", \"Profit_HDI_Upper\"]]\n\n\n\n\n\n\n\n\nGoalie\nExpected_Annual_Profit\nProfitability_Prob\nProfit_HDI_Lower\nProfit_HDI_Upper\n\n\n\n\n62\nA. K.\n170790436\n100.0\n51198446\n297205298\n\n\n64\nV. H.\n165417100\n100.0\n53097600\n279289732\n\n\n73\nJ. H.\n164094708\n100.0\n42952293\n286368258\n\n\n69\nJ. S.\n162353045\n100.0\n39940961\n289616400\n\n\n39\nD. R.\n159999358\n100.0\n49822147\n268045832\n\n\n44\nJ. G.\n139588811\n100.0\n28147238\n254939718\n\n\n34\nI. S.\n138419304\n100.0\n46628693\n223657791\n\n\n12\nC. I.\n137314100\n100.0\n42632083\n228914780\n\n\n22\nJ. M.\n136509122\n100.0\n43062664\n227819462\n\n\n31\nJ. Q.\n135496606\n100.0\n37540900\n218786598\n\n\n35\nE. C.\n131216185\n100.0\n42877182\n217479135\n\n\n56\nC. P.\n124674332\n100.0\n40466382\n213457998\n\n\n28\nI. S.\n121988835\n100.0\n33182011\n199023392\n\n\n78\nD. C.\n121341191\n100.0\n28310071\n228263486\n\n\n20\nJ. B.\n120769854\n100.0\n42119516\n198761783\n\n\n\n\n\n\n\n\nExpected_Annual_Profit(推定資産価値): 予算編成（年俸総額）の基準とする。\nProfitabilly_Prob: 100%に近い選手は「コアメンバー」として固定。\nProfit_HDI_Range: 最悪のシナリオ（Lower）でも赤字にならないかを確認。\n\n\nExpected Annual Profit (期待年間貢献額)\n選手が平均的なゴテンダーと比較して、年間（60試合）でどれだけの失点を防ぎ、それを金額換算（1失点=100万円）したものになります。 「この選手を起用することで、チームの損失を何千万円防げるか？」という指標です。\n\nこの ROI レポートは、現場（コーチ陣）と経営（フロント）の架け橋になります。 「なぜ彼を獲得するのに 1.5 億円必要なのか？」 という問いに対し、 「彼は場所の難易度を考慮しても年間で 1.7 億円分の失点を防ぐため、実質的に 2,000 万円のプラスを生む投資だからです」 と、科学的根拠（エビデンス）を持って答えられるようになります。\n\n\n\nベイズによるスポーツビジネスの不確実へアプローチ\n2024年シーズンの実ショットデータを用い、ベイズ階層モデルによるゴテンダーの貢献度評価（GSAx）から、最終的な経済価値（ROI）の算出までを完結させました。 スポーツアナリティクスを題材にあえて小規模データにし「データの少なさ」と「直感への依存」というリスクの中での分析を実行しました。本分析が提示したアプローチで、不確実性を排除するためのユースケースをご提案出来たかと思います。\n\n1. 統計的実証（HDI）による「運」の排除\n「たまたま数試合調子が良かっただけではないか？」という疑念に対し、HDI（最高密度区間） は科学的な回答を与えます。 500件という限られたデータであっても、階層モデルによる「収縮推定（Shrinkage）」を行うことで、過学習を防ぎつつ、選手が持つ真の実力を表現しました。\n\n\n2. 実質的評価（ROPE）による「ノイズ」の排除\n数字上の微細な差は、ビジネスにおいての判断を難しくさせます。ROPE（実質的等価領域） を導入することで、統計的に有意であっても実務的に意味のない差を「ノイズ」として切り捨て、 投資すべき「対象（Beyond ROPE）」を明確にし判断における有用な情報を提供できました。\n\n\n3. 経済的評価（ROI）による「言語」の統一\n実際のビジネスの現場では、現場と経営の言葉が通じない状況が発生します。本プロジェクトでは GSAx を「年間期待経済貢献額（円）」 という通貨単位に翻訳することで\n\n「A. K. 選手は GSAx が高い」（現場の言葉）\n「A. K. 選手は年間で 1.7 億円の損失を回避する投資価値がある」（経営の言葉）\n\nという対話が可能になり迅速かつ合理的な予算編成と契約交渉を可能にします。\n\n\n倫理的・専門的な配慮：プライバシー・バイ・デザイン\n分析の過程で実施した 「選手名の秘匿化（イニシャル化）」 は、法務リスクの回避に加えて 特定の個人をランク付けして批判するのではなく、「評価システムそのものの妥当性」と「意思決定プロセスの透明性」に焦点をつなげるという利点もあります。\n\n\nまとめ\nベイズ統計は、「不確実性を確率として捉え、リスクを許容可能な範囲に収める」 ための良い手段になります。 本分析を通じて示された「科学的な選手評価」は、スポーツビジネスにおける勝率を上げ、持続可能な成長を実現するための基盤構築に繋げることが出来ます。",
    "crumbs": [
      "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト"
    ]
  }
]