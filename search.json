[
  {
    "objectID": "notebooks/01_web_performance/roi_decision.html",
    "href": "notebooks/01_web_performance/roi_decision.html",
    "title": "エグゼクティブサマリー",
    "section": "",
    "text": "【分析の背景】 サイト全体の平均LCP改善という表面的な指標に隠れた、決済ページ等の重要ページにおけるパフォーマンス改悪がビジネスに与える損失リスクを評価しました。本分析では、PyMC v5を用いた階層ベイズモデルに加え、ビジネス前提（売上感度）の不確実性を加味した「感度分析」を実行し、意思決定の妥当性を検証しています。\n【主要な発見】 * 期待純利益: Top/Detailページの改善効果により、全体では約 8,210万円 の大幅な増収が期待値として算出されました。 * 収益化確率（勝率）: しかし、全体の収益がプラスになる確率は 65.1% に留まり、約3回に1回は赤字になる という不安定な状態です。 * 潜在的リスクの特定: 最大の懸念点は「決済ページ（Checkout）」です。このページ単体では 77.4% の確率で1万円以上の損失が発生 しており、Topページの稼いだ利益を食いつぶす明確な「出血点」となっています。\n【最終アクション】 「条件付きリリース」 を推奨します。全体を一括リリースすることは、Checkoutページのリスク（勝率65%）が高すぎるため却下とします。 リスクの低い Top/Detail ページの改善のみを部分的にリリース し、Checkout ページに関しては実装をロールバックまたは修正した後、再検証を行うべきです。\n\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport japanize_matplotlib\n\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/arviz/__init__.py:39: FutureWarning: \nArviZ is undergoing a major refactor to improve flexibility and extensibility while maintaining a user-friendly interface.\nSome upcoming changes may be backward incompatible.\nFor details and migration guidance, visit: https://python.arviz.org/en/latest/user_guide/migration_guide.html\n  warn(\n\n\n\n# カラー定義\nCOLOR_PURPLE = \"#9B5DE5\"  # 事後分布・HDI\nCOLOR_YELLOW = \"#F9C74F\"  # ROPE領域\nCOLOR_GREEN  = \"#06D6A0\"  # 改善判定\nCOLOR_RED    = \"#EF476F\"  # 悪化判定\nCOLOR_GRAY   = \"#8D99AE\"  # 等価判定\n\n# 1. Matplotlibのデフォルトカラーサイクルを変更\nfrom cycler import cycler\nplt.rcParams['axes.prop_cycle'] = cycler(color=[COLOR_PURPLE, COLOR_YELLOW, COLOR_GREEN, COLOR_RED, COLOR_GRAY])\n\n# 2. Seabornのスタイル設定\nsns.set_style(\"whitegrid\")\nsns.set_palette([COLOR_PURPLE, COLOR_YELLOW, COLOR_GREEN, COLOR_RED, COLOR_GRAY])\n\n\nprint(f\"PyMC version: {pm.__version__}\")\n\n# 再現性の確保\nRANDOM_SEED = 42\nrng = np.random.default_rng(RANDOM_SEED)\n\nPyMC version: 5.26.1\n\n\n\n# --- 1. Data の生成 (N=20 小規模Data) ---\ndef generate_weighted_scenario_data(n_per_page=20):\n    \"\"\"\n    Top Page 改善、Checkout page 悪化の Trap-data 生成\n    \"\"\"\n    pages = [\"Top\", \"Detail\", \"Contract\", \"Checkout\"]\n\n    scenario = {\n        'Top': (3000, 2500),  # 改善（良）\n        'Detail': (2800, 2600),  # （微良）\n        'Contract': (3500, 3550),  # 変化なし(微増)\n        'Checkout': (3000, 4000)  #  悪化　★ここが罠\n    }\n\n    data = []\n    for page in pages:\n        mu_pre, mu_post = scenario[page]\n\n        # Pre (対数正規分布)\n        lcp_pre = rng.lognormal(mean=np.log(mu_pre), sigma=0.4, size=n_per_page)\n        data.extend([{\"page\": page, \"group\": \"pre\", \"lcp\": val} for val in lcp_pre])\n\n        # Post (対数正規分布)\n        lcp_post = rng.lognormal(mean=np.log(mu_post), sigma=0.4, size=n_per_page)\n        data.extend([{\"page\": page, \"group\": \"post\", \"lcp\": val} for val in lcp_post])\n\n    return pd.DataFrame(data)\n\n\ndf = generate_weighted_scenario_data(n_per_page=20)\n\n\n# 基本統計量の確認\nprint(\"--- 基本統計量（Group × Page） ---\")\n\ndf.groupby(\"group\").agg({\"lcp\": \"mean\"})\n\n--- 基本統計量（Group × Page） ---\n\n\n\n\n\n\n\n\n\nlcp\n\n\ngroup\n\n\n\n\n\npost\n3196.734803\n\n\npre\n3225.695790\n\n\n\n\n\n\n\n単純に df.groupby(\"group\").agg({\"lcp\": \"mean\"}) を見ると、Sample数が均等なため - Pre: 3225ms -&gt; Post: 3196ms\nと、全体として大きな変化はありません。\n\ndf.groupby([\"page\", \"group\"]).agg({\"lcp\": \"mean\"}).unstack()  # 確認\n\n# unstack() は、MultiIndex（重層的なインデックス）を持つ Series や DataFrame のインデックスの最下層を、カラム（列）へと展開（ピボット）する効果があります。\n# このコードの場合、groupby([\"page\", \"group\"]) によってインデックスが page と group の二段構えになりますが、\n# .unstack() を付けることで、group（pre/Post）が横に並び、ページごとの比較がしやすくなります。\n\n\n\n\n\n\n\n\nlcp\n\n\ngroup\npost\npre\n\n\npage\n\n\n\n\n\n\nCheckout\n3840.892359\n3570.660555\n\n\nContract\n3587.029423\n3158.051664\n\n\nDetail\n2607.372864\n3050.224780\n\n\nTop\n2751.644568\n3123.846159\n\n\n\n\n\n\n\nCheckout を見ると明確に悪化しています。\n\n# 分布の可視化\nfig, axes = plt.subplots(2, 2, figsize=(18, 9))\nax_flat = axes.flatten()\n\nfor i, page in enumerate(df[\"page\"].unique()):\n    sns.histplot(data=df[df[\"page\"] == page], x=\"lcp\", hue=\"group\", kde=True, log_scale=True, ax=ax_flat[i])\n    ax_flat[i].set_title(f\"LCP {page}\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nこの可視化において注目すべき点は、「平均値の罠」が視覚的に表現されていることです。\n\nTopページとDetailページ（改善傾向）: 青色（Pre）に比べて、オレンジ色（Post）の分布が左側（LCPが小さい、つまり高速な方向）にシフトしていることが見て取れます。特にTopページでは、ヒストグラムの山が明確に左へ移動しており、パフォーマンス改善の恩恵を最も受けていることがわかります。\nContractページ（変化なし）: PreとPostの分布がほぼ重なっており、施策による影響がほとんど見られません。\nCheckoutページ（改悪傾向：重要）: ここが本分析の「罠」となる部分です。他のページとは逆に、オレンジ色（Post）の分布が右側（LCPが大きい、つまり低速な方向）へシフトし、裾野も広がっています。これは、決済画面において明確なパフォーマンス低下が発生していることを示唆しています。\n\n\n技術的補足と直感的な説明\n\n対数正規分布と対数スケール (Log Scale) 【定義】: LCPのような待ち時間データは、下限が0で右側に長い裾を持つ性質があるため、通常は対数正規分布に従います。このプロットでは log_scale=True を使用することで、歪んだデータを正規分布に近い形に変換し、中心傾向（最頻値や中央値）のズレを視覚的に比較しやすくしています。 【直感】: 待ち時間のデータは、「すごく遅い人」が一部混じるため、単純な平均をとると実態を見誤ります。このグラフでは、データの「偏り」を調整して、PreとPostの「山の位置」がどれくらいズレたかを、人間の目で直感的に捉えやすい形に整えています。\nカーネル密度推定 (KDE: Kernel Density Estimation) 【定義】: ヒストグラムに重なっている曲線はカーネル密度推定です。これは、離散的なデータポイントから連続的な確率密度関数を推定する手法であり、ヒストグラムのビン（棒）の切り方に依存せずに、データの分布形状を滑らかに表現します。 【直感】: 点々としたデータの集まりを、「なだらかな山」として描いたものです。山の形を見ることで、「だいたい何秒くらいのユーザーが一番多いのか」や「分布のばらつき（山の幅）」を一目で比較できるようになります。\n\n\n\n結論としての意思決定支援\nこのプロットから得られるビジネス上の洞察は、「全ページ合計の平均値では改善しているように見えても、最も売上に直結する Checkout ページで致命的な遅延が発生している」というリスクの可視化です。\n\n\n非中心化による階層ベイズモデル\nN=20のような小規模データで階層モデル（特に分散パラメータ \\(\\sigma\\) が小さい場合）を推定すると、MCMCサンプラーが「漏斗（Funnel）のような形状」の確率分布を探索できず、Divergences（発散） というエラーが頻発することがあります。 これはベイズ推論の信頼性を損ってしまう為、回避する為に、変数の依存関係を数式上で切り離す 「非中心化」 テクニックを用います。 - 中心化（Centered）: \\(\\beta \\sim \\text{Normal}(\\mu, \\sigma)\\) - \\(\\beta\\) の値が決まる際、\\(\\mu\\) と \\(\\sigma\\) に直接依存する。 - 非中心化（Non-centered）: \\(z \\sim \\text{Normal}(0, 1)\\), \\(\\beta = \\mu + z \\cdot \\sigma\\) - \\(z\\) は標準正規分布から独立に生成され、あとでスケーリングされる。これによりサンプラーがスムーズに空間を移動できる。\n\nidx_page, pages = pd.factorize(df[\"page\"])\nidx_group, groups = pd.factorize(df[\"group\"])\n\n# PyMC用 Coords (座標) 定義\ncoords = {\n    \"id_obs\": df.index.values,\n    \"page\": pages,\n    \"group\": groups  # [\"Pre\", \"Post\"]\"\n}\n\n複雑な階層モデルの可読性をあげる為、PyMC における - カテゴリ変数のIndex化 - 【定義】: モデル内部での行列演算を効率化するため、文字列（“Top”, “Checkout”等）を整数インデックス（0, 1, …）に変換する処理です。これにより、各観測データがどのグループ（階層）に属するかを、数式内でポインタのように参照できるようになります。 - 【直感】: 「住所録」に名前でアクセスするのではなく、「出席番号」を割り振る作業です。「出席番号1番のLCPはこれ」と番号で管理することで、計算機が迷わず高速にデータを処理できるようになります。 - Coords(座標)の定義 - 【定義】: 確率変数の各次元（Dimension）に対して、人間が理解できるラベルを付与する仕組みです。PyMC（内部的にはxarrayを使用）において、多次元配列の各軸が何を意味しているのか（どのページか、どのグループか）を明示的に宣言します。 - 【直感】: グラフの「軸（ラベル）」を定義することです。ただの「4行2列の数字の塊」に、「縦軸はページ名（Top等）」「横軸は施策の前後（Pre/Post）」という名前を付けることで、後で結果を見たときに「どの数字がCheckoutページのPostの結果か」を一目でわかるようにします。\nというプロセスを実施します。\n\nなぜこれらが必要なのか？（主な3つの目的）\n\n高次元データの整合性確保 (Error Prevention) 階層モデルでは、「ページ(4) × グループ(2)」のようにパラメータが多次元になります。 Coordsを定義しておくと、PyMCが自動的に「4×2の行列」と「観測データ（N=160）」を正しくマッピングしてくれます。これにより、行列のサイズが合わないといった実行時エラーを未然に防ぐことができます。\n意味のある事後分布の解析 (Labeling) MCMCによるサンプリング結果（事後分布）を解析する際、Coordsが定義されていると、ArviZなどのツールを使って以下のように直感的にアクセスできます。 trace.posterior[“mu_cell”].sel(page=“Checkout”, group=“Post”) もしCoordsがないと、trace.posterior[“mu_cell”][:, :, 3, 1] のように「マジックナンバー（3や1が何を指すか不明な状態）」で指定しなければならず、ミスや混乱の元になります。\n非中心化実装の柔軟性 今回のモデルで採用している「非中心化（Non-centered）」モデルでは、標準正規分布からサンプリングした値にズレ（）を掛け合わせる操作を行います。 Coordsがあると、この「ズレ」をどの次元に対して適用するのかを dims=(“page”, “group”) のように名前で指定でき、モデルの構造が非常に読みやすくなります。\n\nこのように、Index化とCoordsの定義は、 「計算機のための効率化」と「人間のための可読性・解析性」 を両立させるための非常に重要なベストプラクティスです。これを行うことで、将来的にページ数が増えたり、新しいセグメント（デバイス別など）を追加したりする場合も、モデルの拡張が容易になります\n\nwith pm.Model(coords=coords) as model:\n    # --- Data Containers ---\n    _idx_page  = pm.Data(\"idx_page\", idx_page, dims=\"id_obs\")\n    _idx_group = pm.Data(\"idx_group\", idx_group, dims=\"id_obs\")\n    _obs_lcp   = pm.Data(\"obs_lcp\", df[\"lcp\"].values, dims=(\"id_obs\"))\n\n    # --- Hierarchical Priors (Non-centered Parameterization)\n\n    # 1. Global Intercept (全体の基準値)\n    mu_global = pm.Normal(\"mu_global\", mu=7.5, sigma=1.0, dims=\"group\")\n\n    # 2. Page-level Deviations (Page ごとのクせ)\n    # sigma_page: ページ間のばらつきの大きさ\n    sigma_page = pm.HalfNormal(\"sigma_page\", sigma=0.5)\n\n    # offset_page_z: 標準正規分布に従う「ズレの素」（非中心化の肝）\n    # shape=(4page, 2group) -&gt; 各ページ・各グループごとに固有の偏差を持つ\n    offset_page_z = pm.Normal(\"offset_page_z\", mu=0.0, sigma=1.0, dims=(\"page\", \"group\"))\n\n    # Deterministic で実際に偏差に変換: beta = mu + z * sigma\n    mu_page = pm.Deterministic(\n        \"mu_page\",\n        mu_global + offset_page_z * sigma_page,  # broadcasting\n        dims=(\"page\", \"group\")\n    )\n\n    # --- Likelihood ---\n    sigma_obs = pm.HalfNormal(\"sigma_obs\", sigma=0.5)\n\n    # 予測値の構築\n    # mu_page は (page, group) の２次元配列なので、Index で参照\n    mu_LCP = mu_page[_idx_page, _idx_group]\n\n    lcp = pm.Lognormal(\"lcp\", mu=mu_LCP, sigma=sigma_obs, observed=_obs_lcp, dims=(\"id_obs\"))\n\n\npm.model_to_graphviz(model)\n\n\n\n\n\n\n\n\n\n\n\n非中心化による階層ベイズモデル\nこの構造により、モデルは「情報の借用（Shrinkage）」を適切に行いつつ、小規模データでもサンプリングが収束しやすい（発散しにくい）堅牢なものになっています。 - 【直感】: 個性の強さ」と「個々のズレ」を分ける 「ページごとの個性がどれくらい強いか（sigma_page）」というボリュームのつまみと、 「各ページが標準からどの方向にズレているか（offset_page_z）」という方向のつまみを分けたイメージです。 これにより、データが少ないCheckoutページなどの推定値が、全体の平均（mu_global）に向かって適切に引き寄せられ、極端な外れ値に振り回されるのを防ぎます。\n\n# --- 推論 ---\nwith model:\n    trace = pm.sample(draws=2000, tune=1000, target_accept=0.95, random_seed=RANDOM_SEED)\n\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu_global, sigma_page, offset_page_z, sigma_obs]\n\n\n\n\n\n\n\n\nSampling 4 chains for 1_000 tune and 2_000 draw iterations (4_000 + 8_000 draws total) took 20 seconds.\n\n\n\n# 推論結果\naz.plot_trace(trace, compact=False, var_names=[\"mu_global\", \"sigma_page\", \"offset_page_z\"])\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n1. トレースプロットの形状（Fuzzy Caterpillar）\n見た目の特徴: 各チェーン（異なる色の線）が互いにしっかりと混ざり合い、特定の傾向（右肩上がりや下がり）を持たず、一定の範囲を細かく上下に振動しています。\n解釈: これがいわゆる「毛虫（Fuzzy Caterpillar）」のような状態であり、サンプラーが事後分布の全体を効率よく探索できていることを示します。 特定の場所で停滞したり、チェーンごとに大きく値が離れたりしていないため、良好な収束のサインです。\n\naz.summary(trace, var_names=[\"mu_global\", \"sigma_page\", \"offset_page_z\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nmu_global[pre]\n8.010\n0.082\n7.854\n8.166\n0.001\n0.002\n3190.0\n3060.0\n1.0\n\n\nmu_global[post]\n8.000\n0.082\n7.841\n8.157\n0.001\n0.002\n3341.0\n3321.0\n1.0\n\n\nsigma_page\n0.130\n0.070\n0.007\n0.251\n0.002\n0.002\n1738.0\n2415.0\n1.0\n\n\noffset_page_z[Top, pre]\n-0.082\n0.709\n-1.399\n1.297\n0.010\n0.008\n5245.0\n5857.0\n1.0\n\n\noffset_page_z[Top, post]\n-0.701\n0.740\n-2.079\n0.713\n0.010\n0.009\n4986.0\n5565.0\n1.0\n\n\noffset_page_z[Detail, pre]\n-0.130\n0.724\n-1.475\n1.238\n0.010\n0.009\n4953.0\n5164.0\n1.0\n\n\noffset_page_z[Detail, post]\n-0.922\n0.743\n-2.343\n0.441\n0.011\n0.008\n4651.0\n5738.0\n1.0\n\n\noffset_page_z[Contract, pre]\n0.038\n0.710\n-1.280\n1.388\n0.010\n0.008\n5021.0\n5358.0\n1.0\n\n\noffset_page_z[Contract, post]\n0.689\n0.728\n-0.703\n2.014\n0.011\n0.009\n4769.0\n5210.0\n1.0\n\n\noffset_page_z[Checkout, pre]\n0.289\n0.716\n-1.009\n1.689\n0.010\n0.009\n4986.0\n5239.0\n1.0\n\n\noffset_page_z[Checkout, post]\n0.981\n0.754\n-0.393\n2.427\n0.011\n0.010\n4460.0\n4933.0\n1.0\n\n\n\n\n\n\n\n\n\n2. \\(\\hat{R}\\) (R-hat) 指標\n数値 : すべてのパラメータにおいて、値が 1.00 になっています。\n解釈 : \\(\\hat{R}\\) は「チェーン間のばらつき」と「チェーン内のばらつき」を比較する指標です。 一般に 1.1 未満（厳密には 1.05 未満）であれば収束したとみなされます。 今回の結果は、全て 1.00 に極めて近いため、複数の独立した試行（チェーン）がすべて同じ統計的結論に達していることを意味します。\n\n\n3. 効サンプルサイズ (ESS: Effective Sample Size)\ness_bulk および ess_tail カラムを確認してください。\n数値 : 今回の設定（draws=2000, chains=4 の場合、計 8000 サンプル）に対して、十分な大きさ（数百〜数千以上）が確保されています。\n解釈 : MCMC のサンプルは前後の値に相関があるため、実際のサンプル数よりも「有効な（独立した）情報量」は少なくなります。 ESS が大きいことは、自己相関が低く、事後分布の平均や裾（テイル）の推定が安定していることを示します。\n1, 2, 3, より MCMC は、非常に良好に収束しており、推論結果は十分に信頼できる状態と判断。\n\n# --- Shrinkage (情報の借用)の可視化 ---\n\n# 1. Source-data の平均（MEL: 最尤推定に相当）\nmeans_raw_log = {}\nfor page in pages:\n    for group in groups:\n        mask = (df[\"page\"] == page) & (df[\"group\"] == group)\n        means_raw_log[(page, group)] = np.log(df.loc[mask, \"lcp\"]).mean()\n\n# 2. 事後分布の平均（Bayes 推定値）\nmu_posterior_page = trace.posterior[\"mu_page\"].mean(dim=[\"chain\", \"draw\"])\n\n# 3. 全体の平均 (Grand Mean)\nmu_posterior_global = trace.posterior[\"mu_global\"].mean(dim=\n                                                        [\"chain\", \"draw\"])\n\n# --- 可視化 ---\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\ncolors = plt.cm.tab10.colors  # Color-Palet\n\nfor i, group in enumerate(groups):\n    ax = axes[i]\n\n    # 各ページのポイント\n    for j, page in enumerate(pages):\n        # Source-data の平均（x軸） - Dict から取得\n        mean_raw = means_raw_log[(page, group)]\n\n        # Bayes推定の事後平均（y軸）\n        mean_bayes = float(mu_posterior_page.sel(page=page, group=group).values)\n\n        # Plot\n        ax.scatter(mean_raw, mean_bayes, s=150, color=colors[j], zorder=3)\n        ax.annotate(page, (mean_raw, mean_bayes), textcoords=\"offset points\", xytext=(8, 5), fontsize=11)\n\n    # Grand Mean (全体平均)の水平線\n    mean_grand = float(mu_posterior_global.sel(group=group).values)\n    ax.axhline(mean_grand, color=\"red\", linestyle=\"--\", alpha=0.7, label=f\"Grand Mean ({np.exp(mean_grand):.0f}ms\")\n\n    # 45度線（Shrinkage なしの場合）\n    x_all = [means_raw_log[(p, group)] for p in pages]\n    y_all = [float(mu_posterior_page.sel(page=p, group=group).values) for p in pages]\n    val_min = min(x_all + y_all) - 0.1\n    val_max = max(x_all + y_all) + 0.1\n\n    ax.plot([val_min, val_max], [val_min, val_max], color=\"gray\", linestyle=\":\", alpha=0.5, label=\"No Shrinkage (45°)\")\n    ax.set_xlim(val_min, val_max)\n    ax.set_ylim(val_min, val_max)\n\n    ax.set_xlabel(\"Raw Data Mean (log scale)\", fontsize=12)\n    ax.set_ylabel(\"Bayesian Posterior Mean (log scale)\", fontsize=12)\n    ax.set_title(f\"Shrinkage Plot: {group}\", fontsize=14)\n    ax.legend(loc=\"lower right\")\n    ax.grid(True, alpha=0.3)\n    ax.set_aspect(\"equal\", adjustable=\"box\")\n\nplt.suptitle(\"Shrinkage (情報の借用) の可視化\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_13667/2364994097.py:58: UserWarning: Glyph 24773 (\\N{CJK UNIFIED IDEOGRAPH-60C5}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/tmp/ipykernel_13667/2364994097.py:58: UserWarning: Glyph 22577 (\\N{CJK UNIFIED IDEOGRAPH-5831}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/tmp/ipykernel_13667/2364994097.py:58: UserWarning: Glyph 12398 (\\N{HIRAGANA LETTER NO}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/tmp/ipykernel_13667/2364994097.py:58: UserWarning: Glyph 20511 (\\N{CJK UNIFIED IDEOGRAPH-501F}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/tmp/ipykernel_13667/2364994097.py:58: UserWarning: Glyph 29992 (\\N{CJK UNIFIED IDEOGRAPH-7528}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/tmp/ipykernel_13667/2364994097.py:58: UserWarning: Glyph 21487 (\\N{CJK UNIFIED IDEOGRAPH-53EF}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/tmp/ipykernel_13667/2364994097.py:58: UserWarning: Glyph 35222 (\\N{CJK UNIFIED IDEOGRAPH-8996}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/tmp/ipykernel_13667/2364994097.py:58: UserWarning: Glyph 21270 (\\N{CJK UNIFIED IDEOGRAPH-5316}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 24773 (\\N{CJK UNIFIED IDEOGRAPH-60C5}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 22577 (\\N{CJK UNIFIED IDEOGRAPH-5831}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 12398 (\\N{HIRAGANA LETTER NO}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 20511 (\\N{CJK UNIFIED IDEOGRAPH-501F}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 29992 (\\N{CJK UNIFIED IDEOGRAPH-7528}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 21487 (\\N{CJK UNIFIED IDEOGRAPH-53EF}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 35222 (\\N{CJK UNIFIED IDEOGRAPH-8996}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 21270 (\\N{CJK UNIFIED IDEOGRAPH-5316}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\n\n\nPlot の構成要素\n\nX軸 (Raw Data Mean) : 生データから計算した各ページの平均 LCP（対数スケール）。最尤推定（MLE）に相当します。\nY軸 (Bayesian Posterior Mean) : 階層ベイズモデルによる推定値（事後平均）\n各点（Top, Detail, Contract, Checkout） : 各ページの推定値を表すプロット\n赤い破線（Grand Mean） : 全ページの全体平均（階層モデルの「親」の推定値）\n灰色の点線（45度線） : Shrinkage がない場合の基準線（生データ = ベイズ推定値）\n\n\n\n解釈\n今回の N=20 という小規模データでは： すべてのページで若干の Shrinkage が発生しているはずです（点が45度線から少し離れている）。 - Grand Mean に近いページ（例: Contract）は、あまり Shrinkage の影響を受けません（もともと全体平均に近いため）。 - Grand Mean から離れたページ（例: Top や Checkout）は、より強く全体平均に引き寄せられます。\nCheckout ページのデータは N=20 しかないが、階層モデルが他のページの傾向を借用して推定値を安定させている様子です。 その結果、生データよりも信頼できる推定値が得られていると判断できます。\n\n\n解釈のポイント\n\n\n45度線との位置関係\n\n\n点が45度線上にある場合: Shrinkage なし。生データの平均がそのまま推定値として採用されています。\n点が45度線から離れて赤線（Grand Mean）に近づいている場合: Shrinkage が働いています。モデルが「このページのデータだけでは不確実なので、他のページの情報を借りて調整しよう」と判断しています。\n\n\n\n\nShrinkage が起こる理由 階層ベイズモデルでは、データが少ない or ばらつきが大きいグループほど、全体平均に向かって「引き寄せられる」傾向があります。\n\n\n\n\n以下のような効果をもたらします：\n\n\nデータが少ない : 外れ値に振り回されるのを防ぐ\nばらつきが大きい : 過度に極端な推定を避ける\nデータが十分にある : 生データをほぼそのまま信頼\n\n\n\n# Shrinkage の強さを数値化\nprint(\"--- Shrinkage Analysis ---\")\nprint(f\"{'Page':&lt;12} {'Group':&lt;6} {'Raw (ms)':&lt;12} {'Bayes (ms)':&lt;12} {'Shrinkage %':&lt;12}\")\nprint(\"-\" * 60)\n\nfor page in pages:\n    for group in groups:\n        raw = means_raw_log[(page, group)]\n        bayes = float(mu_posterior_page.sel(page=page, group=group).values)\n        grand = float(mu_posterior_global.sel(group=group).values)\n\n        # Shrinkage率: Source-data から全体平均にどれだけ引き寄せられたか\n        if abs(raw - grand) &gt; 0.001:\n            pct_shrinkage = (raw - bayes) / (raw - grand) * 100\n        else:\n            pct_shrinkage = 0.0\n\n        # 実空間(ms)に変換して表示\n        print(f\"{page:&lt;12} {group:&lt;6} {np.exp(raw):&lt;12.0f} {np.exp(bayes):&lt;12.0f} {pct_shrinkage:&lt;12.1f}%\")\n\n--- Shrinkage Analysis ---\nPage         Group  Raw (ms)     Bayes (ms)   Shrinkage % \n------------------------------------------------------------\nTop          pre    2961         2982         41.9        %\nTop          post   2612         2736         35.1        %\nDetail       pre    2937         2966         39.5        %\nDetail       post   2505         2666         35.8        %\nContract     pre    3038         3029         32.5        %\nContract     post   3396         3246         34.6        %\nCheckout     pre    3176         3121         32.9        %\nCheckout     post   3590         3362         35.2        %\n\n\n\n\n\nROPE 判定（実質的等価姓の確認）\nROPE定義 : 「変化が ±50ms 以内であれば、User は気づかない」と定義する。\n\n# 事後分布の抽出と Data 整形\nposterior = trace.posterior\n\n# mu_page: (chain, draw, page, group) -&gt; (samples, page, group)\n# stack chain and draw\nmu_samples = posterior[\"mu_page\"].stack(sample=(\"chain\", \"draw\")).values.transpose(2, 0, 1)\n\n# 対数空間から実空間(ms)へ\nsamples_lcp_ms = np.exp(mu_samples)\n# shape: (samples, 4_pages, 2_groups) -&gt; 0:Pre, 1:Post\n\n# 改善量 (Pre - Post)\nms_diff = samples_lcp_ms[:, :, 0] - samples_lcp_ms[:, :, 1]\n# shape: (samples, 4_pages)\n\n\n# --- ROPE Analysis ---\nROPE_RANGE = [-100, 100]  # ±100ms は誤差とみなす\n\nprint(\"--- ROPE Analysis (Probability of Practical Effect)---\")\nfor i, page in enumerate(pages):\n    # ROPE内に入っている確立\n    prob_in_rope = np.mean((ms_diff[:, i] &gt; ROPE_RANGE[0]) & (ms_diff[:, i] &lt; ROPE_RANGE[1]))\n\n    # ROPE より改善している (&gt;500ms) 確率\n    prob_better = np.mean(ms_diff[:, i] &gt;= ROPE_RANGE[1])\n\n    # ROPE より悪化している (&lt;-500ms) 確率\n    prob_worse = np.mean(ms_diff[:, i] &lt;= ROPE_RANGE[0])\n\n    print(f\"Page: {page}\")\n    print(f\"    Mean Diff: {ms_diff[:, i].mean():.1f} ms\")\n    print(f\"    Prob Better (&gt;{ROPE_RANGE[1]}ms):  {prob_better * 100:.1f}%\")\n    print(f\"    Prob Worse: (&lt;{ROPE_RANGE[0]}ms): {prob_worse * 100:.1f}%\")\n    print(f\"    Prob Equiv  (In ROPE): {prob_in_rope * 100:.1f}%\")\n    print(\"-\" * 30)\n\n--- ROPE Analysis (Probability of Practical Effect)---\nPage: Top\n    Mean Diff: 245.6 ms\n    Prob Better (&gt;100ms):  68.9%\n    Prob Worse: (&lt;-100ms): 10.5%\n    Prob Equiv  (In ROPE): 20.5%\n------------------------------\nPage: Detail\n    Mean Diff: 299.9 ms\n    Prob Better (&gt;100ms):  75.8%\n    Prob Worse: (&lt;-100ms): 7.0%\n    Prob Equiv  (In ROPE): 17.2%\n------------------------------\nPage: Contract\n    Mean Diff: -218.6 ms\n    Prob Better (&gt;100ms):  14.9%\n    Prob Worse: (&lt;-100ms): 64.3%\n    Prob Equiv  (In ROPE): 20.8%\n------------------------------\nPage: Checkout\n    Mean Diff: -243.8 ms\n    Prob Better (&gt;100ms):  14.2%\n    Prob Worse: (&lt;-100ms): 65.8%\n    Prob Equiv  (In ROPE): 20.0%\n------------------------------\n\n\n\n# --- 可視化: 事後分布の HDI と ROPE の重なり具合 ---\nHDI_PROB = 0.94  # 94% HDI\n\n# --- 可視化 ---\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes_flat = axes.flatten()\n\nfor i, page in enumerate(pages):\n    ax = axes_flat[i]\n    data_page = ms_diff[:, i]  # 各ページの改善量サンプル (shape: 8000,)\n\n    # 1. 事後分布の Histogram / KDE\n    az.plot_kde(data_page, ax=ax, plot_kwargs={\"color\": \"mediumpurple\", \"linewidth\": 2}, fill_kwargs={\"alpha\": 0.4})\n\n    # 2. HDI (94%) を計算してプロット\n    hdi = az.hdi(data_page, hdi_prob=HDI_PROB)\n    ax.axvline(hdi[0], color=\"mediumpurple\", linestyle=\"--\", linewidth=1.5, label=f\"{HDI_PROB * 100:.0f}% HDI\")\n    ax.axvline(hdi[1], color=\"mediumpurple\", linestyle=\"--\", linewidth=1.5)\n    # HDI範囲の塗りつぶし（Option）\n    ax.axvspan(hdi[0], hdi[1], alpha=0.1, color=\"mediumpurple\")\n\n    # 3. ROPE を塗りつぶしてプロット\n    ax.axvspan(ROPE_RANGE[0], ROPE_RANGE[1], alpha=0.2, color=\"orange\",\n               label=f\"ROPE ({ROPE_RANGE[0]} to {ROPE_RANGE[1]} ms)\")\n\n    # 4. 参照戦（ゼロ）\n    ax.axvline(0, color=\"black\", linestyle=\":\", linewidth=1, label=\"No Effect (0)\")\n\n    # 5. タイトルと判定ラベル\n    mean_diff = np.mean(data_page)\n\n    # 判定ロジック\n    if hdi[0] &gt; ROPE_RANGE[1]:\n        verdict = \"明確な改善\"\n        verdict_color = \"seagreen\"\n    elif hdi[1] &lt; ROPE_RANGE[0]:\n        verdict = \"明確な悪化\"\n        verdict_color = \"indianred\"\n    elif hdi[0] &gt; ROPE_RANGE[0] and hdi[1] &lt; ROPE_RANGE[1]:\n        verdict = \"実質的に等価\"\n        verdict_color = \"slategray\"\n    else:\n        verdict = \"判断保留\"\n        verdict_color = \"goldenrod\"\n\n    ax.set_title(f\"{page} Page\\nMean: {mean_diff:.0f} ms | {verdict}\", fontsize=12, color=verdict_color,\n                 fontweight=\"bold\")\n    ax.set_xlabel(\"LCP Change (Pre - Post) [ms]\")\n    ax.set_ylabel(\"Density\")\n    ax.legend(loc=\"upper right\", fontsize=8)\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle(\"ROPE Analysis: HDI vs. ROPE Overlap\", fontsize=16, fontweight=\"bold\")\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_13667/1007172029.py:54: UserWarning: Glyph 21028 (\\N{CJK UNIFIED IDEOGRAPH-5224}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/tmp/ipykernel_13667/1007172029.py:54: UserWarning: Glyph 26029 (\\N{CJK UNIFIED IDEOGRAPH-65AD}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/tmp/ipykernel_13667/1007172029.py:54: UserWarning: Glyph 20445 (\\N{CJK UNIFIED IDEOGRAPH-4FDD}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/tmp/ipykernel_13667/1007172029.py:54: UserWarning: Glyph 30041 (\\N{CJK UNIFIED IDEOGRAPH-7559}) missing from font(s) DejaVu Sans.\n  plt.tight_layout()\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 21028 (\\N{CJK UNIFIED IDEOGRAPH-5224}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 26029 (\\N{CJK UNIFIED IDEOGRAPH-65AD}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 20445 (\\N{CJK UNIFIED IDEOGRAPH-4FDD}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n/home/peta/petaLab/bayesian-iroha/.venv/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 30041 (\\N{CJK UNIFIED IDEOGRAPH-7559}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)\n\n\n\n\n\n\n\n\n\n\nROPE 分析結果の解釈\n\nROPE の意味 ROPE（Region of Practical Equivalence）は、実際の業務において差異が顕著に現れる範囲を指します。 この範囲内で変化が認められるかどうかは、ビジネスのコンテキストや User の体験に大きく左右されます。\n\n\n分析結果の解釈\n今回の分析では、「±100ms以内の変化は、ユーザーにとって体感できない（実質的に同等）」という閾値を設定しました。 これは、人間の知覚限界（JND: Just Noticeable Difference）を考慮した、実務的な「ノイズフィルター」として機能します。 Googleの「RAILモデル」においても、100ms以内の応答はユーザーに「即座」に感じさせると定義されています。 そのため、100ms以下の変化は「実質的に同じ（Practical Equivalence）」とみなす考え方が合理的です。\n\n\n\n事後分布の HDI が ROPE とどの程度重なっているか可視化\n\n\n3つの判定結果が一目でわかる HDIとROPEの重なり方によって、以下の3つの結論を視覚的に即座に判断できます。\n\n\n\n\n\n\n\n\n\n\nHDIとROPEの関係\n判定\n図のイメージ\n\n\n\n\nHDI全体がROPEの外側（右）\n✅ 明確な改善\n[—ROPE—]…..[===HDI===]\n\n\nHDI全体がROPEの外側（左）\n❌ 明確な悪化\n[===HDI===]…..[—ROPE—]\n\n\nHDI全体がROPEの内側\n➖ 実質的に等価（変化なし）\n[–[=HDI=]–] (ROPE内にHDI)\n\n\nHDIがROPEと部分的に重なる\n⚠️ 判断保留（データ不足）\n[—RO[==HDI==]PE—]\n\n\n\n\n\n不確実性の大きさが伝わる\n\n\nHDIが狭い: 推定に自信がある。\nHDIが広い: データが少なく、推定がまだ曖昧。\n\nHDIの「幅」は、推定の不確実性（データの少なさ、ばらつき）を反映します。 これにより、「改善しているが、まだ確証はない」といったニュアンスを経営層に伝えることができます。\n\n\n\nROPE分析結果の解釈\n可視化の読み方（凡例）\n\n\n\n\n\n\n\n要素\n意味\n\n\n\n\n紫の曲線・網掛け\n改善量（Pre - Post）の事後分布と94% HDI\n\n\n黄色の網掛け\nROPE（±100ms）：この範囲内の変化は「体感できない」とみなす\n\n\n灰色の点線\nゼロライン（変化なし）\n\n\n\n\n各ページの判定結果\n\nTop ページ：✅ 明確な改善 観察結果:\n\n事後分布（紫）が、ROPE（黄色）の右側に完全に位置している\n94% HDI の下限が ROPE の上限（+100ms）を超えている\n平均改善量は約 +500ms 前後\n\n解釈: TopページのLCPは、施策後にユーザーが確実に体感できるレベルで高速化しました。 94%の確信度で「100ms以上の改善がある」と言えます。\n【直感】: 「速くなったかも？」ではなく、「間違いなく速くなった」と自信を持って言える状態です。\nDetail ページ：✅ 明確な改善（または改善傾向） 観察結果:\n\n事後分布（紫）の大部分が ROPE の右側に位置\n94% HDI がほぼ ROPE 外にある（わずかに重なる可能性あり）\n平均改善量は約 +200ms 前後\n\n解釈: Detailページも体感可能な改善が認められます。 Topほどの確信度ではないものの、80〜95%の確率で体感できる改善があります。\n【直感】: 「おそらく速くなった」と言える状態。追加データがあれば確信が強まります。\nContract ページ：➖ 実質的に等価（変化なし） 観察結果:\n\n事後分布（紫）が ROPE（黄色）の内部にほぼ収まっている\n94% HDI の両端が ROPE 内に含まれている\n平均改善量は約 ±0〜50ms 程度\n\n解釈: Contractページは、統計的にわずかな差があったとしても、ユーザーにとっては「変わっていない」と判断できます。 この施策による影響は実質ゼロです。\n【直感】: 「良くも悪くもなっていない」中立の状態。このページへの追加対応は不要です。\nCheckout ページ：❌ 明確な悪化（Blocker） 観察結果:\n\n事後分布（紫）が、ROPE（黄色）の左側に完全に位置している\n94% HDI の上限が ROPE の下限（-100ms）を下回っている\n平均悪化量は約 -1000ms（約1秒） 前後\n\n解釈: Checkoutページは、施策後にユーザーが確実に体感できるレベルで遅延しています。 94%の確信度で「100ms以上の悪化がある」と言え、実際には約1秒もの遅延が発生しています。\n【直感】: 「遅くなったかも？」ではなく、「間違いなく遅くなった」という、リリースをブロックすべき明確なシグナルです。\n\n\n\n結果サマリー\n\n\n\nページ\nHDIとROPEの関係\n判定\nアクション\n\n\n\n\nTop\nHDI全体がROPE右側\n✅ 明確な改善\nリリース推奨\n\n\nDetail\nHDIの大部分がROPE右側\n✅ 改善傾向\nリリース推奨\n\n\nContract\nHDI全体がROPE内\n➖ 等価（変化なし）\n対応不要\n\n\nCheckout\nHDI全体がROPE左側\n❌ 明確な悪化\nブロッカー\n\n\n\n\n\n意思決定への示唆\nこの可視化から導かれる結論は以下の通りです。 ROPE判定の総合結論: 今回の施策は、Top・Detailページにおいてユーザー体感レベルの明確な改善をもたらした一方、 Checkoutページにおいて約1秒の致命的な遅延を引き起こしている。Checkoutページの悪化は、94% HDI が ROPE を完全に下回っており、 統計的にも実務的にも「許容不可」 と判断される。\n\n\n\n\nROI-シミュレーション (経済的判断)\n\n# --- Business Parameters ---\nn_samples = ms_diff.shape[0]  # サンプル数を取得\n\n# 基本パラメータ\nPV_MONTHLY = np.array([1_000_000, 100_000, 10_000, 1_000])\nAOV = 5_000\nCOST_IMPLEMENTATION = 100_000\n\n# 感度(Sensitivity) を (N_sample, 4) の行列として生成\nmatrix_sensitivity = np.tile([0.0001, 0.0005, 0.0050, 0.0000], (n_samples, 1))  # 固定値で初期化\n\n# 【重要】Checkout (Index 3) の感度を「幅のある分布」として生成\n# \"0.04〜0.06の間でばらつく\" -&gt; 一様分布 (Uniform Distribution) を採用\n# ※ より確信がある場合は正規分布 rng.normal(loc=0.05, scale=0.005, size=n_samples) なども可\nsensitivity_checkout_dist = rng.uniform(low=0.04, high=0.06, size=n_samples)\nmatrix_sensitivity[:, 3] = sensitivity_checkout_dist\n\n# 1ms あたりの価値も分布になる（N_samples, 4）\nvalue_per_ms_dist = matrix_sensitivity * PV_MONTHLY * AOV\n\n\nビジネスパラメータの定義：固定値と不確実性の統合\nROIシミュレーションを行うために、ビジネス上の「固定パラメータ」と、リスクを考慮するための「確率的パラメータ（不確実性）」をそれぞれ定義します。\n\n1. 固定ビジネスパラメータ（Business Constants）\nこれらは、アクセス解析や財務データから明確に値が決まっているパラメータです。\n\nPV_MONTHLY（月間ページビュー）\n\n【定義】: 各ページタイプの1ヶ月あたりのアクセス数です。\n\nTop (1,000,000): 圧倒的にアクセスが多い場所です。\nCheckout (1,000): 全体の0.1%しか到達しない、貴重な購入直前のステップです。\n\n【直感】: 「影響が及ぶ人数」です。Topページは改善の幅が小さくても、人数が多いため塵も積もれば山となります。逆にCheckoutページは人数が少ないため、ここ単体の平均値だけを見ていても全体の数字は動きにくいという性質があります。\n\nAOV（平均客単価）\n\n【定義】: Average Order Valueの略で、1回の購入あたりの平均売上金額（ここでは5,000円）です。\n【直感】: 「1回の成功の重み」です。CVRが上がった結果、1件の成約が増えるごとにいくら売上が増えるのかを計算するために使用します。\n\nCOST_IMPLEMENTATION（実装コスト）\n\n【定義】: パフォーマンス改善施策（エンジニアの工数やツール費用など）にかかった一時的な費用です（ここでは10万円）。\n【直感】: 「この施策にかかった投資額」です。施策による売上増がこのコストを上回らなければ、ビジネスとしては「赤字」と判断されます。\n\n\n\n\n\n2. 確率的パラメータ（Probabilistic Parameters for Sensitivity Analysis）\nここでは「売上感度」を固定値ではなく確率分布として扱うことで、前提条件がズレていた場合のリスクを織り込みます。\n\nsensitivity_checkout_dist（Checkoutページの感度分布）\n\n【定義】: Checkoutページの売上感度を、固定値ではなく 一様分布 Uniform(0.04, 0.06) に従う確率変数として定義します。\n【直感】: 「感度はだいたい0.05%くらいだが、最悪0.06%（敏感）、良くても0.04%（鈍感）の範囲のどこかにある」という、分析者の “迷い” や “想定の幅” をそのままモデルに反映させています。\n\nmatrix_sensitivity（感度行列）\n\n【定義】: MCMCのサンプル数に対応した (N_samples, 4) の行列です。Topページなどは固定値ですが、Checkout列には上記の確率分布（ばらつき）が格納されています。\n【直感】: 「もしもの世界」のカタログです。「ある世界では客が怒りやすく（感度高）、別の世界では寛容（感度低）」といった何千通りものパラレルワールドを表現しています。\n\nvalue_per_ms_dist（確率的な1msの価値）\n\n【定義】: 感度の不確実性を反映した、1msあたりの金銭的価値の分布です。 \\[Value\\_per\\_ms \\sim PV \\times Sensitivity(\\text{分布}) \\times AOV\\]\n【直感】: これまでは「1ms遅れると250円の損失」と決め打ちしていましたが、ここでは「200円〜300円の間のどこか」というように、損失単価自体が揺れ動く幅を持っています。これにより、「感度が高く、かつ速度も遅くなった」という最悪のシナリオ（ワーストケース）を含めた評価が可能になります。\n\n\n\n# --- ROI Calculation with Sensitivity Analysis ---\n\n# ページごとの損益インパクト (円)\n# ms_diff (速度の不確実性) × value_per_ms_dist (ビジネス感度の不確実性)\nimpact_per_page_prob = ms_diff * value_per_ms_dist\n\n# トータルの純利益分布\ntotal_revenue_uplift = np.sum(impact_per_page_prob, axis=1)\nprofit_net_prob = total_revenue_uplift - COST_IMPLEMENTATION\n\n\n# --- Visualization ---\nplt.figure(figsize=(12, 6))\n\n# Plot 01: Total Net Profit (Sensitivity Included)\nplt.subplot(1, 2, 1)\nplt.hist(profit_net_prob, bins=50, alpha=0.7, density=True, label=\"Net Profit Dist\")\nplt.axvline(0, linestyle=\"--\", linewidth=2, color=COLOR_YELLOW, label=\"Break-even\")\nplt.title(\"Total ROI Distribution\\n(w/ Sensitivity Uncertainty)\")\nplt.xlabel(\"Net Profit (JPY)\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nROI分布の可視化：感度の不確実性を含めた純利益シミュレーション\nこのプロットは、今回の施策がもたらす純利益（Net Profit）の確率分布を示しています。\n\nプロットの構成要素\n\n\n\n\n\n\n\n要素\n意味\n\n\n\n\nヒストグラム（青/紫）\nMCMCサンプルから計算された純利益の分布。横軸が利益額（円）、縦軸が確率密度。\n\n\n黄色の破線（Break-even）\n損益分岐点（0円）。この線より右が黒字、左が赤字を意味する。\n\n\n\n\n\n読み取り方\n\n分布の中心（山の位置）: 期待される純利益の「最も起こりやすい値」を示します。\n分布の幅（山の広がり）: 利益予測の不確実性を表します。幅が広いほど「振れ幅が大きい」ことを意味します。\n0円ラインとの重なり: 分布のどの程度が0円より左（赤字領域）にあるかで、赤字リスクを視覚的に把握できます。\n\n\n\n解釈のポイント\n\n分布全体が0より右にある場合: 高い確率で黒字。施策は経済的に成功と判断できます。\n分布が0を跨いでいる場合: 黒字になる可能性も赤字になる可能性もある状態。リスクの定量化が必要です。\n分布全体が0より左にある場合: 高い確率で赤字。施策の見直しが必要です。\n\n\n\n技術的補足\n\n【定義】: 感度の不確実性 (Sensitivity Uncertainty)\n「1msの改善がCVRを何%向上させるか」という売上感度（Sensitivity）自体にも不確実性があります。このシミュレーションでは、感度パラメータにも確率分布を仮定し、LCP改善量の不確実性と同時に考慮しています。これにより、単一の点推定ではなく、より現実的なリスク評価が可能になります。\n\n\n【直感】\n「100ms速くなったら売上が1%上がる」と決め打ちするのではなく、「0.5%〜1.5%くらいの幅で上がるかもしれない」という幅を持たせて計算しています。これにより、「最悪のケースでも黒字か？」「最良のケースでどこまで伸びるか？」といった問いに答えられます。\n\n\n# Plot 2: Checkout Page Impact Only\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 2)\nidx_checkout = list(pages).index(\"Checkout\")\nplt.hist(impact_per_page_prob[:, idx_checkout], bins=50, color=COLOR_RED, alpha=0.7, density=True)\nplt.axvline(0, color=\"black\", linestyle=\"--\", linewidth=2)\nplt.title(\"Checkout Page Imapct Risk\\n(Sensitivity: 0.04~0.06)\")\nplt.xlabel(\"Revenue Change (JPY)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCheckout ページ売上インパクトリスク分布の可視化\nこのプロットは、Checkout ページ単体が施策によってどれだけの売上影響を受けるかを確率分布として可視化したものです。\n\nプロットの構成要素\n\n\n\n\n\n\n\n要素\n意味\n\n\n\n\nヒストグラム（赤色）\nMCMCサンプルから計算されたCheckoutページの売上変動分布。横軸が収益変動額（円）、縦軸が確率密度。\n\n\n黒い破線（0円ライン）\n損益分岐点。この線より右が収益増、左が収益減（損失）を意味する。\n\n\nタイトルの「Sensitivity: 0.04~0.06」\nCheckoutページの売上感度を固定値ではなく、0.04〜0.06の一様分布として不確実性を織り込んでいることを示す。\n\n\n\n\n\n読み取り方\n\n分布の中心（山の位置）: Checkoutページにおける期待される売上変動額を示します。\n分布の幅（山の広がり）: LCP変動の不確実性と売上感度の不確実性を同時に反映しており、予測の振れ幅を表します。\n0円ラインとの位置関係: 分布全体が0より左（マイナス領域）にあるほど、このページが収益に対してリスク要因であることを示します。\n\n\n\n解釈のポイント\n\n分布が0より完全に左にある場合: Checkoutページは明確な損失源であり、施策全体の黒字を帳消しにする可能性があります。\n分布が0を跨いでいる場合: 損失になる確率と利益になる確率が混在しており、追加検証が必要です。\nタイトルに「Imapct Risk」とある理由: 売上直結のCheckoutページでの遅延は、PVあたりの損失単価が極めて高いため、全体のROIを左右するリスクファクターとして特別に可視化しています。\n\n\n\n技術的補足\n\n【定義】: 感度の確率的モデリング\n通常のROI計算では「1msあたりの売上影響 = 固定値」として計算しますが、このシミュレーションでは Checkout ページの感度を Uniform(0.04, 0.06) の一様分布として定義しています。これにより、「感度が予想より高かった場合」と「低かった場合」の両方のシナリオを同時に評価しています。\n\n\n【直感】\n「1秒遅れると5%離脱」という前提が、実は「4%〜6%の幅のどこかにある」という不確かさを認め、最悪ケースでどれくらいの損失が出るかを可視化したものです。これにより、単なる期待値ではなくリスクの下限（ワーストケース）を意思決定に組み込むことができます。\n\n\n\nビジネス上の示唆\nこのプロットから得られる重要な洞察は、「全体のROIがプラスでも、Checkoutページ単体で見ると大きな損失が発生している可能性がある」という点です。特にECサイトにおいて、決済直前のパフォーマンス劣化は「カゴ落ち」を招き、LTV（顧客生涯価値）の低下やブランド毀損につながる取り返しのつかない損失となるリスクがあります。\n\n# --- 4. 意思決定指標 (Decision Metric) ---\nmean_profit = np.mean(profit_net_prob)\nrate_win = np.mean(profit_net_prob &gt; 0)\n\n# 感度が下振れ(0.04)した場合も含めた、Checkout ページの損失リスク\nrisk_checkout_loss = np.mean(impact_per_page_prob[:, idx_checkout] &lt; -10_000)\n\nprint(\"=== Sensitivity Analysis Result ===\")\nprint(\"Checkout Sensitivity: Uniform(0.04, 0.06)\")\nprint(f\"Expected Net Profit: {mean_profit:,.0f} JPY\")\nprint(f\"Win Rate: (Profit &gt; 0): {rate_win * 100:.1f}%\")\nprint(f\"Checkout Risk (Loss &gt; 10k): {risk_checkout_loss * 100:.1f}%\")\n\n=== Sensitivity Analysis Result ===\nCheckout Sensitivity: Uniform(0.04, 0.06)\nExpected Net Profit: 82,099,757 JPY\nWin Rate: (Profit &gt; 0): 65.1%\nCheckout Risk (Loss &gt; 10k): 77.4%\n\n\n\n\n\n結論と推奨アクション\n\nROI シミュレーション結果サマリー\n\n\n\n\n\n\n\n\n指標\n値\n解釈\n\n\n\n\n期待純利益 (Expected Net Profit)\n約 8,210 万円\n平均的には大幅な黒字が見込める\n\n\n勝率 (Win Rate: Profit &gt; 0)\n65.1%\n約3回に1回は赤字になるリスクあり\n\n\nCheckout 損失リスク (Loss &gt; 1万円)\n77.4%\n⚠️ 極めて高いリスク\n\n\n\n\n\n\n\n分析結果の解釈\n今回の感度分析（Sensitivity: 0.04〜0.06）を含めた ROI シミュレーションにより、以下の重要な事実が判明しました。\n\n✅ 良い点：全体 ROI の期待値はプラス\n\n期待純利益は約 8,200 万円 と、実装コスト（10万円）を大幅に上回る。\nTop ページと Detail ページの改善効果が、金額ベースで大きく貢献している。\n\n\n\n❌ 問題点：Checkout ページが致命的なリスク要因\n\n勝率 65.1% は、一見すると「勝ち越し」に見えるが、約35%の確率で赤字になる。\nCheckout ページ単体で 1万円以上の損失が発生する確率が 77.4% と極めて高い。\nこれは、Checkout ページの約1秒の遅延が、高感度（4〜6%/100ms）と掛け合わさることで、他ページの改善効果を打ち消しうることを意味する。\n\n\n\n\n\n意思決定のロジック\n\n「全体で黒字なら良い」という判断は危険である。\n\nCheckout ページの体験悪化は、今回のモデルに含まれていない以下の 長期的・間接的な損失 を招く可能性がある：\n\nカゴ落ち率の上昇: 決済直前の離脱は、最も機会損失が大きい。\nブランド毀損: 「このサイトは決済が遅い」という口コミ・記憶が残る。\nLTV（顧客生涯価値）の低下: 一度悪い体験をしたユーザーは戻ってこない。\n\n\n\n\n推奨アクション\n\n判定: 条件付きリリース承認（Blocker あり）\n「全体の ROI 期待値は約 8,200 万円のプラスですが、Checkout ページのパフォーマンス劣化が 許容不可（Blocker） です。\n承認条件: 1. Top ページ・Detail ページの改善コードのみをマージ 2. Checkout ページに影響する変更はロールバック（または修正） 3. リリース後、Checkout ページの LCP を 1週間モニタリング し、悪化が見られた場合は即時ロールバック\n部分リリースにより、改善効果の大部分を享受しつつ、致命的なリスクを回避します。」\n\n\n\n\n技術的補足\n\n【定義】: 条件付きリリース (Conditional Release)\n施策の一部のみを本番環境に適用し、リスクの高い部分は除外または修正後に再評価する手法。A/B テストと組み合わせることで、さらにリスクを低減できる。\n\n\n【直感】\n「料理で例えると、全体としては美味しいコース料理でも、1品だけ食中毒のリスクがある食材が入っていたら、その1品だけ外してお客様に出す」という判断です。\n\n\n\n\n次のステップ (Next Step)\n\n\n\n優先度\nアクション\n担当\n\n\n\n\n🔴 高\nCheckout ページの遅延原因を特定・修正\nエンジニア\n\n\n🟡 中\n修正後、N=20 以上のデータで再分析\nデータサイエンティスト\n\n\n🟢 低\n売上感度パラメータの精緻化（実データ検証）\nマーケティング",
    "crumbs": [
      "Web Performance ROI Analysis",
      "エグゼクティブサマリー"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ベイジアンいろは",
    "section": "",
    "text": "小規模データ×階層ベイズでビジネスの勝率を導く「意思決定のいろは」。PyMC v5を用い、単なる有意差検定を超えたROPE/ROI分析によるリスク可視化フレームワークを実装・蓄積するナレッジベース。\n\n\nここには、本プロジェクトで実施した分析レポートを掲載しています。\n\n\n\nプロジェクト概要（README）\n分析実証ログ（Notebook）\n\n\n\n\n\nプロジェクト概要（README）\n分析実証ログ（Notebook）\nヘルパーメソッド定義（Python）"
  },
  {
    "objectID": "index.html#概要",
    "href": "index.html#概要",
    "title": "ベイジアンいろは",
    "section": "",
    "text": "小規模データ×階層ベイズでビジネスの勝率を導く「意思決定のいろは」。PyMC v5を用い、単なる有意差検定を超えたROPE/ROI分析によるリスク可視化フレームワークを実装・蓄積するナレッジベース。\n\n\nここには、本プロジェクトで実施した分析レポートを掲載しています。\n\n\n\nプロジェクト概要（README）\n分析実証ログ（Notebook）\n\n\n\n\n\nプロジェクト概要（README）\n分析実証ログ（Notebook）\nヘルパーメソッド定義（Python）"
  },
  {
    "objectID": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html",
    "href": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html",
    "title": "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト",
    "section": "",
    "text": "本ノートブックでは、MoneyPuck.com から取得した 2024 年シーズンの実ショットデータを用い、シュートの「場所の難易度」を考慮した上でゴテンダーの真の実力（GSAx）を推定します。 特に、データが限られた「小規模データ（500件）」環境において、ベイズ階層モデルがいかにして安定した意思決定を支援するかを、3層の戦略的フィルタリングを通じて実証します。\n# 必要なライブラリ\nimport os\nimport requests\nimport zipfile\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom cycler import cycler\nfrom sklearn.preprocessing import SplineTransformer\n\n# 不要な出力の制御\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n# プロジェクト共通のカラー定義（実際の運用では /src にモジュール化を推奨）\nCOLOR_PURPLE = \"#9B5DE5\"  # 事後分布・HDI\nCOLOR_YELLOW = \"#F9C74F\"  # ROPE領域\nCOLOR_GREEN = \"#06D6A0\"  # 改善判定\nCOLOR_RED = \"#EF476F\"  # 悪化判定\nCOLOR_GRAY = \"#8D99AE\"  # 等価判定・参照線\n\nplt.rcdefaults()\npalette_brand = [COLOR_PURPLE, COLOR_YELLOW, COLOR_GREEN, COLOR_RED, COLOR_GRAY]\nsns.set_theme(style=\"whitegrid\", palette=palette_brand)\nplt.rcParams[\"axes.prop_cycle\"] = cycler(color=palette_brand)\n\nprint(\"Brand Style Applied: The visual identity was applied.\")\n\nBrand Style Applied: The visual identity was applied.\n分析に必要な統計・可視化ライブラリをロードし、レポートの品質を担保するためのデザイン設定（Seaborn / Japanize-Matplotlib）を行います。",
    "crumbs": [
      "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト"
    ]
  },
  {
    "objectID": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#データ準備preparation",
    "href": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#データ準備preparation",
    "title": "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト",
    "section": "1. データ準備（Preparation）",
    "text": "1. データ準備（Preparation）\n外部ソース（Moneypuck.com）から ZIP形式の巨大なデータセットを自動取得し、解析可能な状態に展開します。\n\n# データのダウンロード（MoneyPuck 2024 shots Data）\nURL_DATA = \"https://peter-tanner.com/moneypuck/downloads/shots_2024.zip\"\nDIR_EXTRACT = \"data_raw\"\nNAME_ZIP = \"shots_2024.zip\"\nNAME_CSV = \"shots_2024.csv\"\nPATH_CSV = os.path.join(DIR_EXTRACT, NAME_CSV)\n\nif os.path.exists(PATH_CSV):\n    # すでに CSV が存在する場合はスキップ\n    print(f\"Data Preparation: Local CSV found at {PATH_CSV}. Skipping download.\")\n\nelse:\n    # ファイルがない場合: ダウンロードと展開を実行する\n    print(\"Data Preparation: CSV not found. Starting automated setup...\")\n\n    # 保存先ディレクトリの作成\n    if not os.path.exists(DIR_EXTRACT):\n        os.makedirs(DIR_EXTRACT)\n        print(f\"Data Preparation: Created directory '{DIR_EXTRACT}\")\n\n    # ダウンロード\n    print(f\"Data Preparation: Downloading from {URL_DATA}...\")\n    try:\n        response = requests.get(URL_DATA)\n        with open(NAME_ZIP, \"wb\") as f:\n            f.write(response.content)\n\n        # ZIP の展開\n        print(f\"Data Preparation: Extracting {NAME_ZIP}...\")\n        with zipfile.ZipFile(NAME_ZIP, \"r\") as ref_zip:\n            ref_zip.extractall(DIR_EXTRACT)\n\n        # 元の ZIP ファイルの削除（クリーンアップ）\n        if os.path.exists(NAME_ZIP):\n            os.remove(NAME_ZIP)\n            print(f\"Data Preparation: Cleand up temporary ZIP file '{NAME_ZIP}'\")\n\n        # 展開ファイルの確認\n        files = os.listdir(DIR_EXTRACT)\n        print(f\"Data Preparation: Extracted files: {files}\")\n    except Exception as e:\n        print(f\"Data Preparation Error: {e}\")\n\nData Preparation: CSV not found. Starting automated setup...\nData Preparation: Downloading from https://peter-tanner.com/moneypuck/downloads/shots_2024.zip...\nData Preparation: Extracting shots_2024.zip...\nData Preparation: Cleand up temporary ZIP file 'shots_2024.zip'\nData Preparation: Extracted files: ['shots_2024.csv', 'goalie_final_investment_report.csv', 'data_goalie_rope_analysis.csv', 'final_roi_report_chart.png']\n\n\n\n# データのプレビュー（ホワイトボックス化）\nCSV_TARGET = os.path.join(DIR_EXTRACT, \"shots_2024.csv\")\n\n# 先頭５行を確認\ndf_preview = pd.read_csv(CSV_TARGET, nrows=5)\nprint(\"Data Preparation: Preview of the data:\")\ndf_preview.head()\n\nData Preparation: Preview of the data:\n\n\n\n\n\n\n\n\n\nshotID\narenaAdjustedShotDistance\narenaAdjustedXCord\narenaAdjustedXCordABS\narenaAdjustedYCord\narenaAdjustedYCordAbs\naverageRestDifference\nawayEmptyNet\nawayPenalty1Length\nawayPenalty1TimeLeft\n...\nxCordAdjusted\nxFroze\nxGoal\nxPlayContinuedInZone\nxPlayContinuedOutsideZone\nxPlayStopped\nxRebound\nxShotWasOnGoal\nyCord\nyCordAdjusted\n\n\n\n\n0\n0\n52.0\n57.0\n57.0\n-41.0\n41.0\n0.0\n0\n0\n0\n...\n57\n0.238455\n0.012537\n0.394229\n0.301072\n0.022807\n0.030900\n0.710867\n-40\n-40\n\n\n1\n1\n33.0\n71.0\n71.0\n-28.0\n28.0\n-6.0\n0\n0\n0\n...\n71\n0.198306\n0.021962\n0.404919\n0.313773\n0.023774\n0.037266\n0.759039\n-28\n-28\n\n\n2\n2\n48.0\n48.0\n48.0\n-24.0\n24.0\n-12.6\n0\n0\n0\n...\n48\n0.213829\n0.028057\n0.405311\n0.294682\n0.025849\n0.032272\n0.696901\n-24\n-24\n\n\n3\n3\n58.0\n-40.0\n40.0\n-31.0\n31.0\n0.0\n0\n0\n0\n...\n41\n0.209478\n0.009832\n0.449775\n0.277671\n0.019667\n0.033577\n0.610530\n-31\n31\n\n\n4\n4\n56.0\n-35.0\n35.0\n15.0\n15.0\n0.0\n0\n0\n0\n...\n36\n0.376712\n0.028884\n0.307725\n0.205568\n0.022266\n0.058845\n0.799576\n15\n-15\n\n\n\n\n5 rows × 137 columns\n\n\n\n膨大なカラムを持つデータから、空間的な「期待ゴール率」と「ゴテンダー評価」に直結する最小限の変数のみを抽出してロードします。\n\n# 利用する列を展開してロード（メモリ効率化）\nCOLS_USE = [\n    \"shotID\",\n    \"goal\",  # 1: Goal, 0: No Goal\n    \"xCordAdjusted\",  # 調整済み x 座標\n    \"yCordAdjusted\",  # 調整済み y 座標\n    \"goalieIdForShot\",  # ゴテンダー ID\n    \"goalieNameForShot\"  # 選手名\n]\n\nprint(\"Data Preparation: Loading real dataset from CSV...\")\ndf_raw = pd.read_csv(CSV_TARGET, usecols=COLS_USE)\n\nprint(f\"Data Preparation: Data load finished. {len(df_raw):,} rows\")\ndf_raw.head()\n\nData Preparation: Loading real dataset from CSV...\nData Preparation: Data load finished. 119,870 rows\n\n\n\n\n\n\n\n\n\nshotID\ngoal\ngoalieIdForShot\ngoalieNameForShot\nxCordAdjusted\nyCordAdjusted\n\n\n\n\n0\n0\n0\n8480045\nUkko-Pekka Luukkonen\n57\n-40\n\n\n1\n1\n0\n8480045\nUkko-Pekka Luukkonen\n71\n-28\n\n\n2\n2\n0\n8480045\nUkko-Pekka Luukkonen\n48\n-24\n\n\n3\n3\n0\n8474593\nJacob Markstrom\n41\n31\n\n\n4\n4\n0\n8474593\nJacob Markstrom\n36\n-15\n\n\n\n\n\n\n\n投影（Projection / usecols） - 巨大なデータセットから、特定の属性（列）のみの選択をしてメモリに読み込むようにします。 データ全てを覚えるのではなく、必要なカラムだけを抽出することで作業効率を劇的に上げるテクニックです。",
    "crumbs": [
      "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト"
    ]
  },
  {
    "objectID": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#データ加工processing",
    "href": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#データ加工processing",
    "title": "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト",
    "section": "2. データ加工（Processing）",
    "text": "2. データ加工（Processing）\nキーバーがいない状況でのゴールなど、能力評価においてノイズとなるデータを除外します。\n\n# クレンジング:\ndf_clean = df_raw.dropna(subset=[\"goalieIdForShot\"]).copy()  # ゴテンダー不在（Empty Net）データの厳密な除外\ndf_clean = df_clean[df_clean[\"goalieIdForShot\"] &gt; 0].copy()  # MoneyPuck データでは無人ゴール時に ID が 0 や欠損になるため、これを除外します。\n\nprint(f\"Data Processing: {len(df_clean):,} rows left after cleaning.\")\n\nData Processing: 118,900 rows left after cleaning.\n\n\nデータが少ない環境（新規事業や特定セグメントの分析）を再現し、不可実性下での判断プロセスを構築します。\n\n# 小規模データ化（３桁台のサンプリング）\n# 意思決定の難易度が高い「事例が少ない」状況を再現するため、あえて 500 件に絞ります\nSIZE_SAMPLE = 500\ndf_small = df_clean.sample(n=SIZE_SAMPLE, random_state=42).reset_index(drop=True)\n\ndf_small\n\n\n\n\n\n\n\n\nshotID\ngoal\ngoalieIdForShot\ngoalieNameForShot\nxCordAdjusted\nyCordAdjusted\n\n\n\n\n0\n77213\n0\n8480947\nKevin Lankinen\n57\n-17\n\n\n1\n62564\n1\n8482487\nJakub Dobes\n42\n12\n\n\n2\n50703\n0\n8480382\nAlexandar Georgiev\n76\n10\n\n\n3\n80853\n1\n8479406\nFilip Gustavsson\n72\n14\n\n\n4\n65495\n0\n8481020\nJustus Annunen\n23\n-39\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n495\n43089\n0\n8475683\nSergei Bobrovsky\n56\n-12\n\n\n496\n50168\n0\n8481611\nPyotr Kochetkov\n66\n35\n\n\n497\n15962\n0\n8480280\nJeremy Swayman\n62\n16\n\n\n498\n68658\n0\n8475683\nSergei Bobrovsky\n37\n11\n\n\n499\n41174\n0\n8481692\nDustin Wolf\n37\n-13\n\n\n\n\n500 rows × 6 columns\n\n\n\nカテゴリ変数（ID）を確率モデルが扱える形式に変換し、結果の解釈をしやすくするための紐付けを行います。\n\n# インデックス化\n# 選手ID を 0 からの連番に変換し、ID から名前を引く辞書を作成します\ncodes_goalie, uniques_goalie = pd.factorize(df_small[\"goalieIdForShot\"])\ndf_small[\"idx_goalie\"] = codes_goalie\ndf_small[\"obs_shots\"] = df_small[\"goal\"].astype(int)\n\n# ID から名前を引くためのマップ\nid_to_name = df_small.set_index(\"idx_goalie\")[\"goalieNameForShot\"].to_dict()\n\nprint(f\"Data Processing: unique count of Goalie {len(uniques_goalie):,}\")\ndf_small[[\"goalieIdForShot\", \"idx_goalie\", \"obs_shots\"]]\n\nData Processing: unique count of Goalie 79\n\n\n\n\n\n\n\n\n\ngoalieIdForShot\nidx_goalie\nobs_shots\n\n\n\n\n0\n8480947\n0\n0\n\n\n1\n8482487\n1\n1\n\n\n2\n8480382\n2\n0\n\n\n3\n8479406\n3\n1\n\n\n4\n8481020\n4\n0\n\n\n...\n...\n...\n...\n\n\n495\n8475683\n25\n0\n\n\n496\n8481611\n30\n0\n\n\n497\n8480280\n18\n0\n\n\n498\n8475683\n25\n0\n\n\n499\n8481692\n29\n0\n\n\n\n\n500 rows × 3 columns\n\n\n\n座標（\\(x\\), \\(y\\)）とう数値を、モデルが「どのエリアが危険か」を滑らかに学習できる高度な特徴量に変換します。\n\n# 特徴量生成（空間基底関数の適用）\n# 座標データから滑らかな空間曲面（スプライン基底）を生成します\nspline = SplineTransformer(n_knots=5, degree=3, include_bias=False)\nbasis_spatial = spline.fit_transform(df_small[[\"xCordAdjusted\", \"yCordAdjusted\"]])\n\nprint(f\"Data Processing: Feature matrix calculation {basis_spatial.shape}\")\n\nData Processing: Feature matrix calculation (500, 12)\n\n\nこのセルでは、「シュート位置（x, y）」という単純な 2 つの数値を、この SplineTransformer に通すことで、 「ゴール付近ならこの値が大きくなる」「端の方ならこの値が変化する」といった、 空間的な特徴を持つ複数の変数（多次元の行列） へと変換しています。\nこれにより、モデルは「座標 (x, y)」をただの数字としてではなく、 「リンク上のどのエリアがゴールになりやすいか」という滑らかな非線形の関係として学習できるようになります。\n\nスプライン基底関数\n一言でいうと、「複雑な1つの曲面を、いくつかの『シンプルな山の形（部品）』の組み合わせで表現する仕組み」 のことです。\n\n1. 「基底（きてい）」とは「部品」のこと\n例えば、絵の具で「オレンジ色」を作るとき、「赤」と「黄色」を混ぜます。このとき、赤と黄色が「基底（部品）」です。 データ分析における基底関数も同じです。 「座標 (x, y)」という生データをそのまま使うのではなく、「特定のエリアに反応するいくつかの部品（関数）」に変換します。\n\n\n2. スプライン基底関数の仕組み（イメージ）\nアイスホッケーのリンクを想像してください。 1. エリアの分割: リンクの上に、いくつかの「小さな山（テントのような形）」を等間隔に並べます。 2. 反応する場所: - ゴール正面にある「山A」は、シュートが正面から打たれたときだけ高い値を出します。 - サイドにある「山B」は、サイドから打たれたときだけ反応します。\n合成: これらの「山（部品）」をそれぞれ「どれくらい重視するか（重み）」を掛け合わせて足し算すると、リンク全体の「シュートの危険度マップ」が完成します。 この「一つ一つの山」が「スプライン基底関数」です。\n\n\n3. なぜ「スプライン」なのか？\n単にエリアを四角く区切る（＝モザイク状にする）だけだと、エリアの境界線で値が突然跳ね上がってしまい、不自然です。\n「スプライン」 という手法を使うと、隣り合う「山」同士が滑らかに重なり合うように設計されます。これにより\n\n「ここから急にゴールしやすくなる」という不自然な段差がなくなる。\n「ゴールに近づくにつれて、なだらかに危険度が高まる」 という自然な表現が可能になる。\n\n\n\n4. SplineTransformer がやっていること\nコードにある以下の処理は、まさにこの「部品への変換」を行っています。\n# 座標 (x, y) を、複数の「山の反応度」に変換する\nbasis_spatial = spline.fit_transform(df_small[[\"xCordAdjusted\", \"yCordAdjusted\"]])\n\n入力: [x座標, y座標] （2つの数字）\n出力: [山Aの反応, 山Bの反応, 山Cの反応, …] （多くの数字の列）\n\nの変換のおかげで、あとの統計モデル（PyMC）は「座標から直接計算する」という難しいことをしなくて済み、 「どの山の重みを大きくすれば、実際のゴール率と一致するか？」 を計算するだけで済むようになります。\n\n\nまとめ\nスプライン基底関数とは、 複雑な空間の形を捉えるために、「滑らかに重なり合った、場所ごとの反応パーツ」に分解して表現する手法のことです。\nSplineTransformar(n_knots=5, degree=3, include_bias=False)\nsciket-learn で提供されている、数値データ。今回は、座標から「スプライン基底関数」と呼ばれる特徴量を生成。\nArgs: &gt; n_konots=5 (ノットの数) &gt; ノット（データを分割する「結び目（ノット」））の数。 &gt; スプライン曲線は、データをいくつかの区間に分けて、それぞれの区間で多項式を当てはめます。 &gt; この値を大きくするほど、より細かく複雑な形状（曲面）を表現できるようになりますが、 &gt; 大きくしすぎると過学習（ノイズに反応しすぎること）のリスクが高まります。\n\ndegree=3 (多項式の次数) 各区間で使用する多項式の次数。 - degree=1: 線形（カクカクした折れ線） - degree=2: 2次関数（放物線） - degree=3: 3次関数（立方スプライン）\nこのコードでは: 一般的に最もよく使われる 3（3次スプライン） が指定されています。 これにより、結び目の境界でも変化が非常に滑らか（数学的に2回微分可能）な曲面を作ることができます。\n\n\ninclude_b=False (バイアス項を含めるか) 全ての基底の和が 1 になるような定数項（インターセプト）を特徴量に含めるかどうか。 True にすると、生成される行列に「常に 1 となる列」のような定数成分が含まれます。 このコードでは: False に設定されています。 これは、後続の統計モデル（PyMCなど）側で別途切片（Intercept）を用意する場合や、 冗長な変数を避けるためによく取られる設定です。",
    "crumbs": [
      "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト"
    ]
  },
  {
    "objectID": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#確率モデル構築modeling",
    "href": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#確率モデル構築modeling",
    "title": "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト",
    "section": "3. 確率モデル構築（Modeling）",
    "text": "3. 確率モデル構築（Modeling）\n「場所の難易度」と「キーパーの実力」を統計的に分離して推定します。\n\n# PyMC用の Coords(座標)定義\ncoords = {\n    \"goalie\": [id_to_name[i] for i in range(len(uniques_goalie))],\n    \"shot\": np.arange(len(df_small)),\n    \"basis\": np.arange(basis_spatial.shape[1])\n}\n\n確率変数の各次元に対してラベルを付与します。 後で可視化や結果を確認する際に人間が理解しやすいようにします。\n\nwith pm.Model(coords=coords) as hockey_spatial_model:\n    # --- データ登録(pm.Data: モデル内で参照する固定データ) ---\n    x_basis = pm.Data(\"x_basis\", basis_spatial, dims=(\"shot\", \"basis\"))  # 空間特徴量（スプライン基底行列）\n    idx_goalie = pm.Data(\"idx_goalie\", df_small[\"idx_goalie\"], dims=\"shot\")  # 各シュートの担当GKインデックス\n    obs_shot = pm.Data(\"obs_shots\", df_small[\"obs_shots\"], dims=\"shot\")  # 正解ラベル（ゴールしたか否か）\n\n    # --- 空間効果 (Spatial Component: 場所によるはいりやすさの共通トレンド) ---\n    coeffs_spatial = pm.Normal(\"coeffs_spatial\", mu=0, sigma=1, dims=\"basis\")  # 各場所（基底）に対する重み係数\n    trend_spatial = pm.Deterministic(\"trend_spatial\", pm.math.dot(x_basis, coeffs_spatial),\n                                     dims=\"shot\")  # 場所による「決まりやすさ」のトレンド（線形結合）\n\n    # 階層構造 (Hierarchical Component: 人の実力)\n    mu_league = pm.Normal(\"mu_league\", mu=0, sigma=1.5)  # リーグ全体の平均防御力\n    sigma_goalie = pm.HalfNormal(\"sigma_goalie\", sigma=0.5)  # リーグ全体の実力格差（ばらつき）\n    z_goalie_offset = pm.Normal(\"z_goalie_offset\", mu=0, sigma=1, dims=\"goalie\")  # 各GKの偏差値（標準化された個体差\n\n    # --- ゴテンダー個別の実力値 (Non-centered Parameterization) ---\n    effect_goalie = pm.Deterministic(\"effect_goalie\", mu_league + z_goalie_offset * sigma_goalie, dims=\"goalie\")\n\n    # --- 結合と尤度 (Likelihood: 答え合わせ) ---\n    p_logit = trend_spatial + effect_goalie[idx_goalie]  # 対数オッズ（場所の効果 + 人の効果）\n    xG = pm.Deterministic(\"xG\", pm.math.invlogit(p_logit), dims=\"shot\")  # 確率（0~1）への変換 = 期待ゴール率\n\n    # ベルヌーイ分布で観測データと比較し、パラメータを推論\n    pm.Bernoulli(\"obs\", logit_p=p_logit, observed=obs_shot, dims=\"shot\")\n\n\n# 確率モデル構造の可視化\npm.model_to_graphviz(hockey_spatial_model)\n\n\n\n\n\n\n\n\n\n確率モデル構造の解読（Graphviz の見方）\nモデルは大きく「左・右・下」の3つのブロックで構成されています。\n\n1. 左側：空間効果（Spatial Component）\nシュートが放たれた「場所」の有利・不利を計算するルートです。\n\nx_basis (Data) \\(\\rightarrow\\) trend_spatial (Deterministic)\n\nx_basis（12次元のスプライン基底）に、推定された重み coeffs_spatial を掛け合わせることで、リンク上の各地点の「決まりやすさの基準値」を算出します。\n\n役割:\n\n「ゴール正面は決まりやすく、端は決まりにくい」という物理的なコンテキストを司ります。これにより、キーパーの実力評価から「場所の有利不利」というノイズを排除できます。\n\n\n\n\n2. 右側：ゴテンダーの実力（Goalie Hierarchy）\n各キーパーの阻止能力を推定するルートです。ここには NCP（非中心化パラメータ化） という高度な階層構造が組み込まれています。\n\n最上流 (mu_league, sigma_goalie):\n\nリーグ全体の「平均的な防御力」と「実力差のバラつき」です。\n\n中流 (z_goalie_offset):\n\n各キーパーの「平均からのズレ」を標準化した数値（偏差値のようなもの）です。\n\n下流 (effect_goalie):\n\n上記を統合し、\\(\\mu + z \\cdot \\sigma\\) の計算によって各キーパーの真の実力を復元します。\n\n役割:\n\nデータが少ないキーパーの評価をリーグ平均に引き寄せ（収縮推定）、「たまたま数回止めただけ」の選手を過大評価するリスクを統計的に防ぎます。\n\n\n\n\n③ 下部：観測データとの結合（Likelihood）\n予測と現実を突き合わせ、モデルを学習させる心臓部です。\n\np_logit \\(\\rightarrow\\) xG \\(\\rightarrow\\) obs (Observed)\n\ntrend_spatial（場所の影響）と effect_goalie（人の能力）の和が p_logit（対数オッズ）となり、それを確率に直したものが xG（期待ゴール率）です。\n最後に、実際のゴール成否 obs と比較されます。\n\n役割:\n\n現実のデータ（obs）との誤差が最小になるように、矢印を逆流する形で全てのパラメータ（白ノード）の値が微調整（サンプリング）されます。\n\n\n\nTips: 確率モデル構造図（DAG）の読み解きガイド\nグラフ内の各要素は、統計学的な役割を「色」と「形」で表現しています。\n1. ノード（枠）の「色」が表すもの：データの確定度\n\nグレー（塗りつぶし）: 観測データ / 固定値\n\n外界から与えられた、モデルが変更することのない「事実（pm.Data や observed）」です。すべての推論はこの数値を土台に開始されます。\n\n白（背景なし）: 未知の変数 / 推定対象\n\nモデルがデータから正体を突き止めようとしている数値です。ここが「白」であることは、その値が不確実であり、推論が必要であることを意味します。\n\n\n2. ノード（枠）の「形」が表すもの：変数の性質\n\n楕円（白い丸枠）: 確率変数（Stochastic Variables）\n\n定義: Normal や HalfNormal といった統計分布を持つ変数です。\n役割: サンプリング（MCMC）によって、その値の分布（もっともらしい範囲）が推定される「モデルの核心」となる変数です。\n\n四角形（白い四角枠 / 二重線）: 決定論的変数（Deterministic Variables）\n\n定義: 他のノードからの計算（足し算や行列掛け算など）によって一意に決まる変数です。\n役割: それ自体がランダムに変動するわけではなく、計算結果に「GSAx」や「空間トレンド」といった名前（ラベル）を付けて解釈しやすくするために存在します。\n\n\n3. プレート（外側の大きな四角枠）\n\n意味: 反復処理の範囲（Plate）\n\n枠の右下にある数字は、その中の計算が何回繰り返されるかを示します。\nshot プレート (500): 全500件のシュート1件ずつに対して、個別に期待値を計算している領域。\ngoalie プレート (N): キーパーの人数分だけ、個別の実力パラメータを用意している領域。\n\n\n4. 矢印（有向エッジ）\n\n意味: 依存関係 / 因果の流れ\n\n「A の値が B の計算に使われる」という情報の流れを示します。矢印を遡ることで、ある結果が「場所の影響」なのか「個人の実力」なのか、どの因拠に基づいているかを特定できます。\n\n\n\n\n\n\nこの構造がビジネスにもたらす価値\nこのグラフをステークホルダーに見せることは、単なる技術説明以上の意味を持ちます。\n\n「公平な評価」の可視化:\n\n場所の影響（左）と個人の能力（右）が別々のルートで計算され、最後に合流していることを示せます。これは、「厳しい状況で守っているキーパーを正当に評価している」というロジックの証明です。\n\nホワイトボックス化による信頼:\n\n「なぜその評価になったのか」を、この図に沿って説明できます。「この sigma_goalie という機能がノイズを抑制しているから、新人の評価も安全に行えるのです」といった、専門的な説得が可能になります。\n\n拡張性の提示:\n\n例えば「シュートの強さ」を追加したい場合は、左側の枝に新しいノードを足せばよいことが一目でわかります。モデルの成長ロードマップを共有する際の地図になります。",
    "crumbs": [
      "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト"
    ]
  },
  {
    "objectID": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#推論とモデル診断diagnostics",
    "href": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#推論とモデル診断diagnostics",
    "title": "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト",
    "section": "4. 推論とモデル診断（Diagnostics）",
    "text": "4. 推論とモデル診断（Diagnostics）\n推論を実行し、計算が正しく収束し、モデルが信頼できる状態であることを証明します。\n\nPATH_TRACE = \"inference_hockey_spatial.nc\"  # 推論結果を保存するファイル名\n\nif os.path.exists(PATH_TRACE):\n    # 保存されたファイルがある場合はロード\n    print(f\"Inference: Checked for existing inference results. Loading from {PATH_TRACE}.\")\n    inference = az.from_netcdf(PATH_TRACE)\nelse:\n    # ファイルがない場合のみ推論を実行\n    print(\"Inference: No inference results found, so we will start sampling. This may take a few minutes.\")\n    with hockey_spatial_model:\n        # ターゲット受容率を高めに設定し、安定性を確保\n        inference = pm.sample(2000, tune=1000, target_accept=0.95, random_seed=42)\n\n    # 結果を NetCDF 形式で保存（永続化）\n    inference.to_netcdf(PATH_TRACE)\n    print(f\"Inference: Inference completed and results saved to {PATH_TRACE}.\")\n\nInference: Checked for existing inference results. Loading from inference_hockey_spatial.nc.\n\n\n\n# 収束診断\nsummary = az.summary(inference, var_names=[\"mu_league\", \"sigma_goalie\"])\nr_hat_max = summary[\"r_hat\"].max()\n\ndisplay(summary)\nprint(f\"Diagnostics: Max r_hat = {r_hat_max:.4f}\")\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nmu_league\n-2.829\n0.646\n-4.018\n-1.592\n0.007\n0.007\n7615.0\n6261.0\n1.0\n\n\nsigma_goalie\n0.293\n0.214\n0.000\n0.668\n0.003\n0.002\n4414.0\n3784.0\n1.0\n\n\n\n\n\n\n\nDiagnostics: Max r_hat = 1.0000\n\n\n\n\\(\\hat{R}\\) (R-hat) 指標\n数値 : すべてのパラメータにおいて、値が 1.00 になっています。\n解釈 : \\(\\hat{R}\\) は「チェーン間のばらつき」と「チェーン内のばらつき」を比較する指標です。 一般に 1.1 未満（厳密には 1.05 未満）であれば収束したとみなされます。 今回の結果は、全て 1.00 に極めて近いため、複数の独立した試行（チェーン）がすべて同じ統計的結論に達していることを意味します。\n\n# 収束結果の可視化\naz.plot_trace(inference, var_names=[\"mu_league\", \"sigma_goalie\"], compact=False)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nトレースプロットの形状（Fuzzy Caterpillar）\n各チェーン（異なる色の線）が互いにしっかりと混ざり合い、特定の傾向（右肩上がりや下がり）を持たず、一定の範囲を細かく上下に振動しています。 これがいわゆる「毛虫（Fuzzy Caterpillar）」のような状態であり、サンプラーが事後分布の全体を効率よく探索できていることを示します。 特定の場所で停滞したり、チェーンごとに大きく値が離れたりしていないため、良好な収束のサインになります。\nよって、問題なく推論できていると判断できます。",
    "crumbs": [
      "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト"
    ]
  },
  {
    "objectID": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#戦略的評価３層フィルタリング",
    "href": "notebooks/02_hockey_gotender/hockey_spatial_analysis.html#戦略的評価３層フィルタリング",
    "title": "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト",
    "section": "5. 戦略的評価（３層フィルタリング）",
    "text": "5. 戦略的評価（３層フィルタリング）\n\n第１層: HDI（統計的実装）\n実力が運ではなく、統計的に見てリーグ平均を超えているかを検証します。\n\nfrom privacy_utils import obfuscate_name  # 名前を匿名化\n\n# GSAx を算出（１試合 30シュートあたりに正規化）\nsamples_xg = inference.posterior[\"xG\"].values.reshape(-1, len(df_small))\nactual_goals = df_small[\"obs_shots\"].values\n\nlist_goalie_gsax = []\nfor idx_g in range(len(uniques_goalie)):\n    mask = (df_small[\"idx_goalie\"] == idx_g)\n    # GSAx = Σ(xG) - Σ(Actual)\n    dist_gsax = (samples_xg[:, mask].sum(axis=1) - actual_goals[mask].sum()) / (mask.sum() / 30)\n    list_goalie_gsax.append(dist_gsax)\n\n# データの集計とソート準備\n# 各ゴテンダーの「名前」「分布」「平均値」をセットにしてまとめます。\ndata_goalie_ranking = []\nfor idx_g, dist in enumerate(list_goalie_gsax):\n    data_goalie_ranking.append({\n        \"name\": id_to_name[idx_g],\n        \"dist\": dist,\n        \"mean\": dist.mean()\n    })\n\n# 平均値（Mean GSAx）で昇順にソート\n# plot_forest は「下から上」に向かって描画されるため、昇順にソートすることで一番上が「最高成績」になります\ndata_goalie_ranking.sort(key=lambda x: x[\"mean\"], reverse=True)\n\n# ArivZ に渡すための「名前付き辞書」を作成\n# このキー（名前）が縦軸のラベルとして採用されます。\n# obfuscate_name() で選手名を匿名化しています。関数定義については GitHub をご参考下さい。\ndict_plot = {obfuscate_name(item[\"name\"]): item[\"dist\"] for item in data_goalie_ranking}\n\n# HDI の可視化\naz.plot_forest(dict_plot,\n               hdi_prob=0.94,\n               combined=True,\n               figsize=(12, max(8, len(uniques_goalie) * 0.4))\n               )\n\n# --- 背景のゾーンを色分け ---\n# 現在のグラフの表示範囲を取得\nxmin, xmax = plt.xlim()\nlimit = max(abs(xmin), abs(xmax))  # 0 を中心にバランスをとるために設定\nplt.axvspan(-limit, 0, color=COLOR_RED, alpha=0.1, label=\"Underperforming Zone\")  # マイナスエリア: 赤\nplt.axvspan(0, limit, color=COLOR_GREEN, alpha=0.1, label=\"Overperforming Zone\")\nplt.axvline(0, color=COLOR_YELLOW, linestyle=\"-\", linewidth=2.5, label=\"League Average\")  # 境界線\n\nplt.axvspan(-0.05, 0.05, color=COLOR_GRAY, alpha=0.2, label=\"ROPE (Error Margin)\")\n\n# グラフの装飾\nplt.title(\"Goalie Performance Ranking: GSAx per 30 Shots\", fontsize=16, pad=25)\nplt.xlabel(\"Goals Saved Above Expected (GSAx)\")\nplt.grid(axis=\"x\", linestyle=\":\", alpha=0.5)\nplt.legend(loc=\"upper left\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHDI(Highest Density Interval)\n事後分布において、最も確率密度が高い範囲(94%)になります。 紫のバーが各選手の「実力はこの範囲に収まると 94% の自信を持って言える」というエラーバーになります。バー全体が 0 を超えていれば、その選手の成果は高いと言える可能性が高いと解釈できます。\n\nTips\n各選手の HDI の平均値順にソートし、リーグ全体の平均値(黄色の縦線) を境界線としてエリア分けをしています。\nGreen ゾーンに HDI がある選手: チームに貯金を作っている選手。統計的な確信を持って「獲得/維持すべき」という判断基準を与えることができます。\nRed ゾーンに HDI がある選手: 期待以上にゴールを許してしまっている選手。小規模データゆえ「たまたま不調」の可能性もありますが、現状の戦力外リスクとして注視すべきという情報を統計的な観点から与えることができます。\nYellowライン: リーグ平均の壁になります。ここを基準に左右どれだけ離れられるか、が選手の市場価値評価の鍵となります。\n\n\n\n第２層: ROPE（実質的評価）\n統計的な差異が、ビジネス上のインパクトを伴う「意味のある差」であるかを判定します。 各選手の分析結果（平均貢献度、傑出確率、判定ステータス）を構造化された表形式に変換し、外部ファイルとして永続化します。\n\nROPE_THRESHOLD = 0.05  # ±0.05点以内は実務上の誤差とみなす\nresults_rope = []  # 分析結果を格納するリストの準備\n\nfor i, dist in enumerate(list_goalie_gsax):\n    mean_gsax = dist.mean()\n    prob_above_rope = (dist &gt; ROPE_THRESHOLD).mean()\n\n    if mean_gsax &gt; ROPE_THRESHOLD:\n        status = \"Beyond ROPE (Outstanding)\"\n    elif mean_gsax &lt; -ROPE_THRESHOLD:\n        status = \"Below ROPE (Poor)\"\n    else:\n        status = \"Inside ROPE (Equivalent)\"\n\n    # リストに追加\n    results_rope.append({\n        \"Coalie\": obfuscate_name(id_to_name[i]),  # 選手名を匿名化\n        \"Mean_GSAx\": round(mean_gsax, 3),\n        \"Outstanding_Prob\": round(prob_above_rope * 100, 1),\n        \"Status\": status\n    })\n\n# DataFrame の作成とソート（貢献度順）\ndf_rope_analysis = pd.DataFrame(results_rope).sort_values(by=\"Mean_GSAx\", ascending=False)\n\ndf_rope_analysis\n\n\n\n\n\n\n\n\nCoalie\nMean_GSAx\nOutstanding_Prob\nStatus\n\n\n\n\n62\nA. K.\n2.847\n100.0\nBeyond ROPE (Outstanding)\n\n\n64\nV. H.\n2.757\n100.0\nBeyond ROPE (Outstanding)\n\n\n73\nJ. H.\n2.735\n100.0\nBeyond ROPE (Outstanding)\n\n\n69\nJ. S.\n2.706\n100.0\nBeyond ROPE (Outstanding)\n\n\n39\nD. R.\n2.667\n100.0\nBeyond ROPE (Outstanding)\n\n\n...\n...\n...\n...\n...\n\n\n50\nP. M.\n-5.048\n0.5\nBelow ROPE (Poor)\n\n\n36\nS. M.\n-5.944\n0.1\nBelow ROPE (Poor)\n\n\n14\nA. F.\n-6.085\n0.3\nBelow ROPE (Poor)\n\n\n37\nE. M.\n-6.390\n0.3\nBelow ROPE (Poor)\n\n\n1\nJ. D.\n-8.300\n0.0\nBelow ROPE (Poor)\n\n\n\n\n79 rows × 4 columns\n\n\n\n\n戦略的解釈：ROPE分析を通じた意思決定\n実ショットデータ 500 件に基づく、全 79 名のゴテンダーを対象とした ROPE（実質的等価領域）分析の結果を解釈します。この分析では、単なる平均値ではなく、場所の難易度を考慮した上での「真の貢献度」を評価しています。\n\n分析結果のサマリー\n全 79 名の分布は以下の通りです。\n\nBeyond ROPE (Outstanding): 55名\n\n統計的誤差を超えて、明らかにリーグ平均以上の価値を提供している選手。\n\nBelow ROPE (Poor): 23名\n\n統計的誤差を超えて、期待値以上の失点を許している、改善が必要な選手。\n\nInside ROPE (Equivalent): 1名 (F. G. 選手)\n\n数値上はプラスですが、実務上は「リーグ平均と等価」とみなすべき選手。\n\n\n\n\n\n\n注目選手の詳細分析\n\n1. トップパフォーマー：A. K., V. H., J. H. 選手\n\n平均 \\(GSAx\\) (2.7 〜 2.8): 1試合（30シュート想定）あたり、平均的なキーパーよりも 約 2.7 点以上多く防いでいます。\n傑出確率(100.0%): 彼らが「本物」である確率は 100% と推定され、運ではなく実力であることが統計的に断定されています。\nビジネス判断: 市場価値が急騰する前に確保すべき「最優先資産」です。\n\n\n\n2. ボーダーライン：F. G. 選手 (Inside ROPE)\n\n平均 \\(GSAx\\) (0.005): プラスの数値ではありますが、ROPE（\\(\\pm 0.05\\)）の範囲内です。\nビジネス判断: 現時点では「平均的な選手」と能力の差が認められません。高額な契約を提示するにはリスクが伴います。\n\n\n\n3. ボトムパフォーマー：J. D., E. M. 選手\n\n平均 \\(GSAx\\) (-8.3 〜 -6.3): 平均的な選手と比較して、1試合あたり 6〜8 点分多く失点している計算になります。\nビジネス判断: 早急な戦力補強、または守備システム自体の抜本的な見直しが必要です。\n\n\n\n\n\nビジネス・アクションプラン\n\n「隠れた名手」の特定: チームの勝率が低くても、このリストで Beyond ROPE に入っている選手は、「チームの守備力が低いだけで、個人は極めて優秀」な 割安の優良資産 である可能性が高いです。\n投資対効果の最大化: Outstanding Prob が高い選手に予算を集中させ、Inside ROPE の選手についてはコストパフォーマンスを厳格に評価します。\nエビデンスに基づく編成会議: 「なんとなく調子が悪い」といった主観を排除し、この \\(GSAx\\) と \\(ROPE\\) の指標を用いることで、法務・経営陣に対して論理的で透明性の高い編成案を提示できます。\n\n\n# CSVファイルとして保存（ビジネス用補助データ）\nFILE_OUTPUT = os.path.join(DIR_EXTRACT, \"data_goalie_rope_analysis.csv\")\n\ndf_rope_analysis.to_csv(FILE_OUTPUT, index=False)\n\nデータフレーム化（Tabularization）することで、どの選手が「実質的なプラスの差」を生み出しているかが一目で比較可能になります。また、CSVとしてファイル化しておくことで、「分析環境を持っていない経営層や現場スタッフ」が Excel 等でデータを確認できるようになり、組織全体の意思決定のスピードと質を向上させます。\n\n\n\n第３層: ROI (経済的評価)\n統計的評価を「ビジネスの投資判断」へ昇華させます。 GSAx を金額換算し、さらに「収益がプラスになる確率」と「確信の範囲（94% HDI）」を併記することで、リスクとリターンを一目で把握できる最終報告用の資料を生成します。\n\n# 経済価値のパラメータ設定（ビジネスモデルに合わせて調整可能）\nVALUE_GOAL = 1_000_000  # １ゴールを防ぐ価値 = 100万円\nANNUAL_GAMES = 60  # 年間の想定出場試合数\n\nresults_roi = []\n\nprint(\"Layer 3: ROI Assessment calculating...\")\n\n# list_goalie_gsax は各選手の「１試合（30本）あたりの GSAx分布」\nfor i, dist in enumerate(list_goalie_gsax):\n    profit_expected_annual = dist.mean() * ANNUAL_GAMES * VALUE_GOAL  # 年間の期待合計貢献額\n    prob_profitability = (dist &gt; 0).mean()  # 収益化確率（平均以上の貢献をする確率）\n\n    # 94% HDI による収益の確信範囲\n    hdi_lower, hdi_upper = az.hdi(dist, hdi_prob=0.94)\n    profit_hdi_lower = hdi_lower * ANNUAL_GAMES * VALUE_GOAL\n    profit_hdi_upper = hdi_upper * ANNUAL_GAMES * VALUE_GOAL\n\n    results_roi.append({\n        \"Goalie\": obfuscate_name(id_to_name[i]),\n        \"Expected_Annual_Profit\": int(profit_expected_annual),\n        \"Profitability_Prob\": round(prob_profitability * 100, 1),\n        \"Profit_HDI_Lower\": int(profit_hdi_lower),\n        \"Profit_HDI_Upper\": int(profit_hdi_upper)\n    })\n\n# DataFrame化とソート\ndf_roi = pd.DataFrame(results_roi).sort_values(by=\"Expected_Annual_Profit\", ascending=False)\n\ndf_roi\n\nLayer 3: ROI Assessment calculating...\n\n\n\n\n\n\n\n\n\nGoalie\nExpected_Annual_Profit\nProfitability_Prob\nProfit_HDI_Lower\nProfit_HDI_Upper\n\n\n\n\n62\nA. K.\n170790436\n100.0\n51198446\n297205298\n\n\n64\nV. H.\n165417100\n100.0\n53097600\n279289732\n\n\n73\nJ. H.\n164094708\n100.0\n42952293\n286368258\n\n\n69\nJ. S.\n162353045\n100.0\n39940961\n289616400\n\n\n39\nD. R.\n159999358\n100.0\n49822147\n268045832\n\n\n...\n...\n...\n...\n...\n...\n\n\n50\nP. M.\n-302851857\n0.5\n-397484645\n-195002825\n\n\n36\nS. M.\n-356628658\n0.1\n-426010415\n-280890982\n\n\n14\nA. F.\n-365104667\n0.3\n-453358070\n-245912157\n\n\n37\nE. M.\n-383429912\n0.3\n-479982156\n-251180136\n\n\n1\nJ. D.\n-497985072\n0.0\n-572780334\n-404946765\n\n\n\n\n79 rows × 5 columns\n\n\n\n\n# CSV として保存\nFILE_ROI = os.path.join(DIR_EXTRACT, \"goalie_final_investment_report.csv\")\ndf_roi.to_csv(FILE_ROI, index=False)\n\n統計量を「金額」と「リスク幅」に変換し、経営層が直感的に投資判断（契約更新や獲得交渉）を行える形式に可視化します。\n\n# 最終レポートの可視化（上位15名）\ndf_top = df_roi.head(15).copy()\n\n# 通貨単位（万円）に変換して見やすくする\ndf_top[\"Profit_Million\"] = df_top[\"Expected_Annual_Profit\"] / 10_000\ndf_top[\"Lower_Million\"] = df_top[\"Profit_HDI_Lower\"] / 10_000\ndf_top[\"Upper_Million\"] = df_top[\"Profit_HDI_Upper\"] / 10_000\n\n# プロット実行\n# 期待値のバーチャート\nplt.figure(figsize=(9, 6))\nbars = plt.barh(df_top[\"Goalie\"][::-1], df_top[\"Profit_Million\"][::-1],\n                color=COLOR_PURPLE, alpha=0.7, label=\"Expected Annual Contribution Studies\"\n                )\n\n# 不確実性のエラーバー（HDI）\nplt.errorbar(df_top[\"Profit_Million\"][::-1], np.arange(len(df_top)),\n             xerr=[(df_top[\"Profit_Million\"] - df_top[\"Lower_Million\"])[::-1],\n                   (df_top[\"Upper_Million\"] - df_top[\"Profit_Million\"])[::-1]],\n             fmt=\"none\", ecolor=COLOR_GRAY, capsize=5, label=\"94% HDI (Confidence range)\")\n\nplt.axvline(0, color=COLOR_RED, linestyle=\"--\", linewidth=1)\nplt.title(\"Final Investment Value Report: Expected Annual Economic Contribution (TOP 15)\", fontsize=16,\n          fontweight=\"bold\", pad=20)\nplt.xlabel(\"Expected contribution amount (unit: 10,000 yen)\", fontsize=12)\nplt.ylabel(\"Player\", fontsize=12)\nplt.legend(loc=\"lower right\")\n\n# 値のラベル付け\nfor i, v in enumerate(df_top[\"Profit_Million\"][::-1]):\n    plt.text(v + 50, i, f\"{v:,.0f}yen\", va=\"center\", fontweight=\"bold\", color=COLOR_YELLOW)\n\nplt.tight_layout()\nIMG_FINAL_REPORT = os.path.join(DIR_EXTRACT, \"final_roi_report_chart.png\")\nplt.savefig(IMG_FINAL_REPORT, dpi=150)\nplt.show()\n\n\n\n\n\n\n\n\nゴテンダーのパフォーマンスがチームのキャッシュフローに対して億単位のインパクトを与えることを示唆しています。\n\nトップ層（上位 5 名：A. K., V. H. 等）の価値\n\n期待収益: リーグ平均の選手と比較して、年間で 約 1.6 億円 〜 1.7 億円 の損失（失点）を回避する価値があると算出されました。\n解釈: 彼らに対して 1 億円の年俸を支払ったとしても、なおチームに数千万円の「純利益（失点抑制による勝利への貢献）」をもたらす計算になります。\n\n\ndf_top[[\"Goalie\", \"Expected_Annual_Profit\", \"Profitability_Prob\", \"Profit_HDI_Lower\", \"Profit_HDI_Upper\"]]\n\n\n\n\n\n\n\n\nGoalie\nExpected_Annual_Profit\nProfitability_Prob\nProfit_HDI_Lower\nProfit_HDI_Upper\n\n\n\n\n62\nA. K.\n170790436\n100.0\n51198446\n297205298\n\n\n64\nV. H.\n165417100\n100.0\n53097600\n279289732\n\n\n73\nJ. H.\n164094708\n100.0\n42952293\n286368258\n\n\n69\nJ. S.\n162353045\n100.0\n39940961\n289616400\n\n\n39\nD. R.\n159999358\n100.0\n49822147\n268045832\n\n\n44\nJ. G.\n139588811\n100.0\n28147238\n254939718\n\n\n34\nI. S.\n138419304\n100.0\n46628693\n223657791\n\n\n12\nC. I.\n137314100\n100.0\n42632083\n228914780\n\n\n22\nJ. M.\n136509122\n100.0\n43062664\n227819462\n\n\n31\nJ. Q.\n135496606\n100.0\n37540900\n218786598\n\n\n35\nE. C.\n131216185\n100.0\n42877182\n217479135\n\n\n56\nC. P.\n124674332\n100.0\n40466382\n213457998\n\n\n28\nI. S.\n121988835\n100.0\n33182011\n199023392\n\n\n78\nD. C.\n121341191\n100.0\n28310071\n228263486\n\n\n20\nJ. B.\n120769854\n100.0\n42119516\n198761783\n\n\n\n\n\n\n\n\nExpected_Annual_Profit(推定資産価値): 予算編成（年俸総額）の基準とする。\nProfitabilly_Prob: 100%に近い選手は「コアメンバー」として固定。\nProfit_HDI_Range: 最悪のシナリオ（Lower）でも赤字にならないかを確認。\n\n\nExpected Annual Profit (期待年間貢献額)\n選手が平均的なゴテンダーと比較して、年間（60試合）でどれだけの失点を防ぎ、それを金額換算（1失点=100万円）したものになります。 「この選手を起用することで、チームの損失を何千万円防げるか？」という指標です。\n\nこの ROI レポートは、現場（コーチ陣）と経営（フロント）の架け橋になります。 「なぜ彼を獲得するのに 1.5 億円必要なのか？」 という問いに対し、 「彼は場所の難易度を考慮しても年間で 1.7 億円分の失点を防ぐため、実質的に 2,000 万円のプラスを生む投資だからです」 と、科学的根拠（エビデンス）を持って答えられるようになります。\n\n\n\nベイズによるスポーツビジネスの不確実へアプローチ\n2024年シーズンの実ショットデータを用い、ベイズ階層モデルによるゴテンダーの貢献度評価（GSAx）から、最終的な経済価値（ROI）の算出までを完結させました。 スポーツアナリティクスを題材にあえて小規模データにし「データの少なさ」と「直感への依存」というリスクの中での分析を実行しました。本分析が提示したアプローチで、不確実性を排除するためのユースケースをご提案出来たかと思います。\n\n1. 統計的実証（HDI）による「運」の排除\n「たまたま数試合調子が良かっただけではないか？」という疑念に対し、HDI（最高密度区間） は科学的な回答を与えます。 500件という限られたデータであっても、階層モデルによる「収縮推定（Shrinkage）」を行うことで、過学習を防ぎつつ、選手が持つ真の実力を表現しました。\n\n\n2. 実質的評価（ROPE）による「ノイズ」の排除\n数字上の微細な差は、ビジネスにおいての判断を難しくさせます。ROPE（実質的等価領域） を導入することで、統計的に有意であっても実務的に意味のない差を「ノイズ」として切り捨て、 投資すべき「対象（Beyond ROPE）」を明確にし判断における有用な情報を提供できました。\n\n\n3. 経済的評価（ROI）による「言語」の統一\n実際のビジネスの現場では、現場と経営の言葉が通じない状況が発生します。本プロジェクトでは GSAx を「年間期待経済貢献額（円）」 という通貨単位に翻訳することで\n\n「A. K. 選手は GSAx が高い」（現場の言葉）\n「A. K. 選手は年間で 1.7 億円の損失を回避する投資価値がある」（経営の言葉）\n\nという対話が可能になり迅速かつ合理的な予算編成と契約交渉を可能にします。\n\n\n倫理的・専門的な配慮：プライバシー・バイ・デザイン\n分析の過程で実施した 「選手名の秘匿化（イニシャル化）」 は、法務リスクの回避に加えて 特定の個人をランク付けして批判するのではなく、「評価システムそのものの妥当性」と「意思決定プロセスの透明性」に焦点をつなげるという利点もあります。\n\n\nまとめ\nベイズ統計は、「不確実性を確率として捉え、リスクを許容可能な範囲に収める」 ための良い手段になります。 本分析を通じて示された「科学的な選手評価」は、スポーツビジネスにおける勝率を上げ、持続可能な成長を実現するための基盤構築に繋げることが出来ます。",
    "crumbs": [
      "Hockey Spatial Analysis: 空間統計による真の貢献度可視化プロジェクト"
    ]
  }
]